{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"zCY8Z5kFN0aF"},"outputs":[],"source":["!sudo apt-get install -y fonts-nanum\n","!sudo fc-cache -fv\n","!rm ~/.cache/matplotlib -rf"]},{"cell_type":"code","execution_count":71,"metadata":{"id":"YmzsGDlu3_M9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652089622648,"user_tz":-540,"elapsed":13637,"user":{"displayName":"Ha Jun","userId":"06515366270624202236"}},"outputId":"90cbc81d-9e26-44d0-d323-70ae83df530b"},"outputs":[{"output_type":"stream","name":"stdout","text":["\r0% [Working]\r            \rHit:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n","\r0% [Connecting to archive.ubuntu.com] [Connecting to security.ubuntu.com (185.1\r                                                                               \rHit:2 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n","\r0% [Connecting to archive.ubuntu.com] [Connecting to security.ubuntu.com (185.1\r0% [1 InRelease gpgv 1,575 B] [Connecting to archive.ubuntu.com] [Connecting to\r                                                                               \rIgn:3 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n","Hit:4 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n","Get:5 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n","Hit:6 http://archive.ubuntu.com/ubuntu bionic InRelease\n","Get:7 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n","Get:8 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n","Get:9 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n","Hit:11 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n","Hit:12 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n","Get:13 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [3,168 kB]\n","Hit:14 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n","Get:15 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,272 kB]\n","Get:16 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,956 kB]\n","Get:17 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,498 kB]\n","Get:18 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2,734 kB]\n","Get:19 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [1,003 kB]\n","Fetched 12.9 MB in 4s (3,351 kB/s)\n","Reading package lists... Done\n","Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","g++ is already the newest version (4:7.4.0-1ubuntu2.3).\n","openjdk-8-jdk is already the newest version (8u312-b07-0ubuntu1~18.04).\n","The following packages were automatically installed and are no longer required:\n","  libnvidia-common-460 nsight-compute-2020.2.0\n","Use 'apt autoremove' to remove them.\n","0 upgraded, 0 newly installed, 0 to remove and 52 not upgraded.\n","Requirement already satisfied: konlpy in /usr/local/lib/python3.7/dist-packages (0.6.0)\n","Requirement already satisfied: JPype1-py3 in /usr/local/lib/python3.7/dist-packages (0.5.5.4)\n","Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.21.6)\n","Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n","Requirement already satisfied: JPype1>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.3.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (4.2.0)\n","mecab-ko is already installed\n","mecab-ko-dic is already installed\n","mecab-python is already installed\n","Done.\n"]}],"source":["!apt-get update \n","!apt-get install g++ openjdk-8-jdk \n","!pip install konlpy JPype1-py3 \n","!bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)"]},{"cell_type":"code","execution_count":72,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2746,"status":"ok","timestamp":1652089625350,"user":{"displayName":"Ha Jun","userId":"06515366270624202236"},"user_tz":-540},"id":"lTdwEq6dyyVc","outputId":"7edb6b4f-4d8b-483c-c7bf-04662dbadec5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"]}],"source":["import pandas as pd\n","import matplotlib\n","import os\n","import sys\n","import urllib.request\n","import datetime\n","import time\n","import json\n","import re\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from bs4 import BeautifulSoup\n","import requests\n","\n","from collections import Counter\n","from konlpy.tag import Mecab\n","from google.colab import drive\n","\n","\n","drive.mount('/gdrive')"]},{"cell_type":"code","execution_count":73,"metadata":{"id":"oAsIQ1bt0e0f","executionInfo":{"status":"ok","timestamp":1652089625377,"user_tz":-540,"elapsed":65,"user":{"displayName":"Ha Jun","userId":"06515366270624202236"}}},"outputs":[],"source":["workspace_path = '/gdrive/My Drive/주가분석'\n","KOSPI_data_dir = os.path.join(workspace_path, 'KOSPI_data')\n","com_data_dir = os.path.join(workspace_path, 'com_data')\n","news_link_dir = os.path.join(workspace_path, 'news_link')\n","today_news_dir = os.path.join(workspace_path, 'today_news_link')\n","KOSPI_append_dir = os.path.join(workspace_path, 'KOSPI_data_append')\n","regression_data_dir = os.path.join(workspace_path, 'regression_data')\n","dir_list = sorted(os.listdir(KOSPI_data_dir))"]},{"cell_type":"markdown","metadata":{"id":"4uGQVasTRvPw"},"source":["## KOSPI 1월 ~ 3월 일별 데이터(시가총액 내림차순 정렬)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zZbd70JG1Sct"},"outputs":[],"source":["data_frame_year = [] # data_frame_year[월][일]\n","names = []\n","for cur_dir in dir_list:\n","  data_frame_month = []\n","  name = []\n","  cur_dir = os.path.join(KOSPI_data_dir, cur_dir)\n","  file_list = os.listdir(cur_dir)\n","  for cur_file in file_list:\n","    name.append(cur_file.split('_')[1].split('.')[0])\n","    cur_file = os.path.join(cur_dir, cur_file)\n","    data_frame_day = pd.read_csv(cur_file, encoding = 'cp949').sort_values(by=['시가총액'], ascending = False)\n","    data_frame_month.append(data_frame_day)\n","  data_frame_year.append(data_frame_month)\n","  names.append(name)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lvpw_lysWMB9"},"outputs":[],"source":["# top 100 기업 추출\n","top100 = data_frame_year[0][0][:100]['종목명']\n","com_list = data_frame_year[0][0][:]['종목명']"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":52,"status":"ok","timestamp":1652001852495,"user":{"displayName":"Ha Jun","userId":"06515366270624202236"},"user_tz":-540},"id":"spBxlmzXWqmW","outputId":"9bf86bc1-b1e8-400b-83cb-bd731743ad49"},"outputs":[{"data":{"text/plain":["426        삼성전자\n","147      SK하이닉스\n","100       NAVER\n","420    삼성바이오로직스\n","427       삼성전자우\n","         ...   \n","38         GS건설\n","20        DB하이텍\n","838       한솔케미칼\n","854        한전기술\n","818      한국항공우주\n","Name: 종목명, Length: 100, dtype: object"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["top100"]},{"cell_type":"markdown","metadata":{"id":"lckDwWV4SA0O"},"source":["## 기업별 데이터 생성"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"76j4pPEn8N5z"},"outputs":[],"source":["sample = pd.read_csv('/gdrive/My Drive/주가분석/KOSPI_data/1월/data_20220103.csv', encoding = 'cp949')\n","com_dict = dict(zip(sample['종목명'], [[] for _ in range(len(sample))]))\n","\n","for cur_dir in dir_list:\n","  cur_dir = os.path.join(KOSPI_data_dir, cur_dir)\n","  file_list = os.listdir(cur_dir)\n","  for cur_file in file_list:\n","    day = cur_file.split('_')[1].split('.')[0]\n","    cur_file = os.path.join(cur_dir, cur_file)\n","    data_frame_day = pd.read_csv(cur_file, encoding = 'cp949')\n","    data_frame_day['날짜'] = day\n","    \n","    for i in range(0, len(data_frame_day)):\n","      name = data_frame_day.loc[i]['종목명']\n","      try:\n","        com_dict[name].append(data_frame_day.loc[i])\n","      except KeyError:\n","        com_dict[name] = []"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":379,"status":"ok","timestamp":1652005542159,"user":{"displayName":"Ha Jun","userId":"06515366270624202236"},"user_tz":-540},"id":"rS7lsmZVTEf1","outputId":"c4e6fc10-41b9-43d3-aa05-95ec3d06552e"},"outputs":[{"data":{"text/plain":["종목코드              005930\n","종목명                 삼성전자\n","종가                 78600\n","대비                   300\n","등락률                 0.38\n","시가                 79400\n","고가                 79800\n","저가                 78200\n","거래량             13502112\n","거래대금       1066006837750\n","시가총액     469224908430000\n","상장주식수         5969782550\n","날짜              20220103\n","Name: 426, dtype: object"]},"execution_count":66,"metadata":{},"output_type":"execute_result"}],"source":["#pd.DataFrame(com_dict['삼성전자']).tail()\n","com_dict['삼성전자'][0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PQ5E8A-o4FCF"},"outputs":[],"source":["#com_data_dir에 기업별 데이터 저장\n","if not os.path.exists(com_data_dir):\n","  os.makedirs(com_data_dir)\n","\n","for name in com_dict.keys():\n","  com_data_frame = pd.DataFrame(com_dict[name])\n","  com_data_frame.to_csv(os.path.join(com_data_dir, name) + '.csv')\n"]},{"cell_type":"markdown","metadata":{"id":"j4RWLa4OWJAQ"},"source":["##주가 데이터 추가"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":265,"status":"ok","timestamp":1652059629196,"user":{"displayName":"Ha Jun","userId":"06515366270624202236"},"user_tz":-540},"id":"y7WuYjmYXBOx","outputId":"68e45679-8356-4a76-d6d0-01babda800a7"},"outputs":[{"data":{"text/plain":["['data_20220509.csv']"]},"execution_count":47,"metadata":{},"output_type":"execute_result"}],"source":["append_list = os.listdir(KOSPI_append_dir)\n","append_list"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uWATp-mAWPE3"},"outputs":[],"source":["append_list = os.listdir(KOSPI_append_dir)\n","append_list.sort()\n","\n","sample = pd.read_csv(os.path.join(KOSPI_append_dir, append_list[0]), encoding = 'cp949')\n","com_dict = dict(zip(sample['종목명'], [[] for _ in range(len(sample))]))\n","\n","for file in append_list:\n","  file_path = os.path.join(KOSPI_append_dir, file)\n","  data_frame_append = pd.read_csv(file_path, encoding = 'cp949')\n","  day = file.split('_')[1].split('.')[0]\n","  data_frame_append['날짜'] = day\n","  \n","  for com in data_frame_append['종목명']:\n","    if com_dict.get(com) is None:\n","      com_dict[com] = []\n","    [com_dict[com].append(line) for line in [data_frame_append[data_frame_append['종목명'] == com]]]\n","    \n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":231},"executionInfo":{"elapsed":315,"status":"ok","timestamp":1652059640492,"user":{"displayName":"Ha Jun","userId":"06515366270624202236"},"user_tz":-540},"id":"RVZaxXcFIlHe","outputId":"3d54c257-af27-4fed-bf85-dda46c32d6de"},"outputs":[{"data":{"text/html":["\n","  <div id=\"df-f79e659f-5cba-45d6-bd9b-530fa5e015b9\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>종목코드</th>\n","      <th>종목명</th>\n","      <th>종가</th>\n","      <th>대비</th>\n","      <th>등락률</th>\n","      <th>시가</th>\n","      <th>고가</th>\n","      <th>저가</th>\n","      <th>거래량</th>\n","      <th>거래대금</th>\n","      <th>시가총액</th>\n","      <th>상장주식수</th>\n","      <th>날짜</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>428</th>\n","      <td>005930</td>\n","      <td>삼성전자</td>\n","      <td>66500</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>66300</td>\n","      <td>66900</td>\n","      <td>66100</td>\n","      <td>5241105</td>\n","      <td>348846284300</td>\n","      <td>396990539575000</td>\n","      <td>5969782550</td>\n","      <td>20220509</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f79e659f-5cba-45d6-bd9b-530fa5e015b9')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-f79e659f-5cba-45d6-bd9b-530fa5e015b9 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-f79e659f-5cba-45d6-bd9b-530fa5e015b9');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["       종목코드   종목명     종가  대비  등락률     시가     고가     저가      거래량          거래대금  \\\n","428  005930  삼성전자  66500   0  0.0  66300  66900  66100  5241105  348846284300   \n","\n","                시가총액       상장주식수        날짜  \n","428  396990539575000  5969782550  20220509  "]},"execution_count":49,"metadata":{},"output_type":"execute_result"}],"source":["pd.DataFrame(com_dict['삼성전자'][0])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u95AvSnw8xF6"},"outputs":[],"source":["for com in com_dict.keys():\n","  com_file_path = os.path.join(com_data_dir, com + '.csv')\n","  \n","  if os.path.isfile(com_file_path):\n","    data_frame_com = pd.read_csv(com_file_path, encoding = 'UTF-8')\n","    for line in com_dict[com]:\n","      line = pd.DataFrame(line)\n","      if not data_frame_com.iloc[len(data_frame_com) - 1]['날짜'] >= int(line['날짜']):\n","        data_frame_com = data_frame_com.append(line)\n","    \n","  data_frame_com.drop(columns = ['Unnamed: 0'], inplace = True)\n","  data_frame_com.to_csv(os.path.join(com_data_dir, com + '.csv'))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L2pgmbakjUcx"},"outputs":[],"source":["#특정 날짜만 업데이트(미완성)\n","for com in com_dict.keys():\n","  com_file_path = os.path.join(com_data_dir, com + '.csv')\n","  data_frame_com = pd.read_csv(com_file_path, encoding = 'UTF-8')\n","  for line in com_dict[com]:\n","    data_frame_com[data_frame_com['날짜'] == int(line['날짜'].values[0])].replace(line) \n","\n","  "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":363},"executionInfo":{"elapsed":340,"status":"ok","timestamp":1649215187936,"user":{"displayName":"Ha Jun","userId":"06515366270624202236"},"user_tz":-540},"id":"ZbtPRcLRdGyt","outputId":"7f5f4d0d-4c9b-4e4c-d994-eb9ea1c0c795"},"outputs":[{"data":{"text/html":["\n","  <div id=\"df-0232ffbe-1402-47a8-a12e-4a034ab4ddfe\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>종목코드</th>\n","      <th>종목명</th>\n","      <th>종가</th>\n","      <th>대비</th>\n","      <th>등락률</th>\n","      <th>시가</th>\n","      <th>고가</th>\n","      <th>저가</th>\n","      <th>거래량</th>\n","      <th>거래대금</th>\n","      <th>시가총액</th>\n","      <th>상장주식수</th>\n","      <th>날짜</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>53</th>\n","      <td>938.0</td>\n","      <td>3280</td>\n","      <td>흥아해운</td>\n","      <td>2850</td>\n","      <td>-35</td>\n","      <td>-1.21</td>\n","      <td>2885</td>\n","      <td>2885</td>\n","      <td>2800</td>\n","      <td>550642</td>\n","      <td>1565556400</td>\n","      <td>685210962150</td>\n","      <td>240424899</td>\n","      <td>20220324</td>\n","    </tr>\n","    <tr>\n","      <th>54</th>\n","      <td>938.0</td>\n","      <td>3280</td>\n","      <td>흥아해운</td>\n","      <td>2790</td>\n","      <td>-60</td>\n","      <td>-2.11</td>\n","      <td>2850</td>\n","      <td>2870</td>\n","      <td>2780</td>\n","      <td>623497</td>\n","      <td>1755543805</td>\n","      <td>670785468210</td>\n","      <td>240424899</td>\n","      <td>20220325</td>\n","    </tr>\n","    <tr>\n","      <th>55</th>\n","      <td>939.0</td>\n","      <td>3280</td>\n","      <td>흥아해운</td>\n","      <td>2770</td>\n","      <td>-20</td>\n","      <td>-0.72</td>\n","      <td>2795</td>\n","      <td>2815</td>\n","      <td>2730</td>\n","      <td>574122</td>\n","      <td>1585996600</td>\n","      <td>665976970230</td>\n","      <td>240424899</td>\n","      <td>20220328</td>\n","    </tr>\n","    <tr>\n","      <th>56</th>\n","      <td>939.0</td>\n","      <td>3280</td>\n","      <td>흥아해운</td>\n","      <td>2745</td>\n","      <td>-25</td>\n","      <td>-0.90</td>\n","      <td>2755</td>\n","      <td>2825</td>\n","      <td>2710</td>\n","      <td>555086</td>\n","      <td>1542167000</td>\n","      <td>659966347755</td>\n","      <td>240424899</td>\n","      <td>20220329</td>\n","    </tr>\n","    <tr>\n","      <th>57</th>\n","      <td>939.0</td>\n","      <td>3280</td>\n","      <td>흥아해운</td>\n","      <td>2890</td>\n","      <td>145</td>\n","      <td>5.28</td>\n","      <td>2785</td>\n","      <td>3205</td>\n","      <td>2735</td>\n","      <td>10896198</td>\n","      <td>32961482930</td>\n","      <td>694827958110</td>\n","      <td>240424899</td>\n","      <td>20220330</td>\n","    </tr>\n","    <tr>\n","      <th>58</th>\n","      <td>939.0</td>\n","      <td>3280</td>\n","      <td>흥아해운</td>\n","      <td>2910</td>\n","      <td>20</td>\n","      <td>0.69</td>\n","      <td>2860</td>\n","      <td>2950</td>\n","      <td>2810</td>\n","      <td>1181048</td>\n","      <td>3403526510</td>\n","      <td>699636456090</td>\n","      <td>240424899</td>\n","      <td>20220331</td>\n","    </tr>\n","    <tr>\n","      <th>939</th>\n","      <td>NaN</td>\n","      <td>003280</td>\n","      <td>흥아해운</td>\n","      <td>2850</td>\n","      <td>-60</td>\n","      <td>-2.06</td>\n","      <td>2885</td>\n","      <td>2890</td>\n","      <td>2820</td>\n","      <td>548577</td>\n","      <td>1560746225</td>\n","      <td>685210962150</td>\n","      <td>240424899</td>\n","      <td>20220401</td>\n","    </tr>\n","    <tr>\n","      <th>939</th>\n","      <td>NaN</td>\n","      <td>003280</td>\n","      <td>흥아해운</td>\n","      <td>2855</td>\n","      <td>5</td>\n","      <td>0.18</td>\n","      <td>2870</td>\n","      <td>2875</td>\n","      <td>2785</td>\n","      <td>780112</td>\n","      <td>2253214125</td>\n","      <td>686413086645</td>\n","      <td>240424899</td>\n","      <td>20220404</td>\n","    </tr>\n","    <tr>\n","      <th>939</th>\n","      <td>NaN</td>\n","      <td>003280</td>\n","      <td>흥아해운</td>\n","      <td>3180</td>\n","      <td>325</td>\n","      <td>11.38</td>\n","      <td>2985</td>\n","      <td>3280</td>\n","      <td>2970</td>\n","      <td>10505494</td>\n","      <td>33003491805</td>\n","      <td>764551178820</td>\n","      <td>240424899</td>\n","      <td>20220405</td>\n","    </tr>\n","    <tr>\n","      <th>939</th>\n","      <td>NaN</td>\n","      <td>003280</td>\n","      <td>흥아해운</td>\n","      <td>3060</td>\n","      <td>-120</td>\n","      <td>-3.77</td>\n","      <td>3205</td>\n","      <td>3260</td>\n","      <td>2990</td>\n","      <td>2035871</td>\n","      <td>6402411920</td>\n","      <td>735700190940</td>\n","      <td>240424899</td>\n","      <td>20220406</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0232ffbe-1402-47a8-a12e-4a034ab4ddfe')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-0232ffbe-1402-47a8-a12e-4a034ab4ddfe button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-0232ffbe-1402-47a8-a12e-4a034ab4ddfe');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["     Unnamed: 0    종목코드   종목명    종가   대비    등락률    시가    고가    저가       거래량  \\\n","53        938.0    3280  흥아해운  2850  -35  -1.21  2885  2885  2800    550642   \n","54        938.0    3280  흥아해운  2790  -60  -2.11  2850  2870  2780    623497   \n","55        939.0    3280  흥아해운  2770  -20  -0.72  2795  2815  2730    574122   \n","56        939.0    3280  흥아해운  2745  -25  -0.90  2755  2825  2710    555086   \n","57        939.0    3280  흥아해운  2890  145   5.28  2785  3205  2735  10896198   \n","58        939.0    3280  흥아해운  2910   20   0.69  2860  2950  2810   1181048   \n","939         NaN  003280  흥아해운  2850  -60  -2.06  2885  2890  2820    548577   \n","939         NaN  003280  흥아해운  2855    5   0.18  2870  2875  2785    780112   \n","939         NaN  003280  흥아해운  3180  325  11.38  2985  3280  2970  10505494   \n","939         NaN  003280  흥아해운  3060 -120  -3.77  3205  3260  2990   2035871   \n","\n","            거래대금          시가총액      상장주식수        날짜  \n","53    1565556400  685210962150  240424899  20220324  \n","54    1755543805  670785468210  240424899  20220325  \n","55    1585996600  665976970230  240424899  20220328  \n","56    1542167000  659966347755  240424899  20220329  \n","57   32961482930  694827958110  240424899  20220330  \n","58    3403526510  699636456090  240424899  20220331  \n","939   1560746225  685210962150  240424899  20220401  \n","939   2253214125  686413086645  240424899  20220404  \n","939  33003491805  764551178820  240424899  20220405  \n","939   6402411920  735700190940  240424899  20220406  "]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["data_frame_com.tail(10)"]},{"cell_type":"markdown","metadata":{"id":"NU27-AmMASO5"},"source":["##네이버 API를 활용한 각 기업 뉴스 링크 저장"]},{"cell_type":"code","execution_count":74,"metadata":{"id":"6ENV9b5-AlZ9","executionInfo":{"status":"ok","timestamp":1652089664308,"user_tz":-540,"elapsed":615,"user":{"displayName":"Ha Jun","userId":"06515366270624202236"}}},"outputs":[],"source":["client_id = 'CQwzCYyTxfeamaoxWr7s'\n","client_secret = 'jSDxy71Vsh'\n","\n","#[CODE 1]\n","def getRequestUrl(url):\n","    req = urllib.request.Request(url)\n","    req.add_header(\"X-Naver-Client-Id\", client_id)\n","    req.add_header(\"X-Naver-Client-Secret\", client_secret)\n","    try:\n","        response = urllib.request.urlopen(req)\n","        if response.getcode() == 200:\n","            print(\"[%s] Url Request Success\" % datetime.datetime.now())\n","            return response.read().decode('utf-8')\n","    except Exception as e:\n","        print(e)\n","        print(\"[%s] Error for URL : %s\" % (datetime.datetime.now(), url))\n","    return None\n","\n","#[CODE 2]\n","def getNaverSearch(node, srcText, start, display):\n","    base = \"https://openapi.naver.com/v1/search\"\n","    node = \"/%s.json\" % node\n","    parameters = \"?query=%s&start=%s&display=%s\" % (urllib.parse.quote(srcText), start, display)\n","    url = base + node + parameters\n","    responseDecode = getRequestUrl(url) #[CODE 1]\n","    if (responseDecode == None):\n","        return None\n","    else:\n","        return json.loads(responseDecode)\n","\n","\n","def getPostData(post, jsonResult, cnt):\n","    title = post['title']\n","    description = post['description']\n","    org_link = post['originallink']\n","    link = post['link']\n","    pDate = datetime.datetime.strptime(post['pubDate'], '%a, %d %b %Y %H:%M:%S +0900')\n","    pDate = pDate.strftime('%Y-%m-%d %H:%M:%S')\n","    jsonResult.append({'cnt':cnt, 'title':title, 'description': description, 'org_link':org_link, 'link': org_link, 'pDate':pDate})\n","    return\n","\n","def clean_html(x):\n","  #https://predictor-ver1.tistory.com/4\n","  x = re.sub(\"\\&\\w*\\;\",\"\",x)\n","  x = re.sub(\"<.*?>\",\"\",x)\n","  return x\n","def getDayNews(date, top100, search_num, save = False, latest = False):\n","  if not os.path.exists(today_news_dir):\n","    os.makedirs(today_news_dir)\n","  node = 'news' #크롤링할 대상\n","  top100com_today_news = []\n","  for com in top100:  \n","    srcText = com\n","    com_dir = os.path.join(today_news_dir, com)\n","    if not os.path.exists(com_dir):\n","      os.makedirs(com_dir)\n","\n","    cnt = 0\n","    com_today_news = []\n","    jsonResponse = getNaverSearch(node, srcText, 1, search_num) #[CODE 2]\n","    \n","    print('%s %s' % (date, srcText))\n","    for post in jsonResponse['items']:\n","      \n","      pDate = datetime.datetime.strptime(post['pubDate'], '%a, %d %b %Y %H:%M:%S +0900')\n","      news_date = pDate.strftime('%Y%m%d%H%M')\n","      if latest is True:\n","        if date == news_date:\n","          cnt += 1\n","          getPostData(post, com_today_news, cnt) #[CODE 3]\n","      else:\n","        if date < news_date:\n","          cnt += 1\n","          getPostData(post, com_today_news, cnt) #[CODE 3]\n","    print('%d 건' % cnt)\n","    top100com_today_news.append(com_today_news)\n","    #print('전체 검색 : %d 건' %total)\n","    if save is True:\n","      with open(com_dir + '/%s_%s.json' % (date, srcText), 'w', encoding='utf8') as outfile:\n","          jsonFile = json.dumps(com_today_news, indent = 4, sort_keys = True, ensure_ascii = False)\n","          outfile.write(jsonFile)\n","          print(\"가져온 데이터 : %d 건\" %(cnt))\n","          print('%s_naver_%s.json SAVED' % (srcText, node))\n","  return top100com_today_news\n","def getTop100():\n","  data_frame_day = pd.read_csv(KOSPI_data_dir + '/3월/data_20220331.csv', encoding = 'cp949').sort_values(by=['시가총액'], ascending = False)\n","  top100 = data_frame_day[:100]['종목명']\n","  return top100"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8myPTBmvAnmS"},"outputs":[],"source":["'''\n","#코스닥 940 여개 기업 뉴스 검색\n","node = 'news' #크롤링할 대상\n","if not os.path.exists(news_link_dir):\n","  os.makedirs(news_link_dir)\n","#len(com_list) : 942 ; 검색 API 942 * 10회 호출\n","for com in com_list:  \n","  srcText = com\n","  cnt = 0\n","  jsonResult = []\n","  jsonResponse = getNaverSearch(node, srcText, 1, 100) #[CODE 2]\n","  for post in jsonResponse['items']:\n","    cnt += 1\n","    getPostData(post, jsonResult, cnt) #[CODE 3]\n","  #total = jsonResponse['total']\n","  while ((jsonResponse != None) and (jsonResponse['display'] != 0)):\n","      for post in jsonResponse['items']:\n","          cnt += 1\n","          getPostData(post, jsonResult, cnt) #[CODE 3]\n","      start = jsonResponse['start'] + jsonResponse['display']\n","      jsonResponse = getNaverSearch(node, srcText, start, 100) #[CODE 2]\n","  #print('전체 검색 : %d 건' %total)\n","  with open(news_link_dir + '/%s_naver_%s.json' % (srcText, node), 'w', encoding='utf8') as outfile:\n","      jsonFile = json.dumps(jsonResult, indent = 4, sort_keys = True, ensure_ascii = False)\n","      outfile.write(jsonFile)\n","  print(\"가져온 데이터 : %d 건\" %(cnt))\n","  print('%s_naver_%s.json SAVED' % (srcText, node))\n","'''"]},{"cell_type":"markdown","metadata":{"id":"dYghRxvdIC0o"},"source":["## 뉴스 본문 대략적인 추출"]},{"cell_type":"code","execution_count":75,"metadata":{"id":"uek4jA7KQST3","executionInfo":{"status":"ok","timestamp":1652089670506,"user_tz":-540,"elapsed":1392,"user":{"displayName":"Ha Jun","userId":"06515366270624202236"}}},"outputs":[],"source":["def get_naver_news(url):\n","  res = requests.get(url)\n","  if res.status_code == 200:\n","    html = res.text\n","    soup = BeautifulSoup(html, 'html.parser')\n","    body = soup.find('body')    \n","    return body\n","\n","def koreanLen(text):\n","    hangul = re.compile('[\\u3131-\\u3163\\uac00-\\ud7a3]+')  \n","    result = hangul.findall(text)\n","    return len(''.join(result))\n","\n","def getKorean(text):\n","    hangul = re.compile('[\\u3131-\\u3163\\uac00-\\ud7a3]+')  \n","    result = hangul.findall(text)\n","    return result\n","\n","def getNewsBody(link):\n","  N = 30\n","  try:\n","    body = get_naver_news(link)\n","  except:\n","    return\n","  if body is None:\n","    return\n","  text = body.get_text()\n","\n","  news_body_start = []\n","  text_split = text.split(' ')\n","  rm_set = {''}\n","  text_split = [k for k in text_split if k not in rm_set]\n","\n","  for word_index in range(0, len(text_split) - N):\n","    word_len = 0\n","    korean_word_len = 0\n","    for j in range(0, N):\n","      korean_word_len += koreanLen(text_split[word_index + j])\n","      word_len += len(text_split[word_index + j])\n","    news_body_start.append(korean_word_len / word_len)\n","\n","  avg = []\n","  avg_index = []\n","  y = 0\n","  while y < len(text_split) - N:\n","    avg.append(sum(news_body_start[y:y + N]) / N)\n","    avg_index.append(y)\n","    y += N\n","  if len(avg) == 0:\n","    return\n","  max_index = avg.index(max(avg))\n","  avg_avg = sum(avg) / len(avg)\n","  y = 0\n","  start = max_index\n","  end = max_index\n","  while max_index > y:\n","    if avg[max_index - y] > avg_avg:\n","      start = max_index - y\n","    else:\n","      break\n","    y += 1\n","  y = 0\n","  while (y + max_index) < len(avg):\n","    if avg[max_index + y] > avg_avg:\n","      end = max_index + y\n","    else:\n","      break\n","    y += 1\n","  return ' '.join(text_split[avg_index[start] - N : avg_index[end] + N])\n","\n","def morpheme_separation(sentence):\n","    mecab = Mecab()\n","    split_sentence=[]\n","    temp_X = mecab.pos(sentence)\n","    # http://openuiz.blogspot.com/2016/07/mecab-ko-dic.html\n","    for i in temp_X:\n","        if i[1]=='NNG' or i[1]=='NNP' or i[1]=='VV' or i[1]=='VX' or i[1]=='VA':\n","                if i[1]=='VA':#형용사 일때 '다'를 붙여줌 ex) 넓 + 다\n","                    split_sentence.append(i[0]+'다')\n","                else:\n","                    split_sentence.append(i[0])\n","    return split_sentence"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aQiQ7xx3SbBu"},"outputs":[],"source":["news_keywords = [ '출시',  '선보일', '상승한', '상승했다'  '신모델을' , '기술이다', '신제품에서는' , '제공하는' , '탑재하고' , '탑재했다' , '늘어나고' , '출시한' , '출시했다' , '강력한' , '출시된다' , '혁신을' , '제공하며' , '완성할' , '풍부해진' , '순매수했다' , '최적화된' , '고급스러운' , '출고가는' , '기록했다' , '돋보이는' , '늘었다' , '확대' , '압도적인' , '성능에' , '증가했다' , '마감했다' , '최고의' , '결과물이다' , '달성했다' , '올랐다' , '특징이다' , '팔아치웠다' , '예상된다' , '신제품들을' , '강세' , '이뤘다' , '실적이' , '분석했다' , '집계됐다' , '수준이다' , '완벽한' , '수요가' , '상승에' , '높이는' , '성장을' , '전망이다' , '밝혔습니다' , '차지했다' , '설명이다' , '계약을' , '대표적인' , '꼽힌다' , '기반으로' , '완화' , '출시했다고' , '가능성' , '제공한다' , '지속적으로' , '두드러졌다' , '가능하다' , '콘텐츠를' , '활용한' ,'높다', '줄었다' , '전략' , '영향으로' , '기록하며' ]"]},{"cell_type":"markdown","metadata":{"id":"RgaJdfOzvfAl"},"source":["## 긍정, 부정 키워드 선형회귀 데이터 구축"]},{"cell_type":"code","execution_count":76,"metadata":{"id":"RZHeW-Stv_pr","executionInfo":{"status":"ok","timestamp":1652089680966,"user_tz":-540,"elapsed":405,"user":{"displayName":"Ha Jun","userId":"06515366270624202236"}}},"outputs":[],"source":["positive_keyword = ['개선', '증가', '확대', '흑자', '호재']\n","negative_keyword = ['인플레이션', '스테그플레이션','역성장', '축소', '악재', '하락']"]},{"cell_type":"code","execution_count":77,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":77786,"status":"ok","timestamp":1652089809974,"user":{"displayName":"Ha Jun","userId":"06515366270624202236"},"user_tz":-540},"id":"npYZlepQLbe6","outputId":"5efdc243-df59-40ae-a8bc-bb06fe6116ea"},"outputs":[{"output_type":"stream","name":"stdout","text":["[2022-05-09 09:48:53.422721] Url Request Success\n","202203091030 삼성전자\n","40 건\n","[2022-05-09 09:48:54.171695] Url Request Success\n","202203091030 LG에너지솔루션\n","40 건\n","[2022-05-09 09:48:54.933762] Url Request Success\n","202203091030 SK하이닉스\n","40 건\n","[2022-05-09 09:48:55.650489] Url Request Success\n","202203091030 NAVER\n","40 건\n","[2022-05-09 09:48:56.339501] Url Request Success\n","202203091030 삼성바이오로직스\n","40 건\n","[2022-05-09 09:48:57.374062] Url Request Success\n","202203091030 삼성전자우\n","40 건\n","[2022-05-09 09:48:58.148685] Url Request Success\n","202203091030 카카오\n","40 건\n","[2022-05-09 09:48:59.139302] Url Request Success\n","202203091030 삼성SDI\n","40 건\n","[2022-05-09 09:48:59.843038] Url Request Success\n","202203091030 현대차\n","40 건\n","[2022-05-09 09:49:00.574166] Url Request Success\n","202203091030 LG화학\n","40 건\n","[2022-05-09 09:49:01.343980] Url Request Success\n","202203091030 기아\n","40 건\n","[2022-05-09 09:49:02.086619] Url Request Success\n","202203091030 POSCO홀딩스\n","40 건\n","[2022-05-09 09:49:02.820557] Url Request Success\n","202203091030 KB금융\n","40 건\n","[2022-05-09 09:49:03.542800] Url Request Success\n","202203091030 카카오뱅크\n","40 건\n","[2022-05-09 09:49:04.265426] Url Request Success\n","202203091030 셀트리온\n","40 건\n","[2022-05-09 09:49:04.977575] Url Request Success\n","202203091030 신한지주\n","40 건\n","[2022-05-09 09:49:05.743826] Url Request Success\n","202203091030 삼성물산\n","40 건\n","[2022-05-09 09:49:06.473858] Url Request Success\n","202203091030 현대모비스\n","40 건\n","[2022-05-09 09:49:07.213060] Url Request Success\n","202203091030 SK이노베이션\n","40 건\n","[2022-05-09 09:49:07.951009] Url Request Success\n","202203091030 LG전자\n","40 건\n","[2022-05-09 09:49:08.681721] Url Request Success\n","202203091030 카카오페이\n","40 건\n","[2022-05-09 09:49:09.457153] Url Request Success\n","202203091030 SK\n","40 건\n","[2022-05-09 09:49:10.213624] Url Request Success\n","202203091030 하나금융지주\n","40 건\n","[2022-05-09 09:49:11.039699] Url Request Success\n","202203091030 한국전력\n","40 건\n","[2022-05-09 09:49:11.784549] Url Request Success\n","202203091030 HMM\n","40 건\n","[2022-05-09 09:49:12.804589] Url Request Success\n","202203091030 크래프톤\n","40 건\n","[2022-05-09 09:49:13.801507] Url Request Success\n","202203091030 LG생활건강\n","40 건\n","[2022-05-09 09:49:14.545643] Url Request Success\n","202203091030 삼성생명\n","40 건\n","[2022-05-09 09:49:15.223608] Url Request Success\n","202203091030 하이브\n","40 건\n","[2022-05-09 09:49:16.276004] Url Request Success\n","202203091030 SK텔레콤\n","40 건\n","[2022-05-09 09:49:17.055410] Url Request Success\n","202203091030 두산중공업\n","40 건\n","[2022-05-09 09:49:17.782210] Url Request Success\n","202203091030 삼성전기\n","40 건\n","[2022-05-09 09:49:18.464972] Url Request Success\n","202203091030 SK바이오사이언스\n","40 건\n","[2022-05-09 09:49:19.222699] Url Request Success\n","202203091030 LG\n","40 건\n","[2022-05-09 09:49:20.257784] Url Request Success\n","202203091030 우리금융지주\n","40 건\n","[2022-05-09 09:49:20.959255] Url Request Success\n","202203091030 KT&G\n","40 건\n","[2022-05-09 09:49:21.705001] Url Request Success\n","202203091030 고려아연\n","40 건\n","[2022-05-09 09:49:22.454923] Url Request Success\n","202203091030 S-Oil\n","40 건\n","[2022-05-09 09:49:23.293502] Url Request Success\n","202203091030 삼성에스디에스\n","40 건\n","[2022-05-09 09:49:24.036039] Url Request Success\n","202203091030 현대중공업\n","40 건\n","[2022-05-09 09:49:24.776522] Url Request Success\n","202203091030 대한항공\n","40 건\n","[2022-05-09 09:49:25.607554] Url Request Success\n","202203091030 삼성화재\n","40 건\n","[2022-05-09 09:49:26.345765] Url Request Success\n","202203091030 엔씨소프트\n","40 건\n","[2022-05-09 09:49:27.052330] Url Request Success\n","202203091030 넷마블\n","40 건\n","[2022-05-09 09:49:27.778239] Url Request Success\n","202203091030 아모레퍼시픽\n","40 건\n","[2022-05-09 09:49:28.487287] Url Request Success\n","202203091030 포스코케미칼\n","40 건\n","[2022-05-09 09:49:29.267113] Url Request Success\n","202203091030 KT\n","40 건\n","[2022-05-09 09:49:30.012304] Url Request Success\n","202203091030 LG이노텍\n","40 건\n","[2022-05-09 09:49:30.735971] Url Request Success\n","202203091030 SK아이이테크놀로지\n","40 건\n","[2022-05-09 09:49:31.501729] Url Request Success\n","202203091030 기업은행\n","40 건\n","[2022-05-09 09:49:32.253494] Url Request Success\n","202203091030 SK스퀘어\n","40 건\n","[2022-05-09 09:49:33.010463] Url Request Success\n","202203091030 LG디스플레이\n","40 건\n","[2022-05-09 09:49:33.678426] Url Request Success\n","202203091030 현대글로비스\n","40 건\n","[2022-05-09 09:49:34.377335] Url Request Success\n","202203091030 롯데케미칼\n","40 건\n","[2022-05-09 09:49:35.059556] Url Request Success\n","202203091030 SK바이오팜\n","40 건\n","[2022-05-09 09:49:35.820198] Url Request Success\n","202203091030 한화솔루션\n","40 건\n","[2022-05-09 09:49:36.544780] Url Request Success\n","202203091030 한온시스템\n","40 건\n","[2022-05-09 09:49:37.269282] Url Request Success\n","202203091030 한국조선해양\n","40 건\n","[2022-05-09 09:49:38.026003] Url Request Success\n","202203091030 LG유플러스\n","40 건\n","[2022-05-09 09:49:38.761146] Url Request Success\n","202203091030 강원랜드\n","40 건\n","[2022-05-09 09:49:39.482965] Url Request Success\n","202203091030 SKC\n","40 건\n","[2022-05-09 09:49:40.292275] Url Request Success\n","202203091030 에스디바이오센서\n","40 건\n","[2022-05-09 09:49:41.305157] Url Request Success\n","202203091030 메리츠화재\n","40 건\n","[2022-05-09 09:49:42.257142] Url Request Success\n","202203091030 F&F\n","40 건\n","[2022-05-09 09:49:42.980436] Url Request Success\n","202203091030 CJ제일제당\n","40 건\n","[2022-05-09 09:49:43.687215] Url Request Success\n","202203091030 맥쿼리인프라\n","40 건\n","[2022-05-09 09:49:44.430168] Url Request Success\n","202203091030 현대제철\n","40 건\n","[2022-05-09 09:49:45.210068] Url Request Success\n","202203091030 현대건설\n","40 건\n","[2022-05-09 09:49:46.005063] Url Request Success\n","202203091030 메리츠금융지주\n","40 건\n","[2022-05-09 09:49:46.745722] Url Request Success\n","202203091030 미래에셋증권\n","40 건\n","[2022-05-09 09:49:47.797485] Url Request Success\n","202203091030 삼성엔지니어링\n","40 건\n","[2022-05-09 09:49:48.551344] Url Request Success\n","202203091030 코웨이\n","40 건\n","[2022-05-09 09:49:49.324800] Url Request Success\n","202203091030 삼성중공업\n","40 건\n","[2022-05-09 09:49:50.020256] Url Request Success\n","202203091030 DB손해보험\n","40 건\n","[2022-05-09 09:49:50.794775] Url Request Success\n","202203091030 금호석유\n","40 건\n","[2022-05-09 09:49:51.477476] Url Request Success\n","202203091030 메리츠증권\n","40 건\n","[2022-05-09 09:49:52.197788] Url Request Success\n","202203091030 일진머티리얼즈\n","40 건\n","[2022-05-09 09:49:53.413989] Url Request Success\n","202203091030 한국금융지주\n","40 건\n","[2022-05-09 09:49:54.153406] Url Request Success\n","202203091030 유한양행\n","40 건\n","[2022-05-09 09:49:54.923672] Url Request Success\n","202203091030 현대중공업지주\n","40 건\n","[2022-05-09 09:49:55.639726] Url Request Success\n","202203091030 한국타이어앤테크놀로지\n","40 건\n","[2022-05-09 09:49:56.316957] Url Request Success\n","202203091030 쌍용C&E\n","40 건\n","[2022-05-09 09:49:57.036656] Url Request Success\n","202203091030 한진칼\n","40 건\n","[2022-05-09 09:49:57.741029] Url Request Success\n","202203091030 한국항공우주\n","40 건\n","[2022-05-09 09:49:58.454807] Url Request Success\n","202203091030 GS\n","40 건\n","[2022-05-09 09:49:59.199411] Url Request Success\n","202203091030 GS건설\n","40 건\n","[2022-05-09 09:49:59.932325] Url Request Success\n","202203091030 이마트\n","40 건\n","[2022-05-09 09:50:00.725292] Url Request Success\n","202203091030 두산밥캣\n","40 건\n","[2022-05-09 09:50:01.503667] Url Request Success\n","202203091030 NH투자증권\n","40 건\n","[2022-05-09 09:50:02.531068] Url Request Success\n","202203091030 삼성카드\n","40 건\n","[2022-05-09 09:50:03.293434] Url Request Success\n","202203091030 삼성증권\n","40 건\n","[2022-05-09 09:50:03.973288] Url Request Success\n","202203091030 팬오션\n","40 건\n","[2022-05-09 09:50:04.981033] Url Request Success\n","202203091030 한국가스공사\n","40 건\n","[2022-05-09 09:50:05.736080] Url Request Success\n","202203091030 아모레G\n","40 건\n","[2022-05-09 09:50:06.515238] Url Request Success\n","202203091030 오리온\n","40 건\n","[2022-05-09 09:50:07.267564] Url Request Success\n","202203091030 롯데지주\n","40 건\n","[2022-05-09 09:50:07.981515] Url Request Success\n","202203091030 한미약품\n","40 건\n","[2022-05-09 09:50:08.679052] Url Request Success\n","202203091030 DB하이텍\n","40 건\n","[2022-05-09 09:50:09.460902] Url Request Success\n","202203091030 현대오토에버\n","40 건\n","[2022-05-09 09:50:10.203000] Url Request Success\n","202203091030 한전기술\n","40 건\n"]}],"source":["#top100 = ['제주항공','아시아나항공', '대한항공', '진에어']\n","top100 = getTop100()\n","news = getDayNews('202203091030', top100, 40, False, False) #(날짜, 검색 기업 리스트, 기업 당 검색 건수, 저장여부, 지정날짜의 기사만 검색할 지)"]},{"cell_type":"code","execution_count":78,"metadata":{"id":"QGIRbmXavrwD","executionInfo":{"status":"ok","timestamp":1652089809976,"user_tz":-540,"elapsed":17,"user":{"displayName":"Ha Jun","userId":"06515366270624202236"}}},"outputs":[],"source":["def getKeywordCount(news_list, com_list, keyword_list):\n","  com_list = list(com_list)\n","  df_list = []\n","  columns = ['날짜'] + keyword_list\n","  for com_index, com_news in enumerate(news_list):\n","    \n","    count_df = pd.DataFrame(columns=columns)\n","    for one_news in com_news:\n","      news_body = getNewsBody(one_news['link'])\n","      if news_body is None:\n","        continue\n","      keyword_str = ' '.join(morpheme_separation(' '.join(getKorean(news_body))))\n","      \n","      keyword_cnt = []\n","      for keyword in keyword_list:\n","        cnt = keyword_str.count(keyword)\n","        keyword_cnt.append(cnt)\n","      day = ''.join(one_news['pDate'].split(' ')[0].split('-'))\n","      row = [int(day)] + keyword_cnt\n","      \n","      count_df = count_df.append(pd.Series(row, index = count_df.columns), ignore_index=True) \n","    df_list.append(count_df.groupby('날짜').sum().rename_axis('날짜').reset_index())\n","  return df_list"]},{"cell_type":"code","execution_count":79,"metadata":{"id":"uIa_9ejFyh3L","executionInfo":{"status":"ok","timestamp":1652095248895,"user_tz":-540,"elapsed":5438931,"user":{"displayName":"Ha Jun","userId":"06515366270624202236"}}},"outputs":[],"source":["keyword = positive_keyword + negative_keyword\n","count_df_list = getKeywordCount(news, top100, keyword)"]},{"cell_type":"code","execution_count":80,"metadata":{"id":"ukkSd4QoX-Zc","executionInfo":{"status":"ok","timestamp":1652095248901,"user_tz":-540,"elapsed":44,"user":{"displayName":"Ha Jun","userId":"06515366270624202236"}}},"outputs":[],"source":["def join(top100, count_df_list, save = False):\n","  data_frame_list = []\n","  for com_index, com in enumerate(top100):\n","    data_path = os.path.join(com_data_dir, com + '.csv')\n","    if os.path.isfile(data_path) is False:\n","      continue\n","    data_frame_com = pd.read_csv(data_path, encoding = 'UTF-8')\n","    data_frame = pd.merge(data_frame_com, count_df_list[com_index], how = 'left')\n","    data_frame_list.append(data_frame)\n","    if save is True:\n","      data_frame.to_csv(os.path.join(regression_data_dir, com) + '.csv')\n","  return data_frame_list\n","    "]},{"cell_type":"code","execution_count":81,"metadata":{"id":"YW3xsfXELoWK","executionInfo":{"status":"ok","timestamp":1652095289922,"user_tz":-540,"elapsed":41053,"user":{"displayName":"Ha Jun","userId":"06515366270624202236"}}},"outputs":[],"source":["regression_df_list = join(top100, count_df_list, save = False)"]},{"cell_type":"code","execution_count":82,"metadata":{"id":"0cUwCfrfZ9pb","executionInfo":{"status":"ok","timestamp":1652095289924,"user_tz":-540,"elapsed":79,"user":{"displayName":"Ha Jun","userId":"06515366270624202236"}}},"outputs":[],"source":["regression_df = pd.DataFrame(columns = regression_df_list[0].columns)\n","for com_index, com_df in enumerate(regression_df_list):\n","  regression_df = regression_df.append(com_df.dropna())"]},{"cell_type":"code","execution_count":83,"metadata":{"id":"5SzP1TyQpSfu","executionInfo":{"status":"ok","timestamp":1652095289926,"user_tz":-540,"elapsed":75,"user":{"displayName":"Ha Jun","userId":"06515366270624202236"}}},"outputs":[],"source":["if not os.path.exists(com_data_dir):\n","  os.makedirs(com_data_dir)\n","regression_df.drop(columns = ['Unnamed: 0'], inplace = True)\n","regression_df.to_csv(os.path.join(regression_data_dir, 'regression_data.csv'))"]},{"cell_type":"code","execution_count":84,"metadata":{"id":"vqVwt8q_rMiM","colab":{"base_uri":"https://localhost:8080/","height":467},"executionInfo":{"status":"ok","timestamp":1652095289929,"user_tz":-540,"elapsed":75,"user":{"displayName":"Ha Jun","userId":"06515366270624202236"}},"outputId":"e8c5075e-81b3-4753-8cd7-419313dd378a"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["      종목코드       종목명      종가     대비   등락률      시가      고가      저가      거래량  \\\n","84    5930      삼성전자   66500      0  0.00   66300   66900   66100  5241105   \n","65  373220  LG에너지솔루션  396000  -5500 -1.37  397000  400000  395500    72307   \n","84     660    SK하이닉스  108500   1000  0.93  107000  108500  106500   637886   \n","84   35420     NAVER  274500   2500  0.92  269500  276500  269000   211469   \n","84  207940  삼성바이오로직스  795000   1000  0.13  795000  800000  794000     7953   \n","..     ...       ...     ...    ...   ...     ...     ...     ...      ...   \n","83     990     DB하이텍   66200  -1700 -2.50   66700   67200   65700   646918   \n","84     990     DB하이텍   64200  -2000 -3.02   65400   66300   64200   202924   \n","83  307950    현대오토에버  134500  -1000 -0.74  133000  136000  133000    35725   \n","84  307950    현대오토에버  127500  -7000 -5.20  134500  134500  127000    50725   \n","84   52690      한전기술   84600   -400 -0.47   85100   85600   83800   160366   \n","\n","            거래대금  ...    증가    확대   흑자   호재  인플레이션  스테그플레이션  역성장   축소   악재  \\\n","84  348846284300  ...  18.0  30.0  4.0  1.0    3.0      0.0  0.0  0.0  1.0   \n","65   28742351500  ...   5.0  10.0  0.0  0.0    9.0      0.0  0.0  0.0  1.0   \n","84   68776512500  ...   8.0  15.0  3.0  1.0    6.0      0.0  0.0  4.0  3.0   \n","84   57801686620  ...   4.0  19.0  0.0  0.0   10.0      0.0  0.0  0.0  3.0   \n","84    6334548000  ...   6.0  24.0  0.0  0.0   17.0      0.0  0.0  0.0  4.0   \n","..           ...  ...   ...   ...  ...  ...    ...      ...  ...  ...  ...   \n","83   42851723700  ...   0.0   0.0  0.0  0.0    0.0      0.0  0.0  0.0  0.0   \n","84   13217078400  ...   4.0   5.0  1.0  0.0    1.0      0.0  0.0  1.0  0.0   \n","83    4812101500  ...  21.0  14.0  0.0  0.0    0.0      0.0  0.0  1.0  1.0   \n","84    6614318500  ...   2.0   5.0  0.0  0.0    1.0      0.0  0.0  1.0  0.0   \n","84   13602816600  ...   3.0  26.0  0.0  0.0    0.0      0.0  0.0  4.0  0.0   \n","\n","       하락  \n","84   25.0  \n","65   37.0  \n","84   67.0  \n","84   83.0  \n","84  134.0  \n","..    ...  \n","83   13.0  \n","84    0.0  \n","83    3.0  \n","84    7.0  \n","84    6.0  \n","\n","[185 rows x 24 columns]"],"text/html":["\n","  <div id=\"df-df4f4bda-b936-43d6-8836-1bcff00656bf\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>종목코드</th>\n","      <th>종목명</th>\n","      <th>종가</th>\n","      <th>대비</th>\n","      <th>등락률</th>\n","      <th>시가</th>\n","      <th>고가</th>\n","      <th>저가</th>\n","      <th>거래량</th>\n","      <th>거래대금</th>\n","      <th>...</th>\n","      <th>증가</th>\n","      <th>확대</th>\n","      <th>흑자</th>\n","      <th>호재</th>\n","      <th>인플레이션</th>\n","      <th>스테그플레이션</th>\n","      <th>역성장</th>\n","      <th>축소</th>\n","      <th>악재</th>\n","      <th>하락</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>84</th>\n","      <td>5930</td>\n","      <td>삼성전자</td>\n","      <td>66500</td>\n","      <td>0</td>\n","      <td>0.00</td>\n","      <td>66300</td>\n","      <td>66900</td>\n","      <td>66100</td>\n","      <td>5241105</td>\n","      <td>348846284300</td>\n","      <td>...</td>\n","      <td>18.0</td>\n","      <td>30.0</td>\n","      <td>4.0</td>\n","      <td>1.0</td>\n","      <td>3.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>25.0</td>\n","    </tr>\n","    <tr>\n","      <th>65</th>\n","      <td>373220</td>\n","      <td>LG에너지솔루션</td>\n","      <td>396000</td>\n","      <td>-5500</td>\n","      <td>-1.37</td>\n","      <td>397000</td>\n","      <td>400000</td>\n","      <td>395500</td>\n","      <td>72307</td>\n","      <td>28742351500</td>\n","      <td>...</td>\n","      <td>5.0</td>\n","      <td>10.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>9.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>37.0</td>\n","    </tr>\n","    <tr>\n","      <th>84</th>\n","      <td>660</td>\n","      <td>SK하이닉스</td>\n","      <td>108500</td>\n","      <td>1000</td>\n","      <td>0.93</td>\n","      <td>107000</td>\n","      <td>108500</td>\n","      <td>106500</td>\n","      <td>637886</td>\n","      <td>68776512500</td>\n","      <td>...</td>\n","      <td>8.0</td>\n","      <td>15.0</td>\n","      <td>3.0</td>\n","      <td>1.0</td>\n","      <td>6.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>4.0</td>\n","      <td>3.0</td>\n","      <td>67.0</td>\n","    </tr>\n","    <tr>\n","      <th>84</th>\n","      <td>35420</td>\n","      <td>NAVER</td>\n","      <td>274500</td>\n","      <td>2500</td>\n","      <td>0.92</td>\n","      <td>269500</td>\n","      <td>276500</td>\n","      <td>269000</td>\n","      <td>211469</td>\n","      <td>57801686620</td>\n","      <td>...</td>\n","      <td>4.0</td>\n","      <td>19.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>10.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>3.0</td>\n","      <td>83.0</td>\n","    </tr>\n","    <tr>\n","      <th>84</th>\n","      <td>207940</td>\n","      <td>삼성바이오로직스</td>\n","      <td>795000</td>\n","      <td>1000</td>\n","      <td>0.13</td>\n","      <td>795000</td>\n","      <td>800000</td>\n","      <td>794000</td>\n","      <td>7953</td>\n","      <td>6334548000</td>\n","      <td>...</td>\n","      <td>6.0</td>\n","      <td>24.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>17.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>4.0</td>\n","      <td>134.0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>83</th>\n","      <td>990</td>\n","      <td>DB하이텍</td>\n","      <td>66200</td>\n","      <td>-1700</td>\n","      <td>-2.50</td>\n","      <td>66700</td>\n","      <td>67200</td>\n","      <td>65700</td>\n","      <td>646918</td>\n","      <td>42851723700</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>13.0</td>\n","    </tr>\n","    <tr>\n","      <th>84</th>\n","      <td>990</td>\n","      <td>DB하이텍</td>\n","      <td>64200</td>\n","      <td>-2000</td>\n","      <td>-3.02</td>\n","      <td>65400</td>\n","      <td>66300</td>\n","      <td>64200</td>\n","      <td>202924</td>\n","      <td>13217078400</td>\n","      <td>...</td>\n","      <td>4.0</td>\n","      <td>5.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>83</th>\n","      <td>307950</td>\n","      <td>현대오토에버</td>\n","      <td>134500</td>\n","      <td>-1000</td>\n","      <td>-0.74</td>\n","      <td>133000</td>\n","      <td>136000</td>\n","      <td>133000</td>\n","      <td>35725</td>\n","      <td>4812101500</td>\n","      <td>...</td>\n","      <td>21.0</td>\n","      <td>14.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>3.0</td>\n","    </tr>\n","    <tr>\n","      <th>84</th>\n","      <td>307950</td>\n","      <td>현대오토에버</td>\n","      <td>127500</td>\n","      <td>-7000</td>\n","      <td>-5.20</td>\n","      <td>134500</td>\n","      <td>134500</td>\n","      <td>127000</td>\n","      <td>50725</td>\n","      <td>6614318500</td>\n","      <td>...</td>\n","      <td>2.0</td>\n","      <td>5.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>7.0</td>\n","    </tr>\n","    <tr>\n","      <th>84</th>\n","      <td>52690</td>\n","      <td>한전기술</td>\n","      <td>84600</td>\n","      <td>-400</td>\n","      <td>-0.47</td>\n","      <td>85100</td>\n","      <td>85600</td>\n","      <td>83800</td>\n","      <td>160366</td>\n","      <td>13602816600</td>\n","      <td>...</td>\n","      <td>3.0</td>\n","      <td>26.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>4.0</td>\n","      <td>0.0</td>\n","      <td>6.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>185 rows × 24 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-df4f4bda-b936-43d6-8836-1bcff00656bf')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-df4f4bda-b936-43d6-8836-1bcff00656bf button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-df4f4bda-b936-43d6-8836-1bcff00656bf');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":84}],"source":["regression_df"]},{"cell_type":"markdown","metadata":{"id":"dzuj-mpcMsbV"},"source":["##선형 회귀 모델 학습"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Mie7JN-mMwGM","executionInfo":{"status":"ok","timestamp":1652074614665,"user_tz":-540,"elapsed":7379,"user":{"displayName":"Ha Jun","userId":"06515366270624202236"}},"outputId":"c0fd449f-9736-41c8-c6c0-d923c23eda20"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"]}],"source":["import os\n","import pandas as pd\n","import numpy as np\n","from google.colab import drive\n","import torch\n","\n","drive.mount('/gdrive')"]},{"cell_type":"code","source":["import torch"],"metadata":{"id":"Ks-AhKcfOeKU","executionInfo":{"status":"ok","timestamp":1652095291763,"user_tz":-540,"elapsed":98,"user":{"displayName":"Ha Jun","userId":"06515366270624202236"}}},"execution_count":85,"outputs":[]},{"cell_type":"code","execution_count":86,"metadata":{"id":"3Z0dot0xM7W1","executionInfo":{"status":"ok","timestamp":1652095291766,"user_tz":-540,"elapsed":97,"user":{"displayName":"Ha Jun","userId":"06515366270624202236"}}},"outputs":[],"source":["workspace_path = '/gdrive/My Drive/주가분석'\n","regression_data_dir = os.path.join(workspace_path, 'regression_data')\n","\n","regression_data_path = os.path.join(regression_data_dir, 'regression_data.csv')"]},{"cell_type":"code","execution_count":87,"metadata":{"id":"LjP2vQmFNQOe","executionInfo":{"status":"ok","timestamp":1652095291770,"user_tz":-540,"elapsed":98,"user":{"displayName":"Ha Jun","userId":"06515366270624202236"}}},"outputs":[],"source":["regression_data = pd.read_csv(regression_data_path, encoding = 'utf-8')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J8uATefBNSCj"},"outputs":[],"source":["regression_data.drop(columns = ['Unnamed: 0'], inplace = True)\n","#regression_data.drop(columns = ['Unnamed: 0.1'], inplace = True)"]},{"cell_type":"code","execution_count":90,"metadata":{"id":"Xu1XSLAaNU5p","colab":{"base_uri":"https://localhost:8080/","height":279},"executionInfo":{"status":"ok","timestamp":1652095314793,"user_tz":-540,"elapsed":299,"user":{"displayName":"Ha Jun","userId":"06515366270624202236"}},"outputId":"8110a130-140f-49e2-c1ce-0ed5738be9ef"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["     종목코드       종목명      종가    대비   등락률      시가      고가      저가      거래량  \\\n","0    5930      삼성전자   66500     0  0.00   66300   66900   66100  5241105   \n","1  373220  LG에너지솔루션  396000 -5500 -1.37  397000  400000  395500    72307   \n","2     660    SK하이닉스  108500  1000  0.93  107000  108500  106500   637886   \n","3   35420     NAVER  274500  2500  0.92  269500  276500  269000   211469   \n","4  207940  삼성바이오로직스  795000  1000  0.13  795000  800000  794000     7953   \n","\n","           거래대금  ...    증가    확대   흑자   호재  인플레이션  스테그플레이션  역성장   축소   악재  \\\n","0  348846284300  ...  18.0  30.0  4.0  1.0    3.0      0.0  0.0  0.0  1.0   \n","1   28742351500  ...   5.0  10.0  0.0  0.0    9.0      0.0  0.0  0.0  1.0   \n","2   68776512500  ...   8.0  15.0  3.0  1.0    6.0      0.0  0.0  4.0  3.0   \n","3   57801686620  ...   4.0  19.0  0.0  0.0   10.0      0.0  0.0  0.0  3.0   \n","4    6334548000  ...   6.0  24.0  0.0  0.0   17.0      0.0  0.0  0.0  4.0   \n","\n","      하락  \n","0   25.0  \n","1   37.0  \n","2   67.0  \n","3   83.0  \n","4  134.0  \n","\n","[5 rows x 24 columns]"],"text/html":["\n","  <div id=\"df-8e7df34e-8ac6-44b7-8868-4884112adc11\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>종목코드</th>\n","      <th>종목명</th>\n","      <th>종가</th>\n","      <th>대비</th>\n","      <th>등락률</th>\n","      <th>시가</th>\n","      <th>고가</th>\n","      <th>저가</th>\n","      <th>거래량</th>\n","      <th>거래대금</th>\n","      <th>...</th>\n","      <th>증가</th>\n","      <th>확대</th>\n","      <th>흑자</th>\n","      <th>호재</th>\n","      <th>인플레이션</th>\n","      <th>스테그플레이션</th>\n","      <th>역성장</th>\n","      <th>축소</th>\n","      <th>악재</th>\n","      <th>하락</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>5930</td>\n","      <td>삼성전자</td>\n","      <td>66500</td>\n","      <td>0</td>\n","      <td>0.00</td>\n","      <td>66300</td>\n","      <td>66900</td>\n","      <td>66100</td>\n","      <td>5241105</td>\n","      <td>348846284300</td>\n","      <td>...</td>\n","      <td>18.0</td>\n","      <td>30.0</td>\n","      <td>4.0</td>\n","      <td>1.0</td>\n","      <td>3.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>25.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>373220</td>\n","      <td>LG에너지솔루션</td>\n","      <td>396000</td>\n","      <td>-5500</td>\n","      <td>-1.37</td>\n","      <td>397000</td>\n","      <td>400000</td>\n","      <td>395500</td>\n","      <td>72307</td>\n","      <td>28742351500</td>\n","      <td>...</td>\n","      <td>5.0</td>\n","      <td>10.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>9.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>37.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>660</td>\n","      <td>SK하이닉스</td>\n","      <td>108500</td>\n","      <td>1000</td>\n","      <td>0.93</td>\n","      <td>107000</td>\n","      <td>108500</td>\n","      <td>106500</td>\n","      <td>637886</td>\n","      <td>68776512500</td>\n","      <td>...</td>\n","      <td>8.0</td>\n","      <td>15.0</td>\n","      <td>3.0</td>\n","      <td>1.0</td>\n","      <td>6.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>4.0</td>\n","      <td>3.0</td>\n","      <td>67.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>35420</td>\n","      <td>NAVER</td>\n","      <td>274500</td>\n","      <td>2500</td>\n","      <td>0.92</td>\n","      <td>269500</td>\n","      <td>276500</td>\n","      <td>269000</td>\n","      <td>211469</td>\n","      <td>57801686620</td>\n","      <td>...</td>\n","      <td>4.0</td>\n","      <td>19.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>10.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>3.0</td>\n","      <td>83.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>207940</td>\n","      <td>삼성바이오로직스</td>\n","      <td>795000</td>\n","      <td>1000</td>\n","      <td>0.13</td>\n","      <td>795000</td>\n","      <td>800000</td>\n","      <td>794000</td>\n","      <td>7953</td>\n","      <td>6334548000</td>\n","      <td>...</td>\n","      <td>6.0</td>\n","      <td>24.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>17.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>4.0</td>\n","      <td>134.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 24 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8e7df34e-8ac6-44b7-8868-4884112adc11')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-8e7df34e-8ac6-44b7-8868-4884112adc11 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-8e7df34e-8ac6-44b7-8868-4884112adc11');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":90}],"source":["regression_data.head()"]},{"cell_type":"code","execution_count":91,"metadata":{"id":"qzEwyQCkNVqV","executionInfo":{"status":"ok","timestamp":1652095322462,"user_tz":-540,"elapsed":1350,"user":{"displayName":"Ha Jun","userId":"06515366270624202236"}}},"outputs":[],"source":["regression_data.drop(columns = ['종목코드', '종가', '대비', '시가', '고가', '거래량', '거래대금','시가총액', '상장주식수'], inplace = True)"]},{"cell_type":"code","execution_count":92,"metadata":{"id":"uOoPdtx_NXyT","colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"status":"ok","timestamp":1652095322760,"user_tz":-540,"elapsed":21,"user":{"displayName":"Ha Jun","userId":"06515366270624202236"}},"outputId":"09a80c75-63db-492b-f2e3-f5e12798202a"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["        종목명   등락률      저가        날짜   개선    증가    확대   흑자   호재  인플레이션  \\\n","0      삼성전자  0.00   66100  20220509  7.0  18.0  30.0  4.0  1.0    3.0   \n","1  LG에너지솔루션 -1.37  395500  20220509  0.0   5.0  10.0  0.0  0.0    9.0   \n","2    SK하이닉스  0.93  106500  20220509  3.0   8.0  15.0  3.0  1.0    6.0   \n","3     NAVER  0.92  269000  20220509  6.0   4.0  19.0  0.0  0.0   10.0   \n","4  삼성바이오로직스  0.13  794000  20220509  2.0   6.0  24.0  0.0  0.0   17.0   \n","\n","   스테그플레이션  역성장   축소   악재     하락  \n","0      0.0  0.0  0.0  1.0   25.0  \n","1      0.0  0.0  0.0  1.0   37.0  \n","2      0.0  0.0  4.0  3.0   67.0  \n","3      0.0  0.0  0.0  3.0   83.0  \n","4      0.0  0.0  0.0  4.0  134.0  "],"text/html":["\n","  <div id=\"df-6688ac08-051a-4a19-adf7-093d4ce4b4f2\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>종목명</th>\n","      <th>등락률</th>\n","      <th>저가</th>\n","      <th>날짜</th>\n","      <th>개선</th>\n","      <th>증가</th>\n","      <th>확대</th>\n","      <th>흑자</th>\n","      <th>호재</th>\n","      <th>인플레이션</th>\n","      <th>스테그플레이션</th>\n","      <th>역성장</th>\n","      <th>축소</th>\n","      <th>악재</th>\n","      <th>하락</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>삼성전자</td>\n","      <td>0.00</td>\n","      <td>66100</td>\n","      <td>20220509</td>\n","      <td>7.0</td>\n","      <td>18.0</td>\n","      <td>30.0</td>\n","      <td>4.0</td>\n","      <td>1.0</td>\n","      <td>3.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>25.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>LG에너지솔루션</td>\n","      <td>-1.37</td>\n","      <td>395500</td>\n","      <td>20220509</td>\n","      <td>0.0</td>\n","      <td>5.0</td>\n","      <td>10.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>9.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>37.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>SK하이닉스</td>\n","      <td>0.93</td>\n","      <td>106500</td>\n","      <td>20220509</td>\n","      <td>3.0</td>\n","      <td>8.0</td>\n","      <td>15.0</td>\n","      <td>3.0</td>\n","      <td>1.0</td>\n","      <td>6.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>4.0</td>\n","      <td>3.0</td>\n","      <td>67.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>NAVER</td>\n","      <td>0.92</td>\n","      <td>269000</td>\n","      <td>20220509</td>\n","      <td>6.0</td>\n","      <td>4.0</td>\n","      <td>19.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>10.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>3.0</td>\n","      <td>83.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>삼성바이오로직스</td>\n","      <td>0.13</td>\n","      <td>794000</td>\n","      <td>20220509</td>\n","      <td>2.0</td>\n","      <td>6.0</td>\n","      <td>24.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>17.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>4.0</td>\n","      <td>134.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6688ac08-051a-4a19-adf7-093d4ce4b4f2')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-6688ac08-051a-4a19-adf7-093d4ce4b4f2 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-6688ac08-051a-4a19-adf7-093d4ce4b4f2');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":92}],"source":["regression_data.head()"]},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","train_x, test_x, train_y, test_y = train_test_split(regression_data[regression_data.columns[4:]].to_numpy(),regression_data['등락률'].to_numpy(), test_size=0.2)"],"metadata":{"id":"moepICttPzq2","executionInfo":{"status":"ok","timestamp":1652095326114,"user_tz":-540,"elapsed":312,"user":{"displayName":"Ha Jun","userId":"06515366270624202236"}}},"execution_count":93,"outputs":[]},{"cell_type":"code","source":["class LinearModel(torch.nn.Module):\n","    def __init__(self, input_dim, output_dim):\n","        super(LinearModel, self).__init__()\n","        self.linear_classifier = torch.nn.Linear(input_dim, output_dim, bias=True)\n","        #torch.nn.init.constant_(self.linear_classifier.weight.data, 0)\n","\n","    def forward(self, x):\n","        print(\"inX:\" ,x)\n","        print(\"inSquee\",x.unsqueeze(0))\n","        x = self.linear_classifier(x.unsqueeze(0))\n","        print(\"outX:\" ,x)\n","        return x\n"],"metadata":{"id":"QNz3LUn7P1EK","executionInfo":{"status":"ok","timestamp":1652095329243,"user_tz":-540,"elapsed":9,"user":{"displayName":"Ha Jun","userId":"06515366270624202236"}}},"execution_count":94,"outputs":[]},{"cell_type":"code","source":["input_dim = len(regression_data.columns[4:])\n","output_dim = 1\n","learning_rate = 0.0001\n","model = LinearModel(input_dim, output_dim)\n","\n","optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n","loss_function = torch.nn.MSELoss()"],"metadata":{"id":"FXGIhv-cP2qK","executionInfo":{"status":"ok","timestamp":1652096014806,"user_tz":-540,"elapsed":300,"user":{"displayName":"Ha Jun","userId":"06515366270624202236"}}},"execution_count":118,"outputs":[]},{"cell_type":"code","source":["def train(train_x, train_y, loss_function, optimizer, model):\n","  model.train()\n","  print(model.linear_classifier.weight)\n","  loss_list = []\n","  for idx in range(len(train_x)):\n","    \n","    optimizer.zero_grad()\n","    print(idx)\n","    print(train_x[idx])\n","    train_x_tensor = torch.tensor(train_x[idx]).float() #convert numpy to torch tensor\n","    train_y_tensor = torch.tensor(train_y[idx]).float()\n","    logit = model(train_x_tensor.unsqueeze(0))\n","    print(\"the value of logit\", logit)\n","    loss = loss_function(logit, train_y_tensor)\n","    loss_list.append(loss)\n","    print(\"the value of loss\", loss)\n","\n","    loss.backward()\n","    optimizer.step()\n","\n","  return model, loss_list"],"metadata":{"id":"zVhyBOKyP4FT","executionInfo":{"status":"ok","timestamp":1652095334197,"user_tz":-540,"elapsed":6,"user":{"displayName":"Ha Jun","userId":"06515366270624202236"}}},"execution_count":96,"outputs":[]},{"cell_type":"code","source":["best_model = None\n","best_model, loss_list = train(train_x, train_y, loss_function, optimizer, model)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"imDNsVqQP5v3","executionInfo":{"status":"ok","timestamp":1652096019509,"user_tz":-540,"elapsed":1391,"user":{"displayName":"Ha Jun","userId":"06515366270624202236"}},"outputId":"412077ad-99cb-42fc-d62e-065317413154"},"execution_count":119,"outputs":[{"output_type":"stream","name":"stdout","text":["Parameter containing:\n","tensor([[ 0.2041,  0.0323,  0.2644,  0.2384, -0.1108,  0.2519,  0.1100, -0.2405,\n","         -0.1781, -0.1088,  0.1721]], requires_grad=True)\n","0\n","[ 2.  5.  1.  2.  0.  0.  0.  0.  2.  0. 20.]\n","inX: tensor([[ 2.,  5.,  1.,  2.,  0.,  0.,  0.,  0.,  2.,  0., 20.]])\n","inSquee tensor([[[ 2.,  5.,  1.,  2.,  0.,  0.,  0.,  0.,  2.,  0., 20.]]])\n","outX: tensor([[[4.1315]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[4.1315]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(17.0443, grad_fn=<MseLossBackward0>)\n","1\n","[ 0.  0.  1.  0.  0.  1.  0.  0.  0.  0. 11.]\n","inX: tensor([[ 0.,  0.,  1.,  0.,  0.,  1.,  0.,  0.,  0.,  0., 11.]])\n","inSquee tensor([[[ 0.,  0.,  1.,  0.,  0.,  1.,  0.,  0.,  0.,  0., 11.]]])\n","outX: tensor([[[2.3274]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[2.3274]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(10.3518, grad_fn=<MseLossBackward0>)\n","2\n","[ 1.  0.  5.  0.  0.  0.  0.  0.  1.  0. 19.]\n","inX: tensor([[ 1.,  0.,  5.,  0.,  0.,  0.,  0.,  0.,  1.,  0., 19.]])\n","inSquee tensor([[[ 1.,  0.,  5.,  0.,  0.,  0.,  0.,  0.,  1.,  0., 19.]]])\n","outX: tensor([[[4.5362]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[4.5362]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(8.9774, grad_fn=<MseLossBackward0>)\n","3\n","[4. 3. 2. 0. 1. 0. 0. 0. 0. 0. 0.]\n","inX: tensor([[4., 3., 2., 0., 1., 0., 0., 0., 0., 0., 0.]])\n","inSquee tensor([[[4., 3., 2., 0., 1., 0., 0., 0., 0., 0., 0.]]])\n","outX: tensor([[[1.0764]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[1.0764]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(1.5535, grad_fn=<MseLossBackward0>)\n","4\n","[26.  7.  6.  0.  0.  0.  0.  0.  0.  0.  2.]\n","inX: tensor([[26.,  7.,  6.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  2.]])\n","inSquee tensor([[[26.,  7.,  6.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  2.]]])\n","outX: tensor([[[7.1977]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[7.1977]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(55.3196, grad_fn=<MseLossBackward0>)\n","5\n","[ 2.  7.  7.  0.  0.  1.  0.  0.  1.  2. 34.]\n","inX: tensor([[ 2.,  7.,  7.,  0.,  0.,  1.,  0.,  0.,  1.,  2., 34.]])\n","inSquee tensor([[[ 2.,  7.,  7.,  0.,  0.,  1.,  0.,  0.,  1.,  2., 34.]]])\n","outX: tensor([[[7.5461]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[7.5461]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(79.1406, grad_fn=<MseLossBackward0>)\n","6\n","[1. 4. 2. 0. 0. 0. 0. 0. 0. 1. 1.]\n","inX: tensor([[1., 4., 2., 0., 0., 0., 0., 0., 0., 1., 1.]])\n","inSquee tensor([[[1., 4., 2., 0., 0., 0., 0., 0., 0., 1., 1.]]])\n","outX: tensor([[[0.4177]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[0.4177]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(2.3954, grad_fn=<MseLossBackward0>)\n","7\n","[0. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0.]\n","inX: tensor([[0., 0., 2., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inSquee tensor([[[0., 0., 2., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n","outX: tensor([[[0.2087]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[0.2087]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(0.0435, grad_fn=<MseLossBackward0>)\n","8\n","[2. 3. 1. 0. 0. 0. 0. 0. 0. 1. 0.]\n","inX: tensor([[2., 3., 1., 0., 0., 0., 0., 0., 0., 1., 0.]])\n","inSquee tensor([[[2., 3., 1., 0., 0., 0., 0., 0., 0., 1., 0.]]])\n","outX: tensor([[[0.2147]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[0.2147]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(1.1335, grad_fn=<MseLossBackward0>)\n","9\n","[7. 0. 6. 2. 0. 0. 0. 0. 0. 0. 0.]\n","inX: tensor([[7., 0., 6., 2., 0., 0., 0., 0., 0., 0., 0.]])\n","inSquee tensor([[[7., 0., 6., 2., 0., 0., 0., 0., 0., 0., 0.]]])\n","outX: tensor([[[2.7712]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[2.7712]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(18.7591, grad_fn=<MseLossBackward0>)\n","10\n","[ 2. 10. 10.  0.  0.  1.  0.  0.  0.  1.  4.]\n","inX: tensor([[ 2., 10., 10.,  0.,  0.,  1.,  0.,  0.,  0.,  1.,  4.]])\n","inSquee tensor([[[ 2., 10., 10.,  0.,  0.,  1.,  0.,  0.,  0.,  1.,  4.]]])\n","outX: tensor([[[3.0471]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[3.0471]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(18.0383, grad_fn=<MseLossBackward0>)\n","11\n","[1. 9. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","inX: tensor([[1., 9., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inSquee tensor([[[1., 9., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n","outX: tensor([[[-0.0960]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-0.0960]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(3.2546, grad_fn=<MseLossBackward0>)\n","12\n","[ 4.  1. 15.  0.  1.  4.  0.  0.  4. 13. 11.]\n","inX: tensor([[ 4.,  1., 15.,  0.,  1.,  4.,  0.,  0.,  4., 13., 11.]])\n","inSquee tensor([[[ 4.,  1., 15.,  0.,  1.,  4.,  0.,  0.,  4., 13., 11.]]])\n","outX: tensor([[[3.5383]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[3.5383]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(45.9449, grad_fn=<MseLossBackward0>)\n","13\n","[ 2.  7.  2.  2.  3.  1.  0.  0.  1.  0. 18.]\n","inX: tensor([[ 2.,  7.,  2.,  2.,  3.,  1.,  0.,  0.,  1.,  0., 18.]])\n","inSquee tensor([[[ 2.,  7.,  2.,  2.,  3.,  1.,  0.,  0.,  1.,  0., 18.]]])\n","outX: tensor([[[2.2000]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[2.2000]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(26.5223, grad_fn=<MseLossBackward0>)\n","14\n","[ 7.  4. 12.  0.  0.  1.  0.  0.  3.  2. 14.]\n","inX: tensor([[ 7.,  4., 12.,  0.,  0.,  1.,  0.,  0.,  3.,  2., 14.]])\n","inSquee tensor([[[ 7.,  4., 12.,  0.,  0.,  1.,  0.,  0.,  3.,  2., 14.]]])\n","outX: tensor([[[3.5366]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[3.5366]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(29.6653, grad_fn=<MseLossBackward0>)\n","15\n","[ 9. 31. 24.  2.  0.  3.  0.  0.  4.  3. 35.]\n","inX: tensor([[ 9., 31., 24.,  2.,  0.,  3.,  0.,  0.,  4.,  3., 35.]])\n","inSquee tensor([[[ 9., 31., 24.,  2.,  0.,  3.,  0.,  0.,  4.,  3., 35.]]])\n","outX: tensor([[[7.0329]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[7.0329]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(56.4432, grad_fn=<MseLossBackward0>)\n","16\n","[ 0.  1.  2.  0.  0.  1.  0.  0.  1.  0. 23.]\n","inX: tensor([[ 0.,  1.,  2.,  0.,  0.,  1.,  0.,  0.,  1.,  0., 23.]])\n","inSquee tensor([[[ 0.,  1.,  2.,  0.,  0.,  1.,  0.,  0.,  1.,  0., 23.]]])\n","outX: tensor([[[0.0507]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[0.0507]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(1.6111, grad_fn=<MseLossBackward0>)\n","17\n","[1. 2. 2. 1. 0. 0. 0. 0. 0. 0. 1.]\n","inX: tensor([[1., 2., 2., 1., 0., 0., 0., 0., 0., 0., 1.]])\n","inSquee tensor([[[1., 2., 2., 1., 0., 0., 0., 0., 0., 0., 1.]]])\n","outX: tensor([[[0.2767]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[0.2767]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(0.0267, grad_fn=<MseLossBackward0>)\n","18\n","[2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","inX: tensor([[2., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inSquee tensor([[[2., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n","outX: tensor([[[-0.0277]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-0.0277]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(1.0451, grad_fn=<MseLossBackward0>)\n","19\n","[ 0.  1.  5.  0.  1.  3.  0.  0.  0.  1. 49.]\n","inX: tensor([[ 0.,  1.,  5.,  0.,  1.,  3.,  0.,  0.,  0.,  1., 49.]])\n","inSquee tensor([[[ 0.,  1.,  5.,  0.,  1.,  3.,  0.,  0.,  0.,  1., 49.]]])\n","outX: tensor([[[1.2610]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[1.2610]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(9.9285, grad_fn=<MseLossBackward0>)\n","20\n","[ 0.  0.  1.  0.  0.  2.  0.  0.  0.  1. 14.]\n","inX: tensor([[ 0.,  0.,  1.,  0.,  0.,  2.,  0.,  0.,  0.,  1., 14.]])\n","inSquee tensor([[[ 0.,  0.,  1.,  0.,  0.,  2.,  0.,  0.,  0.,  1., 14.]]])\n","outX: tensor([[[-0.1218]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-0.1218]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(7.2267, grad_fn=<MseLossBackward0>)\n","21\n","[0. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0.]\n","inX: tensor([[0., 0., 2., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inSquee tensor([[[0., 0., 2., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n","outX: tensor([[[0.0230]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[0.0230]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(1.4472, grad_fn=<MseLossBackward0>)\n","22\n","[10. 30. 26.  9.  1.  5.  0.  0.  4.  2. 54.]\n","inX: tensor([[10., 30., 26.,  9.,  1.,  5.,  0.,  0.,  4.,  2., 54.]])\n","inSquee tensor([[[10., 30., 26.,  9.,  1.,  5.,  0.,  0.,  4.,  2., 54.]]])\n","outX: tensor([[[3.4750]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[3.4750]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(8.9099, grad_fn=<MseLossBackward0>)\n","23\n","[ 7. 18. 30.  4.  1.  3.  0.  0.  0.  1. 25.]\n","inX: tensor([[ 7., 18., 30.,  4.,  1.,  3.,  0.,  0.,  0.,  1., 25.]])\n","inSquee tensor([[[ 7., 18., 30.,  4.,  1.,  3.,  0.,  0.,  0.,  1., 25.]]])\n","outX: tensor([[[2.9381]]], grad_fn=<AddBackward0>)\n","the value of logit "]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:529: UserWarning: Using a target size (torch.Size([])) that is different to the input size (torch.Size([1, 1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","  return F.mse_loss(input, target, reduction=self.reduction)\n"]},{"output_type":"stream","name":"stdout","text":["tensor([[[2.9381]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(8.6327, grad_fn=<MseLossBackward0>)\n","24\n","[1. 0. 2. 0. 1. 0. 0. 0. 0. 0. 0.]\n","inX: tensor([[1., 0., 2., 0., 1., 0., 0., 0., 0., 0., 0.]])\n","inSquee tensor([[[1., 0., 2., 0., 1., 0., 0., 0., 0., 0., 0.]]])\n","outX: tensor([[[-0.0491]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-0.0491]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(1.9297, grad_fn=<MseLossBackward0>)\n","25\n","[ 5.  7.  6.  0.  0.  1.  0.  0.  1.  1. 19.]\n","inX: tensor([[ 5.,  7.,  6.,  0.,  0.,  1.,  0.,  0.,  1.,  1., 19.]])\n","inSquee tensor([[[ 5.,  7.,  6.,  0.,  0.,  1.,  0.,  0.,  1.,  1., 19.]]])\n","outX: tensor([[[-1.2138]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-1.2138]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(0.3797, grad_fn=<MseLossBackward0>)\n","26\n","[ 2. 30.  7.  0.  2.  1.  0.  0.  1.  0. 18.]\n","inX: tensor([[ 2., 30.,  7.,  0.,  2.,  1.,  0.,  0.,  1.,  0., 18.]])\n","inSquee tensor([[[ 2., 30.,  7.,  0.,  2.,  1.,  0.,  0.,  1.,  0., 18.]]])\n","outX: tensor([[[-3.5781]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-3.5781]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(0.2133, grad_fn=<MseLossBackward0>)\n","27\n","[4. 9. 9. 0. 0. 0. 0. 0. 0. 1. 3.]\n","inX: tensor([[4., 9., 9., 0., 0., 0., 0., 0., 0., 1., 3.]])\n","inSquee tensor([[[4., 9., 9., 0., 0., 0., 0., 0., 0., 1., 3.]]])\n","outX: tensor([[[-0.0026]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-0.0026]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(9.3474, grad_fn=<MseLossBackward0>)\n","28\n","[ 7. 26. 10.  4.  4.  3.  0.  0.  4.  1. 31.]\n","inX: tensor([[ 7., 26., 10.,  4.,  4.,  3.,  0.,  0.,  4.,  1., 31.]])\n","inSquee tensor([[[ 7., 26., 10.,  4.,  4.,  3.,  0.,  0.,  4.,  1., 31.]]])\n","outX: tensor([[[-3.3534]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-3.3534]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(4.2992, grad_fn=<MseLossBackward0>)\n","29\n","[ 1.  1.  3.  0.  0.  1.  0.  0.  0.  0. 18.]\n","inX: tensor([[ 1.,  1.,  3.,  0.,  0.,  1.,  0.,  0.,  0.,  0., 18.]])\n","inSquee tensor([[[ 1.,  1.,  3.,  0.,  0.,  1.,  0.,  0.,  0.,  0., 18.]]])\n","outX: tensor([[[-0.9528]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-0.9528]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(0.5001, grad_fn=<MseLossBackward0>)\n","30\n","[ 6.  3. 11.  0.  0.  6.  0.  0.  0.  1. 23.]\n","inX: tensor([[ 6.,  3., 11.,  0.,  0.,  6.,  0.,  0.,  0.,  1., 23.]])\n","inSquee tensor([[[ 6.,  3., 11.,  0.,  0.,  6.,  0.,  0.,  0.,  1., 23.]]])\n","outX: tensor([[[0.9490]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[0.9490]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(2.8865, grad_fn=<MseLossBackward0>)\n","31\n","[ 3. 12.  0.  2.  2.  0.  0.  0.  0.  1.  6.]\n","inX: tensor([[ 3., 12.,  0.,  2.,  2.,  0.,  0.,  0.,  0.,  1.,  6.]])\n","inSquee tensor([[[ 3., 12.,  0.,  2.,  2.,  0.,  0.,  0.,  0.,  1.,  6.]]])\n","outX: tensor([[[-1.4132]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-1.4132]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(1.5457, grad_fn=<MseLossBackward0>)\n","32\n","[0. 4. 5. 1. 0. 1. 0. 0. 1. 0. 0.]\n","inX: tensor([[0., 4., 5., 1., 0., 1., 0., 0., 1., 0., 0.]])\n","inSquee tensor([[[0., 4., 5., 1., 0., 1., 0., 0., 1., 0., 0.]]])\n","outX: tensor([[[0.1907]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[0.1907]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(10.3089, grad_fn=<MseLossBackward0>)\n","33\n","[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n","inX: tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inSquee tensor([[[0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n","outX: tensor([[[-0.1731]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-0.1731]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(0.0313, grad_fn=<MseLossBackward0>)\n","34\n","[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","inX: tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inSquee tensor([[[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n","outX: tensor([[[-0.3686]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-0.3686]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(0.1359, grad_fn=<MseLossBackward0>)\n","35\n","[ 8. 34. 24.  4.  0.  1.  0.  0.  1.  0. 21.]\n","inX: tensor([[ 8., 34., 24.,  4.,  0.,  1.,  0.,  0.,  1.,  0., 21.]])\n","inSquee tensor([[[ 8., 34., 24.,  4.,  0.,  1.,  0.,  0.,  1.,  0., 21.]]])\n","outX: tensor([[[-0.5355]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-0.5355]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(0.7647, grad_fn=<MseLossBackward0>)\n","36\n","[2. 1. 1. 0. 1. 0. 0. 0. 1. 0. 1.]\n","inX: tensor([[2., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1.]])\n","inSquee tensor([[[2., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1.]]])\n","outX: tensor([[[-0.4439]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-0.4439]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(0.0656, grad_fn=<MseLossBackward0>)\n","37\n","[ 2. 13.  2.  0.  1.  2.  0.  0.  1.  1. 25.]\n","inX: tensor([[ 2., 13.,  2.,  0.,  1.,  2.,  0.,  0.,  1.,  1., 25.]])\n","inSquee tensor([[[ 2., 13.,  2.,  0.,  1.,  2.,  0.,  0.,  1.,  1., 25.]]])\n","outX: tensor([[[-3.1585]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-3.1585]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(0.2290, grad_fn=<MseLossBackward0>)\n","38\n","[15. 33. 16.  0.  0.  6.  0.  0.  2.  1. 64.]\n","inX: tensor([[15., 33., 16.,  0.,  0.,  6.,  0.,  0.,  2.,  1., 64.]])\n","inSquee tensor([[[15., 33., 16.,  0.,  0.,  6.,  0.,  0.,  2.,  1., 64.]]])\n","outX: tensor([[[-4.3786]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-4.3786]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(20.2371, grad_fn=<MseLossBackward0>)\n","39\n","[3. 1. 1. 0. 0. 0. 0. 0. 0. 0. 2.]\n","inX: tensor([[3., 1., 1., 0., 0., 0., 0., 0., 0., 0., 2.]])\n","inSquee tensor([[[3., 1., 1., 0., 0., 0., 0., 0., 0., 0., 2.]]])\n","outX: tensor([[[0.1019]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[0.1019]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(0.0795, grad_fn=<MseLossBackward0>)\n","40\n","[17. 15. 11.  1.  0.  0.  0.  0.  5.  0.  4.]\n","inX: tensor([[17., 15., 11.,  1.,  0.,  0.,  0.,  0.,  5.,  0.,  4.]])\n","inSquee tensor([[[17., 15., 11.,  1.,  0.,  0.,  0.,  0.,  5.,  0.,  4.]]])\n","outX: tensor([[[1.3416]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[1.3416]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(9.7442, grad_fn=<MseLossBackward0>)\n","41\n","[0. 3. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","inX: tensor([[0., 3., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inSquee tensor([[[0., 3., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n","outX: tensor([[[-0.4980]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-0.4980]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(0.2621, grad_fn=<MseLossBackward0>)\n","42\n","[ 7. 23. 13.  0.  1.  1.  0.  0.  1.  3. 24.]\n","inX: tensor([[ 7., 23., 13.,  0.,  1.,  1.,  0.,  0.,  1.,  3., 24.]])\n","inSquee tensor([[[ 7., 23., 13.,  0.,  1.,  1.,  0.,  0.,  1.,  3., 24.]]])\n","outX: tensor([[[-0.8467]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-0.8467]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(0.0005, grad_fn=<MseLossBackward0>)\n","43\n","[ 2. 38.  8.  0.  0.  2.  0.  0.  2.  0. 17.]\n","inX: tensor([[ 2., 38.,  8.,  0.,  0.,  2.,  0.,  0.,  2.,  0., 17.]])\n","inSquee tensor([[[ 2., 38.,  8.,  0.,  0.,  2.,  0.,  0.,  2.,  0., 17.]]])\n","outX: tensor([[[-2.3154]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-2.3154]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(0.0422, grad_fn=<MseLossBackward0>)\n","44\n","[ 8. 14. 10.  0.  0.  4.  0.  0.  1.  1. 14.]\n","inX: tensor([[ 8., 14., 10.,  0.,  0.,  4.,  0.,  0.,  1.,  1., 14.]])\n","inSquee tensor([[[ 8., 14., 10.,  0.,  0.,  4.,  0.,  0.,  1.,  1., 14.]]])\n","outX: tensor([[[0.9775]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[0.9775]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(8.9853, grad_fn=<MseLossBackward0>)\n","45\n","[1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 6.]\n","inX: tensor([[1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 6.]])\n","inSquee tensor([[[1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 6.]]])\n","outX: tensor([[[-0.2697]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-0.2697]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(0.3367, grad_fn=<MseLossBackward0>)\n","46\n","[5. 5. 6. 0. 0. 0. 0. 0. 1. 0. 5.]\n","inX: tensor([[5., 5., 6., 0., 0., 0., 0., 0., 1., 0., 5.]])\n","inSquee tensor([[[5., 5., 6., 0., 0., 0., 0., 0., 1., 0., 5.]]])\n","outX: tensor([[[0.1231]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[0.1231]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(1.6981, grad_fn=<MseLossBackward0>)\n","47\n","[ 9. 21.  2.  0.  0.  0.  0.  0.  0.  1.  9.]\n","inX: tensor([[ 9., 21.,  2.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  9.]])\n","inSquee tensor([[[ 9., 21.,  2.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  9.]]])\n","outX: tensor([[[-1.2731]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-1.2731]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(2.0075, grad_fn=<MseLossBackward0>)\n","48\n","[ 4. 13. 19.  0.  0.  4.  0.  0.  2.  2. 43.]\n","inX: tensor([[ 4., 13., 19.,  0.,  0.,  4.,  0.,  0.,  2.,  2., 43.]])\n","inSquee tensor([[[ 4., 13., 19.,  0.,  0.,  4.,  0.,  0.,  2.,  2., 43.]]])\n","outX: tensor([[[-0.4488]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-0.4488]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(6.8706, grad_fn=<MseLossBackward0>)\n","49\n","[ 43. 109.   7.   1.   1.   0.   0.   0.   0.   0.   9.]\n","inX: tensor([[ 43., 109.,   7.,   1.,   1.,   0.,   0.,   0.,   0.,   0.,   9.]])\n","inSquee tensor([[[ 43., 109.,   7.,   1.,   1.,   0.,   0.,   0.,   0.,   0.,   9.]]])\n","outX: tensor([[[-5.8770]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-5.8770]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(37.4176, grad_fn=<MseLossBackward0>)\n","50\n","[0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","inX: tensor([[0., 2., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inSquee tensor([[[0., 2., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n","outX: tensor([[[-0.2020]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-0.2020]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(0.6053, grad_fn=<MseLossBackward0>)\n","51\n","[ 0.  1.  3.  1.  0.  3.  0.  0.  1.  1. 17.]\n","inX: tensor([[ 0.,  1.,  3.,  1.,  0.,  3.,  0.,  0.,  1.,  1., 17.]])\n","inSquee tensor([[[ 0.,  1.,  3.,  1.,  0.,  3.,  0.,  0.,  1.,  1., 17.]]])\n","outX: tensor([[[-0.2156]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-0.2156]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(1.6756, grad_fn=<MseLossBackward0>)\n","52\n","[  0.   3.  15.   0.   1.  11.   0.   0.   0.   1. 104.]\n","inX: tensor([[  0.,   3.,  15.,   0.,   1.,  11.,   0.,   0.,   0.,   1., 104.]])\n","inSquee tensor([[[  0.,   3.,  15.,   0.,   1.,  11.,   0.,   0.,   0.,   1., 104.]]])\n","outX: tensor([[[-2.0561]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-2.0561]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(4.2277, grad_fn=<MseLossBackward0>)\n","53\n","[ 7. 23.  4.  0.  5.  1.  0.  0.  0.  1. 28.]\n","inX: tensor([[ 7., 23.,  4.,  0.,  5.,  1.,  0.,  0.,  0.,  1., 28.]])\n","inSquee tensor([[[ 7., 23.,  4.,  0.,  5.,  1.,  0.,  0.,  0.,  1., 28.]]])\n","outX: tensor([[[1.3577]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[1.3577]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(13.0157, grad_fn=<MseLossBackward0>)\n","54\n","[ 1.  5.  7.  0.  1.  0.  0.  0.  2.  1. 17.]\n","inX: tensor([[ 1.,  5.,  7.,  0.,  1.,  0.,  0.,  0.,  2.,  1., 17.]])\n","inSquee tensor([[[ 1.,  5.,  7.,  0.,  1.,  0.,  0.,  0.,  2.,  1., 17.]]])\n","outX: tensor([[[-0.4851]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-0.4851]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(0.0072, grad_fn=<MseLossBackward0>)\n","55\n","[ 7. 24. 21.  4.  0.  1.  0.  0.  1.  2. 33.]\n","inX: tensor([[ 7., 24., 21.,  4.,  0.,  1.,  0.,  0.,  1.,  2., 33.]])\n","inSquee tensor([[[ 7., 24., 21.,  4.,  0.,  1.,  0.,  0.,  1.,  2., 33.]]])\n","outX: tensor([[[3.1367]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[3.1367]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(9.8386, grad_fn=<MseLossBackward0>)\n","56\n","[ 4.  0. 15.  2.  1.  1.  0.  0.  1.  0.  9.]\n","inX: tensor([[ 4.,  0., 15.,  2.,  1.,  1.,  0.,  0.,  1.,  0.,  9.]])\n","inSquee tensor([[[ 4.,  0., 15.,  2.,  1.,  1.,  0.,  0.,  1.,  0.,  9.]]])\n","outX: tensor([[[1.5565]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[1.5565]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(9.3424, grad_fn=<MseLossBackward0>)\n","57\n","[0. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0.]\n","inX: tensor([[0., 0., 2., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inSquee tensor([[[0., 0., 2., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n","outX: tensor([[[-0.1197]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-0.1197]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(0.0049, grad_fn=<MseLossBackward0>)\n","58\n","[ 3.  7.  6.  0.  1.  1.  0.  0.  2.  1. 14.]\n","inX: tensor([[ 3.,  7.,  6.,  0.,  1.,  1.,  0.,  0.,  2.,  1., 14.]])\n","inSquee tensor([[[ 3.,  7.,  6.,  0.,  1.,  1.,  0.,  0.,  2.,  1., 14.]]])\n","outX: tensor([[[-0.5424]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-0.5424]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(60.6461, grad_fn=<MseLossBackward0>)\n","59\n","[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","inX: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inSquee tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n","outX: tensor([[[-0.2844]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-0.2844]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(5.9750, grad_fn=<MseLossBackward0>)\n","60\n","[ 0.  1.  4.  0.  0.  0.  0.  0.  0.  0. 35.]\n","inX: tensor([[ 0.,  1.,  4.,  0.,  0.,  0.,  0.,  0.,  0.,  0., 35.]])\n","inSquee tensor([[[ 0.,  1.,  4.,  0.,  0.,  0.,  0.,  0.,  0.,  0., 35.]]])\n","outX: tensor([[[-2.7780]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-2.7780]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(13.3076, grad_fn=<MseLossBackward0>)\n","61\n","[ 0.  0.  1.  0.  0.  1.  0.  0.  0.  0. 17.]\n","inX: tensor([[ 0.,  0.,  1.,  0.,  0.,  1.,  0.,  0.,  0.,  0., 17.]])\n","inSquee tensor([[[ 0.,  0.,  1.,  0.,  0.,  1.,  0.,  0.,  0.,  0., 17.]]])\n","outX: tensor([[[-0.8959]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-0.8959]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(0.0007, grad_fn=<MseLossBackward0>)\n","62\n","[ 0.  0.  0.  0.  0.  2.  0.  0.  0.  2. 24.]\n","inX: tensor([[ 0.,  0.,  0.,  0.,  0.,  2.,  0.,  0.,  0.,  2., 24.]])\n","inSquee tensor([[[ 0.,  0.,  0.,  0.,  0.,  2.,  0.,  0.,  0.,  2., 24.]]])\n","outX: tensor([[[-1.4091]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-1.4091]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(0.3259, grad_fn=<MseLossBackward0>)\n","63\n","[ 6.  2.  8.  0.  0.  5.  0.  0.  0.  1. 42.]\n","inX: tensor([[ 6.,  2.,  8.,  0.,  0.,  5.,  0.,  0.,  0.,  1., 42.]])\n","inSquee tensor([[[ 6.,  2.,  8.,  0.,  0.,  5.,  0.,  0.,  0.,  1., 42.]]])\n","outX: tensor([[[-0.2308]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-0.2308]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(1.7934, grad_fn=<MseLossBackward0>)\n","64\n","[17. 29. 15.  0.  0.  1.  0.  0.  4.  0. 24.]\n","inX: tensor([[17., 29., 15.,  0.,  0.,  1.,  0.,  0.,  4.,  0., 24.]])\n","inSquee tensor([[[17., 29., 15.,  0.,  0.,  1.,  0.,  0.,  4.,  0., 24.]]])\n","outX: tensor([[[0.8697]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[0.8697]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(0.7564, grad_fn=<MseLossBackward0>)\n","65\n","[ 3.  8. 15.  3.  1.  6.  0.  0.  4.  3. 67.]\n","inX: tensor([[ 3.,  8., 15.,  3.,  1.,  6.,  0.,  0.,  4.,  3., 67.]])\n","inSquee tensor([[[ 3.,  8., 15.,  3.,  1.,  6.,  0.,  0.,  4.,  3., 67.]]])\n","outX: tensor([[[-3.0508]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-3.0508]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(15.8466, grad_fn=<MseLossBackward0>)\n","66\n","[25.  5.  2.  1.  0.  0.  0.  0.  0.  0. 12.]\n","inX: tensor([[25.,  5.,  2.,  1.,  0.,  0.,  0.,  0.,  0.,  0., 12.]])\n","inSquee tensor([[[25.,  5.,  2.,  1.,  0.,  0.,  0.,  0.,  0.,  0., 12.]]])\n","outX: tensor([[[3.3018]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[3.3018]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(35.7823, grad_fn=<MseLossBackward0>)\n","67\n","[6. 5. 2. 0. 0. 0. 0. 0. 0. 0. 6.]\n","inX: tensor([[6., 5., 2., 0., 0., 0., 0., 0., 0., 0., 6.]])\n","inSquee tensor([[[6., 5., 2., 0., 0., 0., 0., 0., 0., 0., 6.]]])\n","outX: tensor([[[0.2856]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[0.2856]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(2.6751, grad_fn=<MseLossBackward0>)\n","68\n","[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 4.]\n","inX: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 4.]])\n","inSquee tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 4.]]])\n","outX: tensor([[[-0.4243]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-0.4243]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(0.0816, grad_fn=<MseLossBackward0>)\n","69\n","[17. 15. 10.  0.  1.  0.  0.  0.  1.  0. 16.]\n","inX: tensor([[17., 15., 10.,  0.,  1.,  0.,  0.,  0.,  1.,  0., 16.]])\n","inSquee tensor([[[17., 15., 10.,  0.,  1.,  0.,  0.,  0.,  1.,  0., 16.]]])\n","outX: tensor([[[1.2840]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[1.2840]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(6.2200, grad_fn=<MseLossBackward0>)\n","70\n","[10. 18. 10.  0.  1.  0.  0.  0.  1.  0. 13.]\n","inX: tensor([[10., 18., 10.,  0.,  1.,  0.,  0.,  0.,  1.,  0., 13.]])\n","inSquee tensor([[[10., 18., 10.,  0.,  1.,  0.,  0.,  0.,  1.,  0., 13.]]])\n","outX: tensor([[[0.2543]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[0.2543]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(0.9689, grad_fn=<MseLossBackward0>)\n","71\n","[2. 7. 5. 1. 0. 0. 0. 0. 1. 1. 3.]\n","inX: tensor([[2., 7., 5., 1., 0., 0., 0., 0., 1., 1., 3.]])\n","inSquee tensor([[[2., 7., 5., 1., 0., 0., 0., 0., 1., 1., 3.]]])\n","outX: tensor([[[-0.1297]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-0.1297]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(0.1298, grad_fn=<MseLossBackward0>)\n","72\n","[  2.   6.  24.   0.   0.  17.   0.   0.   0.   4. 134.]\n","inX: tensor([[  2.,   6.,  24.,   0.,   0.,  17.,   0.,   0.,   0.,   4., 134.]])\n","inSquee tensor([[[  2.,   6.,  24.,   0.,   0.,  17.,   0.,   0.,   0.,   4., 134.]]])\n","outX: tensor([[[-1.2845]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-1.2845]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(2.0008, grad_fn=<MseLossBackward0>)\n","73\n","[ 0.  5. 10.  0.  0.  9.  0.  0.  0.  1. 37.]\n","inX: tensor([[ 0.,  5., 10.,  0.,  0.,  9.,  0.,  0.,  0.,  1., 37.]])\n","inSquee tensor([[[ 0.,  5., 10.,  0.,  0.,  9.,  0.,  0.,  0.,  1., 37.]]])\n","outX: tensor([[[2.0985]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[2.0985]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(12.0307, grad_fn=<MseLossBackward0>)\n","74\n","[ 5.  6.  7.  0.  0.  4.  0.  0.  0.  0. 12.]\n","inX: tensor([[ 5.,  6.,  7.,  0.,  0.,  4.,  0.,  0.,  0.,  0., 12.]])\n","inSquee tensor([[[ 5.,  6.,  7.,  0.,  0.,  4.,  0.,  0.,  0.,  0., 12.]]])\n","outX: tensor([[[1.0776]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[1.0776]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(6.0397, grad_fn=<MseLossBackward0>)\n","75\n","[9. 3. 1. 0. 0. 1. 0. 0. 0. 0. 0.]\n","inX: tensor([[9., 3., 1., 0., 0., 1., 0., 0., 0., 0., 0.]])\n","inSquee tensor([[[9., 3., 1., 0., 0., 1., 0., 0., 0., 0., 0.]]])\n","outX: tensor([[[0.7692]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[0.7692]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(3.1655, grad_fn=<MseLossBackward0>)\n","76\n","[ 0.  1.  8.  0.  0.  1.  0.  0.  1.  1. 18.]\n","inX: tensor([[ 0.,  1.,  8.,  0.,  0.,  1.,  0.,  0.,  1.,  1., 18.]])\n","inSquee tensor([[[ 0.,  1.,  8.,  0.,  0.,  1.,  0.,  0.,  1.,  1., 18.]]])\n","outX: tensor([[[-0.5988]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-0.5988]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(2.4687, grad_fn=<MseLossBackward0>)\n","77\n","[ 4.  3.  2.  0.  0.  3.  0.  0.  0.  3. 13.]\n","inX: tensor([[ 4.,  3.,  2.,  0.,  0.,  3.,  0.,  0.,  0.,  3., 13.]])\n","inSquee tensor([[[ 4.,  3.,  2.,  0.,  0.,  3.,  0.,  0.,  0.,  3., 13.]]])\n","outX: tensor([[[-0.2138]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-0.2138]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(0.1415, grad_fn=<MseLossBackward0>)\n","78\n","[ 6. 12. 19.  1.  0.  1.  0.  0.  1.  0.  1.]\n","inX: tensor([[ 6., 12., 19.,  1.,  0.,  1.,  0.,  0.,  1.,  0.,  1.]])\n","inSquee tensor([[[ 6., 12., 19.,  1.,  0.,  1.,  0.,  0.,  1.,  0.,  1.]]])\n","outX: tensor([[[1.3756]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[1.3756]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(11.7349, grad_fn=<MseLossBackward0>)\n","79\n","[ 2. 40.  7.  0.  1.  2.  0.  0.  2.  1. 45.]\n","inX: tensor([[ 2., 40.,  7.,  0.,  1.,  2.,  0.,  0.,  2.,  1., 45.]])\n","inSquee tensor([[[ 2., 40.,  7.,  0.,  1.,  2.,  0.,  0.,  2.,  1., 45.]]])\n","outX: tensor([[[-3.4606]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-3.4606]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(3.7271, grad_fn=<MseLossBackward0>)\n","80\n","[10. 16. 14.  0.  0.  1.  0.  0.  1.  2. 11.]\n","inX: tensor([[10., 16., 14.,  0.,  0.,  1.,  0.,  0.,  1.,  2., 11.]])\n","inSquee tensor([[[10., 16., 14.,  0.,  0.,  1.,  0.,  0.,  1.,  2., 11.]]])\n","outX: tensor([[[0.4474]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[0.4474]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(0.3450, grad_fn=<MseLossBackward0>)\n","81\n","[ 1.  3.  3.  0.  0.  0.  0.  0.  1.  1. 12.]\n","inX: tensor([[ 1.,  3.,  3.,  0.,  0.,  0.,  0.,  0.,  1.,  1., 12.]])\n","inSquee tensor([[[ 1.,  3.,  3.,  0.,  0.,  0.,  0.,  0.,  1.,  1., 12.]]])\n","outX: tensor([[[-0.8219]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-0.8219]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(0.1458, grad_fn=<MseLossBackward0>)\n","82\n","[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 4.]\n","inX: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 4.]])\n","inSquee tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 4.]]])\n","outX: tensor([[[-0.4069]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-0.4069]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(0.1656, grad_fn=<MseLossBackward0>)\n","83\n","[ 9.  3. 45.  1.  0.  0.  0.  0.  1.  0.  3.]\n","inX: tensor([[ 9.,  3., 45.,  1.,  0.,  0.,  0.,  0.,  1.,  0.,  3.]])\n","inSquee tensor([[[ 9.,  3., 45.,  1.,  0.,  0.,  0.,  0.,  1.,  0.,  3.]]])\n","outX: tensor([[[2.7563]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[2.7563]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(7.5974, grad_fn=<MseLossBackward0>)\n","84\n","[2. 2. 3. 0. 0. 2. 0. 0. 2. 0. 3.]\n","inX: tensor([[2., 2., 3., 0., 0., 2., 0., 0., 2., 0., 3.]])\n","inSquee tensor([[[2., 2., 3., 0., 0., 2., 0., 0., 2., 0., 3.]]])\n","outX: tensor([[[-0.1317]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-0.1317]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(0.2225, grad_fn=<MseLossBackward0>)\n","85\n","[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 5.]\n","inX: tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 5.]])\n","inSquee tensor([[[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 5.]]])\n","outX: tensor([[[-0.4647]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-0.4647]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(4.7728, grad_fn=<MseLossBackward0>)\n","86\n","[ 0.  1.  0.  0.  0.  1.  0.  0.  0.  1. 15.]\n","inX: tensor([[ 0.,  1.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  1., 15.]])\n","inSquee tensor([[[ 0.,  1.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  1., 15.]]])\n","outX: tensor([[[-0.6570]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-0.6570]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(4.1493, grad_fn=<MseLossBackward0>)\n","87\n","[13. 33. 24.  1.  3. 16.  0.  0.  2.  1. 27.]\n","inX: tensor([[13., 33., 24.,  1.,  3., 16.,  0.,  0.,  2.,  1., 27.]])\n","inSquee tensor([[[13., 33., 24.,  1.,  3., 16.,  0.,  0.,  2.,  1., 27.]]])\n","outX: tensor([[[3.0958]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[3.0958]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(20.0328, grad_fn=<MseLossBackward0>)\n","88\n","[0. 3. 3. 0. 0. 0. 0. 0. 0. 0. 3.]\n","inX: tensor([[0., 3., 3., 0., 0., 0., 0., 0., 0., 0., 3.]])\n","inSquee tensor([[[0., 3., 3., 0., 0., 0., 0., 0., 0., 0., 3.]]])\n","outX: tensor([[[-0.5582]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-0.5582]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(3.0340, grad_fn=<MseLossBackward0>)\n","89\n","[ 2. 33.  1.  0.  0.  1.  0.  0.  0.  1. 20.]\n","inX: tensor([[ 2., 33.,  1.,  0.,  0.,  1.,  0.,  0.,  0.,  1., 20.]])\n","inSquee tensor([[[ 2., 33.,  1.,  0.,  0.,  1.,  0.,  0.,  0.,  1., 20.]]])\n","outX: tensor([[[-2.7480]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-2.7480]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(0.4873, grad_fn=<MseLossBackward0>)\n","90\n","[0. 2. 5. 0. 0. 1. 0. 0. 1. 0. 7.]\n","inX: tensor([[0., 2., 5., 0., 0., 1., 0., 0., 1., 0., 7.]])\n","inSquee tensor([[[0., 2., 5., 0., 0., 1., 0., 0., 1., 0., 7.]]])\n","outX: tensor([[[-0.6555]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-0.6555]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(20.6521, grad_fn=<MseLossBackward0>)\n","91\n","[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","inX: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inSquee tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n","outX: tensor([[[-0.2888]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-0.2888]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(0.2105, grad_fn=<MseLossBackward0>)\n","92\n","[ 4. 21. 14.  0.  0.  0.  0.  0.  1.  1.  3.]\n","inX: tensor([[ 4., 21., 14.,  0.,  0.,  0.,  0.,  0.,  1.,  1.,  3.]])\n","inSquee tensor([[[ 4., 21., 14.,  0.,  0.,  0.,  0.,  0.,  1.,  1.,  3.]]])\n","outX: tensor([[[-1.5212]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-1.5212]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(0.6102, grad_fn=<MseLossBackward0>)\n","93\n","[ 6.  4. 19.  0.  0. 10.  0.  0.  0.  3. 83.]\n","inX: tensor([[ 6.,  4., 19.,  0.,  0., 10.,  0.,  0.,  0.,  3., 83.]])\n","inSquee tensor([[[ 6.,  4., 19.,  0.,  0., 10.,  0.,  0.,  0.,  3., 83.]]])\n","outX: tensor([[[-2.5111]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-2.5111]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(11.7723, grad_fn=<MseLossBackward0>)\n","94\n","[ 4.  3.  1.  0.  0.  1.  0.  0.  0.  0. 15.]\n","inX: tensor([[ 4.,  3.,  1.,  0.,  0.,  1.,  0.,  0.,  0.,  0., 15.]])\n","inSquee tensor([[[ 4.,  3.,  1.,  0.,  0.,  1.,  0.,  0.,  0.,  0., 15.]]])\n","outX: tensor([[[0.2022]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[0.2022]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(5.5327, grad_fn=<MseLossBackward0>)\n","95\n","[ 5. 20. 19.  3.  0.  3.  0.  0.  3.  0. 29.]\n","inX: tensor([[ 5., 20., 19.,  3.,  0.,  3.,  0.,  0.,  3.,  0., 29.]])\n","inSquee tensor([[[ 5., 20., 19.,  3.,  0.,  3.,  0.,  0.,  3.,  0., 29.]]])\n","outX: tensor([[[0.1749]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[0.1749]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(17.9341, grad_fn=<MseLossBackward0>)\n","96\n","[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 7.]\n","inX: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 7.]])\n","inSquee tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 7.]]])\n","outX: tensor([[[-0.4687]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-0.4687]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(0.8077, grad_fn=<MseLossBackward0>)\n","97\n","[ 6.  3. 26.  0.  0.  0.  0.  0.  4.  0.  6.]\n","inX: tensor([[ 6.,  3., 26.,  0.,  0.,  0.,  0.,  0.,  4.,  0.,  6.]])\n","inSquee tensor([[[ 6.,  3., 26.,  0.,  0.,  0.,  0.,  0.,  4.,  0.,  6.]]])\n","outX: tensor([[[-1.0272]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-1.0272]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(0.3105, grad_fn=<MseLossBackward0>)\n","98\n","[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","inX: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inSquee tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n","outX: tensor([[[-0.2889]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-0.2889]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(2.1937, grad_fn=<MseLossBackward0>)\n","99\n","[ 8.  8. 21.  0.  0. 10.  0.  0.  2.  3. 83.]\n","inX: tensor([[ 8.,  8., 21.,  0.,  0., 10.,  0.,  0.,  2.,  3., 83.]])\n","inSquee tensor([[[ 8.,  8., 21.,  0.,  0., 10.,  0.,  0.,  2.,  3., 83.]]])\n","outX: tensor([[[-0.8013]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-0.8013]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(1.6935, grad_fn=<MseLossBackward0>)\n","100\n","[14. 30. 18.  0.  1.  2.  0.  0.  1.  1. 22.]\n","inX: tensor([[14., 30., 18.,  0.,  1.,  2.,  0.,  0.,  1.,  1., 22.]])\n","inSquee tensor([[[14., 30., 18.,  0.,  1.,  2.,  0.,  0.,  1.,  1., 22.]]])\n","outX: tensor([[[-0.9957]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-0.9957]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(10.9274, grad_fn=<MseLossBackward0>)\n","101\n","[5. 0. 5. 0. 0. 1. 0. 0. 0. 0. 1.]\n","inX: tensor([[5., 0., 5., 0., 0., 1., 0., 0., 0., 0., 1.]])\n","inSquee tensor([[[5., 0., 5., 0., 0., 1., 0., 0., 0., 0., 1.]]])\n","outX: tensor([[[0.4456]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[0.4456]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(0.9888, grad_fn=<MseLossBackward0>)\n","102\n","[0. 2. 1. 0. 0. 0. 0. 0. 0. 0. 6.]\n","inX: tensor([[0., 2., 1., 0., 0., 0., 0., 0., 0., 0., 6.]])\n","inSquee tensor([[[0., 2., 1., 0., 0., 0., 0., 0., 0., 0., 6.]]])\n","outX: tensor([[[-0.2687]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-0.2687]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(0.1947, grad_fn=<MseLossBackward0>)\n","103\n","[1. 0. 5. 0. 0. 0. 0. 0. 0. 0. 2.]\n","inX: tensor([[1., 0., 5., 0., 0., 0., 0., 0., 0., 0., 2.]])\n","inSquee tensor([[[1., 0., 5., 0., 0., 0., 0., 0., 0., 0., 2.]]])\n","outX: tensor([[[-0.0740]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-0.0740]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(0.0818, grad_fn=<MseLossBackward0>)\n","104\n","[0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1.]\n","inX: tensor([[0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1.]])\n","inSquee tensor([[[0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1.]]])\n","outX: tensor([[[-0.2940]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-0.2940]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(1.2859, grad_fn=<MseLossBackward0>)\n","105\n","[ 22. 140.  26.   2.   0.  27.   0.   0.   1.   0.  39.]\n","inX: tensor([[ 22., 140.,  26.,   2.,   0.,  27.,   0.,   0.,   1.,   0.,  39.]])\n","inSquee tensor([[[ 22., 140.,  26.,   2.,   0.,  27.,   0.,   0.,   1.,   0.,  39.]]])\n","outX: tensor([[[3.2257]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[3.2257]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(25.5600, grad_fn=<MseLossBackward0>)\n","106\n","[0. 3. 2. 0. 0. 0. 0. 0. 1. 0. 0.]\n","inX: tensor([[0., 3., 2., 0., 0., 0., 0., 0., 1., 0., 0.]])\n","inSquee tensor([[[0., 3., 2., 0., 0., 0., 0., 0., 1., 0., 0.]]])\n","outX: tensor([[[-1.0471]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-1.0471]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(8.9826, grad_fn=<MseLossBackward0>)\n","107\n","[ 2. 20.  7.  0.  1.  2.  0.  0.  0.  1. 29.]\n","inX: tensor([[ 2., 20.,  7.,  0.,  1.,  2.,  0.,  0.,  0.,  1., 29.]])\n","inSquee tensor([[[ 2., 20.,  7.,  0.,  1.,  2.,  0.,  0.,  0.,  1., 29.]]])\n","outX: tensor([[[-4.4379]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-4.4379]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(70.3575, grad_fn=<MseLossBackward0>)\n","108\n","[0. 0. 2. 0. 0. 0. 0. 0. 0. 0. 1.]\n","inX: tensor([[0., 0., 2., 0., 0., 0., 0., 0., 0., 0., 1.]])\n","inSquee tensor([[[0., 0., 2., 0., 0., 0., 0., 0., 0., 0., 1.]]])\n","outX: tensor([[[-0.2482]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-0.2482]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(2.4707, grad_fn=<MseLossBackward0>)\n","109\n","[5. 8. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","inX: tensor([[5., 8., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inSquee tensor([[[5., 8., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n","outX: tensor([[[-1.1444]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-1.1444]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(2.3239, grad_fn=<MseLossBackward0>)\n","110\n","[  2. 119. 142.   2.   0.   1.   0.   0.   0.   1.  16.]\n","inX: tensor([[  2., 119., 142.,   2.,   0.,   1.,   0.,   0.,   0.,   1.,  16.]])\n","inSquee tensor([[[  2., 119., 142.,   2.,   0.,   1.,   0.,   0.,   0.,   1.,  16.]]])\n","outX: tensor([[[-15.2162]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-15.2162]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(219.2234, grad_fn=<MseLossBackward0>)\n","111\n","[ 6. 18. 21.  0.  0.  1.  0.  0.  0.  0. 17.]\n","inX: tensor([[ 6., 18., 21.,  0.,  0.,  1.,  0.,  0.,  0.,  0., 17.]])\n","inSquee tensor([[[ 6., 18., 21.,  0.,  0.,  1.,  0.,  0.,  0.,  0., 17.]]])\n","outX: tensor([[[14.2587]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[14.2587]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(196.5237, grad_fn=<MseLossBackward0>)\n","112\n","[ 6. 27. 18.  1.  0.  2.  0.  0.  2.  3. 23.]\n","inX: tensor([[ 6., 27., 18.,  1.,  0.,  2.,  0.,  0.,  2.,  3., 23.]])\n","inSquee tensor([[[ 6., 27., 18.,  1.,  0.,  2.,  0.,  0.,  2.,  3., 23.]]])\n","outX: tensor([[[11.2438]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[11.2438]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(174.6034, grad_fn=<MseLossBackward0>)\n","113\n","[ 5.  3. 25.  8.  0.  2.  0.  0.  1.  1. 49.]\n","inX: tensor([[ 5.,  3., 25.,  8.,  0.,  2.,  0.,  0.,  1.,  1., 49.]])\n","inSquee tensor([[[ 5.,  3., 25.,  8.,  0.,  2.,  0.,  0.,  1.,  1., 49.]]])\n","outX: tensor([[[8.0386]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[8.0386]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(145.6512, grad_fn=<MseLossBackward0>)\n","114\n","[2. 1. 2. 0. 0. 0. 0. 0. 0. 0. 1.]\n","inX: tensor([[2., 1., 2., 0., 0., 0., 0., 0., 0., 0., 1.]])\n","inSquee tensor([[[2., 1., 2., 0., 0., 0., 0., 0., 0., 0., 1.]]])\n","outX: tensor([[[0.2015]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[0.2015]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(0.0165, grad_fn=<MseLossBackward0>)\n","115\n","[0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0.]\n","inX: tensor([[0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0.]])\n","inSquee tensor([[[0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0.]]])\n","outX: tensor([[[-0.1705]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-0.1705]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(0.1931, grad_fn=<MseLossBackward0>)\n","116\n","[7. 1. 2. 0. 0. 0. 0. 0. 0. 1. 0.]\n","inX: tensor([[7., 1., 2., 0., 0., 0., 0., 0., 0., 1., 0.]])\n","inSquee tensor([[[7., 1., 2., 0., 0., 0., 0., 0., 0., 1., 0.]]])\n","outX: tensor([[[0.3331]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[0.3331]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(0.1110, grad_fn=<MseLossBackward0>)\n","117\n","[ 0.  0.  3.  1.  0.  1.  0.  0.  0.  0. 13.]\n","inX: tensor([[ 0.,  0.,  3.,  1.,  0.,  1.,  0.,  0.,  0.,  0., 13.]])\n","inSquee tensor([[[ 0.,  0.,  3.,  1.,  0.,  1.,  0.,  0.,  0.,  0., 13.]]])\n","outX: tensor([[[-1.1755]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-1.1755]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(0.8546, grad_fn=<MseLossBackward0>)\n","118\n","[ 8. 31. 27.  2.  1.  3.  0.  0.  3.  0. 35.]\n","inX: tensor([[ 8., 31., 27.,  2.,  1.,  3.,  0.,  0.,  3.,  0., 35.]])\n","inSquee tensor([[[ 8., 31., 27.,  2.,  1.,  3.,  0.,  0.,  3.,  0., 35.]]])\n","outX: tensor([[[4.0093]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[4.0093]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(28.6147, grad_fn=<MseLossBackward0>)\n","119\n","[4. 4. 2. 0. 0. 0. 0. 0. 0. 0. 4.]\n","inX: tensor([[4., 4., 2., 0., 0., 0., 0., 0., 0., 0., 4.]])\n","inSquee tensor([[[4., 4., 2., 0., 0., 0., 0., 0., 0., 0., 4.]]])\n","outX: tensor([[[-0.3732]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-0.3732]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(2.2107, grad_fn=<MseLossBackward0>)\n","120\n","[15. 23. 24.  0.  1.  4.  0.  0.  0.  2. 31.]\n","inX: tensor([[15., 23., 24.,  0.,  1.,  4.,  0.,  0.,  0.,  2., 31.]])\n","inSquee tensor([[[15., 23., 24.,  0.,  1.,  4.,  0.,  0.,  0.,  2., 31.]]])\n","outX: tensor([[[0.6681]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[0.6681]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(6.9596, grad_fn=<MseLossBackward0>)\n","121\n","[ 5. 14. 16.  5.  2.  0.  0.  0.  0.  0.  0.]\n","inX: tensor([[ 5., 14., 16.,  5.,  2.,  0.,  0.,  0.,  0.,  0.,  0.]])\n","inSquee tensor([[[ 5., 14., 16.,  5.,  2.,  0.,  0.,  0.,  0.,  0.,  0.]]])\n","outX: tensor([[[4.4635]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[4.4635]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(23.6533, grad_fn=<MseLossBackward0>)\n","122\n","[ 2. 18.  3.  0.  0.  0.  0.  0.  0.  0.  6.]\n","inX: tensor([[ 2., 18.,  3.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  6.]])\n","inSquee tensor([[[ 2., 18.,  3.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  6.]]])\n","outX: tensor([[[-0.6144]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-0.6144]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(0.1645, grad_fn=<MseLossBackward0>)\n","123\n","[0. 1. 1. 0. 1. 0. 0. 0. 1. 0. 5.]\n","inX: tensor([[0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 5.]])\n","inSquee tensor([[[0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 5.]]])\n","outX: tensor([[[-1.4936]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-1.4936]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(3.9347, grad_fn=<MseLossBackward0>)\n","124\n","[1. 7. 1. 1. 0. 0. 0. 0. 0. 1. 6.]\n","inX: tensor([[1., 7., 1., 1., 0., 0., 0., 0., 0., 1., 6.]])\n","inSquee tensor([[[1., 7., 1., 1., 0., 0., 0., 0., 0., 1., 6.]]])\n","outX: tensor([[[-1.1946]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-1.1946]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(2.0604, grad_fn=<MseLossBackward0>)\n","125\n","[ 8.  8. 23.  0.  0.  4.  0.  0.  2.  1. 40.]\n","inX: tensor([[ 8.,  8., 23.,  0.,  0.,  4.,  0.,  0.,  2.,  1., 40.]])\n","inSquee tensor([[[ 8.,  8., 23.,  0.,  0.,  4.,  0.,  0.,  2.,  1., 40.]]])\n","outX: tensor([[[-3.9715]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-3.9715]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(9.9317, grad_fn=<MseLossBackward0>)\n","126\n","[ 1.  3. 12.  0.  0.  8.  0.  0.  1.  0. 33.]\n","inX: tensor([[ 1.,  3., 12.,  0.,  0.,  8.,  0.,  0.,  1.,  0., 33.]])\n","inSquee tensor([[[ 1.,  3., 12.,  0.,  0.,  8.,  0.,  0.,  1.,  0., 33.]]])\n","outX: tensor([[[-2.6461]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-2.6461]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(3.5196, grad_fn=<MseLossBackward0>)\n","127\n","[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","inX: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inSquee tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n","outX: tensor([[[-0.2938]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-0.2938]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(0.2849, grad_fn=<MseLossBackward0>)\n","128\n","[0. 1. 5. 0. 0. 0. 0. 0. 1. 0. 0.]\n","inX: tensor([[0., 1., 5., 0., 0., 0., 0., 0., 1., 0., 0.]])\n","inSquee tensor([[[0., 1., 5., 0., 0., 0., 0., 0., 1., 0., 0.]]])\n","outX: tensor([[[0.6250]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[0.6250]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(0.0132, grad_fn=<MseLossBackward0>)\n","129\n","[1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n","inX: tensor([[1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inSquee tensor([[[1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n","outX: tensor([[[-0.0655]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-0.0655]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(1.5240, grad_fn=<MseLossBackward0>)\n","130\n","[ 4.  4. 11.  5.  0.  2.  0.  0.  1.  2. 25.]\n","inX: tensor([[ 4.,  4., 11.,  5.,  0.,  2.,  0.,  0.,  1.,  2., 25.]])\n","inSquee tensor([[[ 4.,  4., 11.,  5.,  0.,  2.,  0.,  0.,  1.,  2., 25.]]])\n","outX: tensor([[[-1.3579]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-1.3579]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(0.6759, grad_fn=<MseLossBackward0>)\n","131\n","[3. 5. 2. 1. 0. 0. 0. 0. 0. 1. 0.]\n","inX: tensor([[3., 5., 2., 1., 0., 0., 0., 0., 0., 1., 0.]])\n","inSquee tensor([[[3., 5., 2., 1., 0., 0., 0., 0., 0., 1., 0.]]])\n","outX: tensor([[[0.3200]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[0.3200]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(0.1681, grad_fn=<MseLossBackward0>)\n","132\n","[0. 1. 5. 0. 0. 0. 0. 0. 3. 0. 0.]\n","inX: tensor([[0., 1., 5., 0., 0., 0., 0., 0., 3., 0., 0.]])\n","inSquee tensor([[[0., 1., 5., 0., 0., 0., 0., 0., 3., 0., 0.]]])\n","outX: tensor([[[0.1844]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[0.1844]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(0.1565, grad_fn=<MseLossBackward0>)\n","133\n","[ 6.  4. 39.  0.  0.  0.  0.  0.  7.  0.  8.]\n","inX: tensor([[ 6.,  4., 39.,  0.,  0.,  0.,  0.,  0.,  7.,  0.,  8.]])\n","inSquee tensor([[[ 6.,  4., 39.,  0.,  0.,  0.,  0.,  0.,  7.,  0.,  8.]]])\n","outX: tensor([[[5.4745]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[5.4745]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(29.9703, grad_fn=<MseLossBackward0>)\n","134\n","[0. 2. 1. 0. 0. 0. 0. 0. 0. 0. 5.]\n","inX: tensor([[0., 2., 1., 0., 0., 0., 0., 0., 0., 0., 5.]])\n","inSquee tensor([[[0., 2., 1., 0., 0., 0., 0., 0., 0., 0., 5.]]])\n","outX: tensor([[[-1.0350]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-1.0350]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(6.9433, grad_fn=<MseLossBackward0>)\n","135\n","[1. 5. 3. 0. 2. 0. 0. 0. 0. 1. 1.]\n","inX: tensor([[1., 5., 3., 0., 2., 0., 0., 0., 0., 1., 1.]])\n","inSquee tensor([[[1., 5., 3., 0., 2., 0., 0., 0., 0., 1., 1.]]])\n","outX: tensor([[[-0.2414]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-0.2414]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(2.5556, grad_fn=<MseLossBackward0>)\n","136\n","[ 5.  4.  6.  1.  0.  1.  0.  0.  0.  1. 19.]\n","inX: tensor([[ 5.,  4.,  6.,  1.,  0.,  1.,  0.,  0.,  0.,  1., 19.]])\n","inSquee tensor([[[ 5.,  4.,  6.,  1.,  0.,  1.,  0.,  0.,  0.,  1., 19.]]])\n","outX: tensor([[[-2.5195]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-2.5195]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(1.0190, grad_fn=<MseLossBackward0>)\n","137\n","[  1.   3.  20.   0.   1.  15.   0.   0.   0.   4. 134.]\n","inX: tensor([[  1.,   3.,  20.,   0.,   1.,  15.,   0.,   0.,   0.,   4., 134.]])\n","inSquee tensor([[[  1.,   3.,  20.,   0.,   1.,  15.,   0.,   0.,   0.,   4., 134.]]])\n","outX: tensor([[[-19.3569]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-19.3569]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(291.9605, grad_fn=<MseLossBackward0>)\n","138\n","[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","inX: tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inSquee tensor([[[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n","outX: tensor([[[-0.2619]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-0.2619]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(0.0078, grad_fn=<MseLossBackward0>)\n","139\n","[ 1.  0.  4.  0.  0.  2.  0.  0.  0.  1. 51.]\n","inX: tensor([[ 1.,  0.,  4.,  0.,  0.,  2.,  0.,  0.,  0.,  1., 51.]])\n","inSquee tensor([[[ 1.,  0.,  4.,  0.,  0.,  2.,  0.,  0.,  0.,  1., 51.]]])\n","outX: tensor([[[14.9644]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[14.9644]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(333.9541, grad_fn=<MseLossBackward0>)\n","140\n","[0. 6. 5. 2. 1. 0. 0. 0. 2. 0. 6.]\n","inX: tensor([[0., 6., 5., 2., 1., 0., 0., 0., 2., 0., 6.]])\n","inSquee tensor([[[0., 6., 5., 2., 1., 0., 0., 0., 2., 0., 6.]]])\n","outX: tensor([[[1.3842]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[1.3842]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(1.9161, grad_fn=<MseLossBackward0>)\n","141\n","[1. 3. 4. 0. 0. 1. 0. 0. 0. 0. 6.]\n","inX: tensor([[1., 3., 4., 0., 0., 1., 0., 0., 0., 0., 6.]])\n","inSquee tensor([[[1., 3., 4., 0., 0., 1., 0., 0., 0., 0., 6.]]])\n","outX: tensor([[[1.4515]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[1.4515]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(25.1155, grad_fn=<MseLossBackward0>)\n","142\n","[ 0.  4.  2.  0.  0.  5.  0.  0.  1.  1. 20.]\n","inX: tensor([[ 0.,  4.,  2.,  0.,  0.,  5.,  0.,  0.,  1.,  1., 20.]])\n","inSquee tensor([[[ 0.,  4.,  2.,  0.,  0.,  5.,  0.,  0.,  1.,  1., 20.]]])\n","outX: tensor([[[2.6216]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[2.6216]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(9.2516, grad_fn=<MseLossBackward0>)\n","143\n","[0. 0. 2. 0. 0. 0. 0. 0. 0. 0. 9.]\n","inX: tensor([[0., 0., 2., 0., 0., 0., 0., 0., 0., 0., 9.]])\n","inSquee tensor([[[0., 0., 2., 0., 0., 0., 0., 0., 0., 0., 9.]]])\n","outX: tensor([[[0.7559]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[0.7559]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(1.4301, grad_fn=<MseLossBackward0>)\n","144\n","[10.  6. 14.  0.  0.  1.  0.  0.  1.  0.  7.]\n","inX: tensor([[10.,  6., 14.,  0.,  0.,  1.,  0.,  0.,  1.,  0.,  7.]])\n","inSquee tensor([[[10.,  6., 14.,  0.,  0.,  1.,  0.,  0.,  1.,  0.,  7.]]])\n","outX: tensor([[[3.4282]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[3.4282]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(17.8777, grad_fn=<MseLossBackward0>)\n","145\n","[1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n","inX: tensor([[1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inSquee tensor([[[1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n","outX: tensor([[[-0.0945]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-0.0945]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(0.1486, grad_fn=<MseLossBackward0>)\n","146\n","[1. 1. 3. 0. 0. 0. 0. 0. 0. 0. 0.]\n","inX: tensor([[1., 1., 3., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inSquee tensor([[[1., 1., 3., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n","outX: tensor([[[0.3495]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[0.3495]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(0.4892, grad_fn=<MseLossBackward0>)\n","147\n","[ 4.  4. 20.  0.  0.  3.  0.  0.  0.  2. 23.]\n","inX: tensor([[ 4.,  4., 20.,  0.,  0.,  3.,  0.,  0.,  0.,  2., 23.]])\n","inSquee tensor([[[ 4.,  4., 20.,  0.,  0.,  3.,  0.,  0.,  0.,  2., 23.]]])\n","outX: tensor([[[5.7410]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[5.7410]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(52.1426, grad_fn=<MseLossBackward0>)\n"]}]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","loss = [loss_tensor.item() for loss_tensor in loss_list]"],"metadata":{"id":"E5elVetJP8pa","executionInfo":{"status":"ok","timestamp":1652096027333,"user_tz":-540,"elapsed":550,"user":{"displayName":"Ha Jun","userId":"06515366270624202236"}}},"execution_count":120,"outputs":[]},{"cell_type":"code","source":["plt.plot(loss)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":287},"id":"cV1tbzVXP9xZ","executionInfo":{"status":"ok","timestamp":1652096029217,"user_tz":-540,"elapsed":23,"user":{"displayName":"Ha Jun","userId":"06515366270624202236"}},"outputId":"30e6c1ed-28c5-4cbf-91f4-89da5adfffab"},"execution_count":121,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[<matplotlib.lines.Line2D at 0x7f031eac8890>]"]},"metadata":{},"execution_count":121},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","source":["##선형 회귀 모델을 이용한 주가 예측"],"metadata":{"id":"JGYsMJ1TUgdJ"}},{"cell_type":"code","source":["top100 = getTop100()\n","news = getDayNews('202205091030', top100, 1, False, False) #(날짜, 검색 기업 리스트, 기업 당 검색 건수, 저장여부, 지정날짜의 기사만 검색할 지)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JMxl16SFUpE1","executionInfo":{"status":"ok","timestamp":1652095527007,"user_tz":-540,"elapsed":61809,"user":{"displayName":"Ha Jun","userId":"06515366270624202236"}},"outputId":"4e435490-5f46-40c6-cd41-7cbfe40ed8cc"},"execution_count":105,"outputs":[{"output_type":"stream","name":"stdout","text":["[2022-05-09 11:24:27.224116] Url Request Success\n","202205091030 삼성전자\n","1 건\n","[2022-05-09 11:24:27.804251] Url Request Success\n","202205091030 LG에너지솔루션\n","1 건\n","[2022-05-09 11:24:28.552146] Url Request Success\n","202205091030 SK하이닉스\n","1 건\n","[2022-05-09 11:24:29.092170] Url Request Success\n","202205091030 NAVER\n","1 건\n","[2022-05-09 11:24:29.658267] Url Request Success\n","202205091030 삼성바이오로직스\n","1 건\n","[2022-05-09 11:24:30.458464] Url Request Success\n","202205091030 삼성전자우\n","1 건\n","[2022-05-09 11:24:31.037943] Url Request Success\n","202205091030 카카오\n","1 건\n","[2022-05-09 11:24:31.632519] Url Request Success\n","202205091030 삼성SDI\n","1 건\n","[2022-05-09 11:24:32.450257] Url Request Success\n","202205091030 현대차\n","1 건\n","[2022-05-09 11:24:33.175624] Url Request Success\n","202205091030 LG화학\n","1 건\n","[2022-05-09 11:24:33.979957] Url Request Success\n","202205091030 기아\n","1 건\n","[2022-05-09 11:24:34.538946] Url Request Success\n","202205091030 POSCO홀딩스\n","1 건\n","[2022-05-09 11:24:35.158121] Url Request Success\n","202205091030 KB금융\n","1 건\n","[2022-05-09 11:24:35.776721] Url Request Success\n","202205091030 카카오뱅크\n","1 건\n","[2022-05-09 11:24:36.359007] Url Request Success\n","202205091030 셀트리온\n","1 건\n","[2022-05-09 11:24:37.138544] Url Request Success\n","202205091030 신한지주\n","1 건\n","[2022-05-09 11:24:37.707129] Url Request Success\n","202205091030 삼성물산\n","1 건\n","[2022-05-09 11:24:38.490825] Url Request Success\n","202205091030 현대모비스\n","1 건\n","[2022-05-09 11:24:39.133703] Url Request Success\n","202205091030 SK이노베이션\n","1 건\n","[2022-05-09 11:24:39.757073] Url Request Success\n","202205091030 LG전자\n","1 건\n","[2022-05-09 11:24:40.339224] Url Request Success\n","202205091030 카카오페이\n","1 건\n","[2022-05-09 11:24:41.115366] Url Request Success\n","202205091030 SK\n","1 건\n","[2022-05-09 11:24:41.634454] Url Request Success\n","202205091030 하나금융지주\n","1 건\n","[2022-05-09 11:24:42.237168] Url Request Success\n","202205091030 한국전력\n","1 건\n","[2022-05-09 11:24:42.807914] Url Request Success\n","202205091030 HMM\n","1 건\n","[2022-05-09 11:24:43.388324] Url Request Success\n","202205091030 크래프톤\n","1 건\n","[2022-05-09 11:24:43.945700] Url Request Success\n","202205091030 LG생활건강\n","1 건\n","[2022-05-09 11:24:44.544193] Url Request Success\n","202205091030 삼성생명\n","1 건\n","[2022-05-09 11:24:45.108286] Url Request Success\n","202205091030 하이브\n","1 건\n","[2022-05-09 11:24:45.643278] Url Request Success\n","202205091030 SK텔레콤\n","1 건\n","[2022-05-09 11:24:46.437745] Url Request Success\n","202205091030 두산중공업\n","1 건\n","[2022-05-09 11:24:47.025002] Url Request Success\n","202205091030 삼성전기\n","1 건\n","[2022-05-09 11:24:47.821426] Url Request Success\n","202205091030 SK바이오사이언스\n","1 건\n","[2022-05-09 11:24:48.370321] Url Request Success\n","202205091030 LG\n","1 건\n","[2022-05-09 11:24:49.217902] Url Request Success\n","202205091030 우리금융지주\n","1 건\n","[2022-05-09 11:24:49.743236] Url Request Success\n","202205091030 KT&G\n","1 건\n","[2022-05-09 11:24:50.274511] Url Request Success\n","202205091030 고려아연\n","1 건\n","[2022-05-09 11:24:50.856983] Url Request Success\n","202205091030 S-Oil\n","1 건\n","[2022-05-09 11:24:51.382800] Url Request Success\n","202205091030 삼성에스디에스\n","1 건\n","[2022-05-09 11:24:51.984521] Url Request Success\n","202205091030 현대중공업\n","1 건\n","[2022-05-09 11:24:52.568203] Url Request Success\n","202205091030 대한항공\n","1 건\n","[2022-05-09 11:24:53.362103] Url Request Success\n","202205091030 삼성화재\n","1 건\n","[2022-05-09 11:24:53.921098] Url Request Success\n","202205091030 엔씨소프트\n","1 건\n","[2022-05-09 11:24:54.509036] Url Request Success\n","202205091030 넷마블\n","1 건\n","[2022-05-09 11:24:55.305527] Url Request Success\n","202205091030 아모레퍼시픽\n","1 건\n","[2022-05-09 11:24:55.875311] Url Request Success\n","202205091030 포스코케미칼\n","1 건\n","[2022-05-09 11:24:56.409377] Url Request Success\n","202205091030 KT\n","1 건\n","[2022-05-09 11:24:56.990693] Url Request Success\n","202205091030 LG이노텍\n","1 건\n","[2022-05-09 11:24:57.534029] Url Request Success\n","202205091030 SK아이이테크놀로지\n","1 건\n","[2022-05-09 11:24:58.172374] Url Request Success\n","202205091030 기업은행\n","1 건\n","[2022-05-09 11:24:58.731391] Url Request Success\n","202205091030 SK스퀘어\n","1 건\n","[2022-05-09 11:24:59.311051] Url Request Success\n","202205091030 LG디스플레이\n","1 건\n","[2022-05-09 11:24:59.863869] Url Request Success\n","202205091030 현대글로비스\n","1 건\n","[2022-05-09 11:25:00.409376] Url Request Success\n","202205091030 롯데케미칼\n","1 건\n","[2022-05-09 11:25:01.175072] Url Request Success\n","202205091030 SK바이오팜\n","1 건\n","[2022-05-09 11:25:01.957540] Url Request Success\n","202205091030 한화솔루션\n","1 건\n","[2022-05-09 11:25:02.735093] Url Request Success\n","202205091030 한온시스템\n","1 건\n","[2022-05-09 11:25:03.349655] Url Request Success\n","202205091030 한국조선해양\n","1 건\n","[2022-05-09 11:25:03.945610] Url Request Success\n","202205091030 LG유플러스\n","1 건\n","[2022-05-09 11:25:04.492632] Url Request Success\n","202205091030 강원랜드\n","1 건\n","[2022-05-09 11:25:05.030330] Url Request Success\n","202205091030 SKC\n","1 건\n","[2022-05-09 11:25:05.555078] Url Request Success\n","202205091030 에스디바이오센서\n","1 건\n","[2022-05-09 11:25:06.127360] Url Request Success\n","202205091030 메리츠화재\n","1 건\n","[2022-05-09 11:25:06.678002] Url Request Success\n","202205091030 F&F\n","1 건\n","[2022-05-09 11:25:07.289985] Url Request Success\n","202205091030 CJ제일제당\n","1 건\n","[2022-05-09 11:25:07.886520] Url Request Success\n","202205091030 맥쿼리인프라\n","1 건\n","[2022-05-09 11:25:08.467749] Url Request Success\n","202205091030 현대제철\n","1 건\n","[2022-05-09 11:25:09.066206] Url Request Success\n","202205091030 현대건설\n","1 건\n","[2022-05-09 11:25:09.739485] Url Request Success\n","202205091030 메리츠금융지주\n","1 건\n","[2022-05-09 11:25:10.352291] Url Request Success\n","202205091030 미래에셋증권\n","1 건\n","[2022-05-09 11:25:10.884884] Url Request Success\n","202205091030 삼성엔지니어링\n","1 건\n","[2022-05-09 11:25:11.416098] Url Request Success\n","202205091030 코웨이\n","1 건\n","[2022-05-09 11:25:11.988258] Url Request Success\n","202205091030 삼성중공업\n","1 건\n","[2022-05-09 11:25:12.598330] Url Request Success\n","202205091030 DB손해보험\n","1 건\n","[2022-05-09 11:25:13.172372] Url Request Success\n","202205091030 금호석유\n","1 건\n","[2022-05-09 11:25:13.729365] Url Request Success\n","202205091030 메리츠증권\n","1 건\n","[2022-05-09 11:25:14.261243] Url Request Success\n","202205091030 일진머티리얼즈\n","1 건\n","[2022-05-09 11:25:15.075693] Url Request Success\n","202205091030 한국금융지주\n","1 건\n","[2022-05-09 11:25:15.682958] Url Request Success\n","202205091030 유한양행\n","1 건\n","[2022-05-09 11:25:16.278137] Url Request Success\n","202205091030 현대중공업지주\n","1 건\n","[2022-05-09 11:25:16.889567] Url Request Success\n","202205091030 한국타이어앤테크놀로지\n","1 건\n","[2022-05-09 11:25:17.481706] Url Request Success\n","202205091030 쌍용C&E\n","1 건\n","[2022-05-09 11:25:18.038780] Url Request Success\n","202205091030 한진칼\n","0 건\n","[2022-05-09 11:25:18.604415] Url Request Success\n","202205091030 한국항공우주\n","1 건\n","[2022-05-09 11:25:19.162704] Url Request Success\n","202205091030 GS\n","1 건\n","[2022-05-09 11:25:19.728341] Url Request Success\n","202205091030 GS건설\n","1 건\n","[2022-05-09 11:25:20.264536] Url Request Success\n","202205091030 이마트\n","1 건\n","[2022-05-09 11:25:20.876293] Url Request Success\n","202205091030 두산밥캣\n","1 건\n","[2022-05-09 11:25:21.462577] Url Request Success\n","202205091030 NH투자증권\n","1 건\n","[2022-05-09 11:25:22.237167] Url Request Success\n","202205091030 삼성카드\n","1 건\n","[2022-05-09 11:25:22.874330] Url Request Success\n","202205091030 삼성증권\n","1 건\n","[2022-05-09 11:25:23.473654] Url Request Success\n","202205091030 팬오션\n","1 건\n","[2022-05-09 11:25:24.106902] Url Request Success\n","202205091030 한국가스공사\n","1 건\n","[2022-05-09 11:25:24.690806] Url Request Success\n","202205091030 아모레G\n","0 건\n","[2022-05-09 11:25:25.233571] Url Request Success\n","202205091030 오리온\n","1 건\n","[2022-05-09 11:25:25.805883] Url Request Success\n","202205091030 롯데지주\n","1 건\n","[2022-05-09 11:25:26.356497] Url Request Success\n","202205091030 한미약품\n","1 건\n","[2022-05-09 11:25:26.954431] Url Request Success\n","202205091030 DB하이텍\n","1 건\n","[2022-05-09 11:25:27.492306] Url Request Success\n","202205091030 현대오토에버\n","1 건\n","[2022-05-09 11:25:28.060755] Url Request Success\n","202205091030 한전기술\n","1 건\n"]}]},{"cell_type":"code","source":["keyword = positive_keyword + negative_keyword\n","count_df_list = getKeywordCount(news, top100, keyword)"],"metadata":{"id":"BLMYQ8-WUsHI","executionInfo":{"status":"ok","timestamp":1652096217625,"user_tz":-540,"elapsed":140123,"user":{"displayName":"Ha Jun","userId":"06515366270624202236"}}},"execution_count":122,"outputs":[]},{"cell_type":"code","source":["count_df_list[0][count_df_list[0].columns[1:]].to_numpy()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y9TqRnPgYmMW","executionInfo":{"status":"ok","timestamp":1652096217628,"user_tz":-540,"elapsed":87,"user":{"displayName":"Ha Jun","userId":"06515366270624202236"}},"outputId":"ed1c7330-b781-47e9-bd49-cf764d3fefb2"},"execution_count":123,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1]])"]},"metadata":{},"execution_count":123}]},{"cell_type":"code","source":["best_model, loss_list = train(train_x, train_y, loss_function, optimizer, model)"],"metadata":{"id":"fUESSeLnYTEM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652096218829,"user_tz":-540,"elapsed":1232,"user":{"displayName":"Ha Jun","userId":"06515366270624202236"}},"outputId":"4c8209b2-4450-4edb-d340-2fc68b36a1fd"},"execution_count":124,"outputs":[{"output_type":"stream","name":"stdout","text":["Parameter containing:\n","tensor([[-0.0163,  0.0111,  0.1843,  0.1929, -0.1197,  0.2214,  0.1100, -0.2405,\n","         -0.2250, -0.1409,  0.0254]], requires_grad=True)\n","0\n","[ 2.  5.  1.  2.  0.  0.  0.  0.  2.  0. 20.]\n","inX: tensor([[ 2.,  5.,  1.,  2.,  0.,  0.,  0.,  0.,  2.,  0., 20.]])\n","inSquee tensor([[[ 2.,  5.,  1.,  2.,  0.,  0.,  0.,  0.,  2.,  0., 20.]]])\n","outX: tensor([[[0.3524]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[0.3524]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(62.5299, grad_fn=<MseLossBackward0>)\n","1\n","[ 0.  0.  1.  0.  0.  1.  0.  0.  0.  0. 11.]\n","inX: tensor([[ 0.,  0.,  1.,  0.,  0.,  1.,  0.,  0.,  0.,  0., 11.]])\n","inSquee tensor([[[ 0.,  0.,  1.,  0.,  0.,  1.,  0.,  0.,  0.,  0., 11.]]])\n","outX: tensor([[[0.7372]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[0.7372]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(2.6479, grad_fn=<MseLossBackward0>)\n","2\n","[ 1.  0.  5.  0.  0.  0.  0.  0.  1.  0. 19.]\n","inX: tensor([[ 1.,  0.,  5.,  0.,  0.,  0.,  0.,  0.,  1.,  0., 19.]])\n","inSquee tensor([[[ 1.,  0.,  5.,  0.,  0.,  0.,  0.,  0.,  1.,  0., 19.]]])\n","outX: tensor([[[1.4110]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[1.4110]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(0.0166, grad_fn=<MseLossBackward0>)\n","3\n","[4. 3. 2. 0. 1. 0. 0. 0. 0. 0. 0.]\n","inX: tensor([[4., 3., 2., 0., 1., 0., 0., 0., 0., 0., 0.]])\n","inSquee tensor([[[4., 3., 2., 0., 1., 0., 0., 0., 0., 0., 0.]]])\n","outX: tensor([[[-0.0421]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-0.0421]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(0.0163, grad_fn=<MseLossBackward0>)\n","4\n","[26.  7.  6.  0.  0.  0.  0.  0.  0.  0.  2.]\n","inX: tensor([[26.,  7.,  6.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  2.]])\n","inSquee tensor([[[26.,  7.,  6.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  2.]]])\n","outX: tensor([[[0.7113]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[0.7113]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(0.9050, grad_fn=<MseLossBackward0>)\n","5\n","[ 2.  7.  7.  0.  0.  1.  0.  0.  1.  2. 34.]\n","inX: tensor([[ 2.,  7.,  7.,  0.,  0.,  1.,  0.,  0.,  1.,  2., 34.]])\n","inSquee tensor([[[ 2.,  7.,  7.,  0.,  0.,  1.,  0.,  0.,  1.,  2., 34.]]])\n","outX: tensor([[[2.6199]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[2.6199]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(15.7603, grad_fn=<MseLossBackward0>)\n","6\n","[1. 4. 2. 0. 0. 0. 0. 0. 0. 1. 1.]\n","inX: tensor([[1., 4., 2., 0., 0., 0., 0., 0., 0., 1., 1.]])\n","inSquee tensor([[[1., 4., 2., 0., 0., 0., 0., 0., 0., 1., 1.]]])\n","outX: tensor([[[-0.0289]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-0.0289]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(1.2125, grad_fn=<MseLossBackward0>)\n","7\n","[0. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0.]\n","inX: tensor([[0., 0., 2., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inSquee tensor([[[0., 0., 2., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n","outX: tensor([[[0.0575]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[0.0575]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(0.0033, grad_fn=<MseLossBackward0>)\n","8\n","[2. 3. 1. 0. 0. 0. 0. 0. 0. 1. 0.]\n","inX: tensor([[2., 3., 1., 0., 0., 0., 0., 0., 0., 1., 0.]])\n","inSquee tensor([[[2., 3., 1., 0., 0., 0., 0., 0., 0., 1., 0.]]])\n","outX: tensor([[[-0.2702]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-0.2702]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(0.3362, grad_fn=<MseLossBackward0>)\n","9\n","[7. 0. 6. 2. 0. 0. 0. 0. 0. 0. 0.]\n","inX: tensor([[7., 0., 6., 2., 0., 0., 0., 0., 0., 0., 0.]])\n","inSquee tensor([[[7., 0., 6., 2., 0., 0., 0., 0., 0., 0., 0.]]])\n","outX: tensor([[[1.0208]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[1.0208]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(6.6606, grad_fn=<MseLossBackward0>)\n","10\n","[ 2. 10. 10.  0.  0.  1.  0.  0.  0.  1.  4.]\n","inX: tensor([[ 2., 10., 10.,  0.,  0.,  1.,  0.,  0.,  0.,  1.,  4.]])\n","inSquee tensor([[[ 2., 10., 10.,  0.,  0.,  1.,  0.,  0.,  0.,  1.,  4.]]])\n","outX: tensor([[[1.6958]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[1.6958]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(8.3856, grad_fn=<MseLossBackward0>)\n","11\n","[1. 9. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","inX: tensor([[1., 9., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inSquee tensor([[[1., 9., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n","outX: tensor([[[-0.2804]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-0.2804]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(2.6230, grad_fn=<MseLossBackward0>)\n","12\n","[ 4.  1. 15.  0.  1.  4.  0.  0.  4. 13. 11.]\n","inX: tensor([[ 4.,  1., 15.,  0.,  1.,  4.,  0.,  0.,  4., 13., 11.]])\n","inSquee tensor([[[ 4.,  1., 15.,  0.,  1.,  4.,  0.,  0.,  4., 13., 11.]]])\n","outX: tensor([[[0.4110]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[0.4110]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(13.3297, grad_fn=<MseLossBackward0>)\n","13\n","[ 2.  7.  2.  2.  3.  1.  0.  0.  1.  0. 18.]\n","inX: tensor([[ 2.,  7.,  2.,  2.,  3.,  1.,  0.,  0.,  1.,  0., 18.]])\n","inSquee tensor([[[ 2.,  7.,  2.,  2.,  3.,  1.,  0.,  0.,  1.,  0., 18.]]])\n","outX: tensor([[[0.2771]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[0.2771]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(10.4143, grad_fn=<MseLossBackward0>)\n","14\n","[ 7.  4. 12.  0.  0.  1.  0.  0.  3.  2. 14.]\n","inX: tensor([[ 7.,  4., 12.,  0.,  0.,  1.,  0.,  0.,  3.,  2., 14.]])\n","inSquee tensor([[[ 7.,  4., 12.,  0.,  0.,  1.,  0.,  0.,  3.,  2., 14.]]])\n","outX: tensor([[[0.6586]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[0.6586]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(6.5978, grad_fn=<MseLossBackward0>)\n","15\n","[ 9. 31. 24.  2.  0.  3.  0.  0.  4.  3. 35.]\n","inX: tensor([[ 9., 31., 24.,  2.,  0.,  3.,  0.,  0.,  4.,  3., 35.]])\n","inSquee tensor([[[ 9., 31., 24.,  2.,  0.,  3.,  0.,  0.,  4.,  3., 35.]]])\n","outX: tensor([[[2.4277]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[2.4277]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(8.4547, grad_fn=<MseLossBackward0>)\n","16\n","[ 0.  1.  2.  0.  0.  1.  0.  0.  1.  0. 23.]\n","inX: tensor([[ 0.,  1.,  2.,  0.,  0.,  1.,  0.,  0.,  1.,  0., 23.]])\n","inSquee tensor([[[ 0.,  1.,  2.,  0.,  0.,  1.,  0.,  0.,  1.,  0., 23.]]])\n","outX: tensor([[[-0.6004]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-0.6004]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(3.6878, grad_fn=<MseLossBackward0>)\n","17\n","[1. 2. 2. 1. 0. 0. 0. 0. 0. 0. 1.]\n","inX: tensor([[1., 2., 2., 1., 0., 0., 0., 0., 0., 0., 1.]])\n","inSquee tensor([[[1., 2., 2., 1., 0., 0., 0., 0., 0., 0., 1.]]])\n","outX: tensor([[[0.0668]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[0.0668]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(0.1392, grad_fn=<MseLossBackward0>)\n","18\n","[2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","inX: tensor([[2., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inSquee tensor([[[2., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n","outX: tensor([[[-0.3796]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-0.3796]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(0.4495, grad_fn=<MseLossBackward0>)\n","19\n","[ 0.  1.  5.  0.  1.  3.  0.  0.  0.  1. 49.]\n","inX: tensor([[ 0.,  1.,  5.,  0.,  1.,  3.,  0.,  0.,  0.,  1., 49.]])\n","inSquee tensor([[[ 0.,  1.,  5.,  0.,  1.,  3.,  0.,  0.,  0.,  1., 49.]]])\n","outX: tensor([[[0.0326]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[0.0326]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(3.6963, grad_fn=<MseLossBackward0>)\n","20\n","[ 0.  0.  1.  0.  0.  2.  0.  0.  0.  1. 14.]\n","inX: tensor([[ 0.,  0.,  1.,  0.,  0.,  2.,  0.,  0.,  0.,  1., 14.]])\n","inSquee tensor([[[ 0.,  0.,  1.,  0.,  0.,  2.,  0.,  0.,  0.,  1., 14.]]])\n","outX: tensor([[[-0.3600]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-0.3600]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(6.0026, grad_fn=<MseLossBackward0>)\n","21\n","[0. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0.]\n","inX: tensor([[0., 0., 2., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inSquee tensor([[[0., 0., 2., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n","outX: tensor([[[-0.0328]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-0.0328]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(1.3160, grad_fn=<MseLossBackward0>)\n","22\n","[10. 30. 26.  9.  1.  5.  0.  0.  4.  2. 54.]\n","inX: tensor([[10., 30., 26.,  9.,  1.,  5.,  0.,  0.,  4.,  2., 54.]])\n","inSquee tensor([[[10., 30., 26.,  9.,  1.,  5.,  0.,  0.,  4.,  2., 54.]]])\n","outX: tensor([[[1.4157]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[1.4157]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(0.8569, grad_fn=<MseLossBackward0>)\n","23\n","[ 7. 18. 30.  4.  1.  3.  0.  0.  0.  1. 25.]\n","inX: tensor([[ 7., 18., 30.,  4.,  1.,  3.,  0.,  0.,  0.,  1., 25.]])\n","inSquee tensor([[[ 7., 18., 30.,  4.,  1.,  3.,  0.,  0.,  0.,  1., 25.]]])\n","outX: tensor([[[2.6765]]], grad_fn=<AddBackward0>)\n","the value of logit "]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:529: UserWarning: Using a target size (torch.Size([])) that is different to the input size (torch.Size([1, 1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","  return F.mse_loss(input, target, reduction=self.reduction)\n"]},{"output_type":"stream","name":"stdout","text":["tensor([[[2.6765]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(7.1636, grad_fn=<MseLossBackward0>)\n","24\n","[1. 0. 2. 0. 1. 0. 0. 0. 0. 0. 0.]\n","inX: tensor([[1., 0., 2., 0., 1., 0., 0., 0., 0., 0., 0.]])\n","inSquee tensor([[[1., 0., 2., 0., 1., 0., 0., 0., 0., 0., 0.]]])\n","outX: tensor([[[-0.2441]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-0.2441]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(2.5093, grad_fn=<MseLossBackward0>)\n","25\n","[ 5.  7.  6.  0.  0.  1.  0.  0.  1.  1. 19.]\n","inX: tensor([[ 5.,  7.,  6.,  0.,  0.,  1.,  0.,  0.,  1.,  1., 19.]])\n","inSquee tensor([[[ 5.,  7.,  6.,  0.,  0.,  1.,  0.,  0.,  1.,  1., 19.]]])\n","outX: tensor([[[-1.4832]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-1.4832]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(0.1203, grad_fn=<MseLossBackward0>)\n","26\n","[ 2. 30.  7.  0.  2.  1.  0.  0.  1.  0. 18.]\n","inX: tensor([[ 2., 30.,  7.,  0.,  2.,  1.,  0.,  0.,  1.,  0., 18.]])\n","inSquee tensor([[[ 2., 30.,  7.,  0.,  2.,  1.,  0.,  0.,  1.,  0., 18.]]])\n","outX: tensor([[[-2.1845]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-2.1845]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(3.4428, grad_fn=<MseLossBackward0>)\n","27\n","[4. 9. 9. 0. 0. 0. 0. 0. 0. 1. 3.]\n","inX: tensor([[4., 9., 9., 0., 0., 0., 0., 0., 0., 1., 3.]])\n","inSquee tensor([[[4., 9., 9., 0., 0., 0., 0., 0., 0., 1., 3.]]])\n","outX: tensor([[[-0.2983]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-0.2983]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(7.6270, grad_fn=<MseLossBackward0>)\n","28\n","[ 7. 26. 10.  4.  4.  3.  0.  0.  4.  1. 31.]\n","inX: tensor([[ 7., 26., 10.,  4.,  4.,  3.,  0.,  0.,  4.,  1., 31.]])\n","inSquee tensor([[[ 7., 26., 10.,  4.,  4.,  3.,  0.,  0.,  4.,  1., 31.]]])\n","outX: tensor([[[-3.4485]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-3.4485]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(4.7022, grad_fn=<MseLossBackward0>)\n","29\n","[ 1.  1.  3.  0.  0.  1.  0.  0.  0.  0. 18.]\n","inX: tensor([[ 1.,  1.,  3.,  0.,  0.,  1.,  0.,  0.,  0.,  0., 18.]])\n","inSquee tensor([[[ 1.,  1.,  3.,  0.,  0.,  1.,  0.,  0.,  0.,  0., 18.]]])\n","outX: tensor([[[-0.9197]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-0.9197]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(0.5480, grad_fn=<MseLossBackward0>)\n","30\n","[ 6.  3. 11.  0.  0.  6.  0.  0.  0.  1. 23.]\n","inX: tensor([[ 6.,  3., 11.,  0.,  0.,  6.,  0.,  0.,  0.,  1., 23.]])\n","inSquee tensor([[[ 6.,  3., 11.,  0.,  0.,  6.,  0.,  0.,  0.,  1., 23.]]])\n","outX: tensor([[[0.1910]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[0.1910]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(0.8855, grad_fn=<MseLossBackward0>)\n","31\n","[ 3. 12.  0.  2.  2.  0.  0.  0.  0.  1.  6.]\n","inX: tensor([[ 3., 12.,  0.,  2.,  2.,  0.,  0.,  0.,  0.,  1.,  6.]])\n","inSquee tensor([[[ 3., 12.,  0.,  2.,  2.,  0.,  0.,  0.,  0.,  1.,  6.]]])\n","outX: tensor([[[-1.3933]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-1.3933]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(1.4964, grad_fn=<MseLossBackward0>)\n","32\n","[0. 4. 5. 1. 0. 1. 0. 0. 1. 0. 0.]\n","inX: tensor([[0., 4., 5., 1., 0., 1., 0., 0., 1., 0., 0.]])\n","inSquee tensor([[[0., 4., 5., 1., 0., 1., 0., 0., 1., 0., 0.]]])\n","outX: tensor([[[0.2441]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[0.2441]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(10.6543, grad_fn=<MseLossBackward0>)\n","33\n","[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n","inX: tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inSquee tensor([[[0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n","outX: tensor([[[-0.2005]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-0.2005]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(0.0224, grad_fn=<MseLossBackward0>)\n","34\n","[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","inX: tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inSquee tensor([[[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n","outX: tensor([[[-0.3499]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-0.3499]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(0.1224, grad_fn=<MseLossBackward0>)\n","35\n","[ 8. 34. 24.  4.  0.  1.  0.  0.  1.  0. 21.]\n","inX: tensor([[ 8., 34., 24.,  4.,  0.,  1.,  0.,  0.,  1.,  0., 21.]])\n","inSquee tensor([[[ 8., 34., 24.,  4.,  0.,  1.,  0.,  0.,  1.,  0., 21.]]])\n","outX: tensor([[[-0.2505]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-0.2505]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(1.3443, grad_fn=<MseLossBackward0>)\n","36\n","[2. 1. 1. 0. 1. 0. 0. 0. 1. 0. 1.]\n","inX: tensor([[2., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1.]])\n","inSquee tensor([[[2., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1.]]])\n","outX: tensor([[[-0.7726]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-0.7726]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(0.0053, grad_fn=<MseLossBackward0>)\n","37\n","[ 2. 13.  2.  0.  1.  2.  0.  0.  1.  1. 25.]\n","inX: tensor([[ 2., 13.,  2.,  0.,  1.,  2.,  0.,  0.,  1.,  1., 25.]])\n","inSquee tensor([[[ 2., 13.,  2.,  0.,  1.,  2.,  0.,  0.,  1.,  1., 25.]]])\n","outX: tensor([[[-2.7119]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-2.7119]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(0.0010, grad_fn=<MseLossBackward0>)\n","38\n","[15. 33. 16.  0.  0.  6.  0.  0.  2.  1. 64.]\n","inX: tensor([[15., 33., 16.,  0.,  0.,  6.,  0.,  0.,  2.,  1., 64.]])\n","inSquee tensor([[[15., 33., 16.,  0.,  0.,  6.,  0.,  0.,  2.,  1., 64.]]])\n","outX: tensor([[[-4.9403]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-4.9403]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(25.6069, grad_fn=<MseLossBackward0>)\n","39\n","[3. 1. 1. 0. 0. 0. 0. 0. 0. 0. 2.]\n","inX: tensor([[3., 1., 1., 0., 0., 0., 0., 0., 0., 0., 2.]])\n","inSquee tensor([[[3., 1., 1., 0., 0., 0., 0., 0., 0., 0., 2.]]])\n","outX: tensor([[[-0.3111]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-0.3111]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(0.0172, grad_fn=<MseLossBackward0>)\n","40\n","[17. 15. 11.  1.  0.  0.  0.  0.  5.  0.  4.]\n","inX: tensor([[17., 15., 11.,  1.,  0.,  0.,  0.,  0.,  5.,  0.,  4.]])\n","inSquee tensor([[[17., 15., 11.,  1.,  0.,  0.,  0.,  0.,  5.,  0.,  4.]]])\n","outX: tensor([[[-0.8020]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-0.8020]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(0.9565, grad_fn=<MseLossBackward0>)\n","41\n","[0. 3. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","inX: tensor([[0., 3., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inSquee tensor([[[0., 3., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n","outX: tensor([[[-0.3688]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-0.3688]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(0.4111, grad_fn=<MseLossBackward0>)\n","42\n","[ 7. 23. 13.  0.  1.  1.  0.  0.  1.  3. 24.]\n","inX: tensor([[ 7., 23., 13.,  0.,  1.,  1.,  0.,  0.,  1.,  3., 24.]])\n","inSquee tensor([[[ 7., 23., 13.,  0.,  1.,  1.,  0.,  0.,  1.,  3., 24.]]])\n","outX: tensor([[[-0.3041]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-0.3041]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(0.3202, grad_fn=<MseLossBackward0>)\n","43\n","[ 2. 38.  8.  0.  0.  2.  0.  0.  2.  0. 17.]\n","inX: tensor([[ 2., 38.,  8.,  0.,  0.,  2.,  0.,  0.,  2.,  0., 17.]])\n","inSquee tensor([[[ 2., 38.,  8.,  0.,  0.,  2.,  0.,  0.,  2.,  0., 17.]]])\n","outX: tensor([[[-0.5673]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-0.5673]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(2.3800, grad_fn=<MseLossBackward0>)\n","44\n","[ 8. 14. 10.  0.  0.  4.  0.  0.  1.  1. 14.]\n","inX: tensor([[ 8., 14., 10.,  0.,  0.,  4.,  0.,  0.,  1.,  1., 14.]])\n","inSquee tensor([[[ 8., 14., 10.,  0.,  0.,  4.,  0.,  0.,  1.,  1., 14.]]])\n","outX: tensor([[[0.2812]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[0.2812]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(5.2957, grad_fn=<MseLossBackward0>)\n","45\n","[1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 6.]\n","inX: tensor([[1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 6.]])\n","inSquee tensor([[[1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 6.]]])\n","outX: tensor([[[-0.3610]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-0.3610]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(0.2391, grad_fn=<MseLossBackward0>)\n","46\n","[5. 5. 6. 0. 0. 0. 0. 0. 1. 0. 5.]\n","inX: tensor([[5., 5., 6., 0., 0., 0., 0., 0., 1., 0., 5.]])\n","inSquee tensor([[[5., 5., 6., 0., 0., 0., 0., 0., 1., 0., 5.]]])\n","outX: tensor([[[-0.4182]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-0.4182]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(0.5803, grad_fn=<MseLossBackward0>)\n","47\n","[ 9. 21.  2.  0.  0.  0.  0.  0.  0.  1.  9.]\n","inX: tensor([[ 9., 21.,  2.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  9.]])\n","inSquee tensor([[[ 9., 21.,  2.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  9.]]])\n","outX: tensor([[[-1.7177]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-1.7177]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(0.9454, grad_fn=<MseLossBackward0>)\n","48\n","[ 4. 13. 19.  0.  0.  4.  0.  0.  2.  2. 43.]\n","inX: tensor([[ 4., 13., 19.,  0.,  0.,  4.,  0.,  0.,  2.,  2., 43.]])\n","inSquee tensor([[[ 4., 13., 19.,  0.,  0.,  4.,  0.,  0.,  2.,  2., 43.]]])\n","outX: tensor([[[-0.0381]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-0.0381]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(9.1924, grad_fn=<MseLossBackward0>)\n","49\n","[ 43. 109.   7.   1.   1.   0.   0.   0.   0.   0.   9.]\n","inX: tensor([[ 43., 109.,   7.,   1.,   1.,   0.,   0.,   0.,   0.,   0.,   9.]])\n","inSquee tensor([[[ 43., 109.,   7.,   1.,   1.,   0.,   0.,   0.,   0.,   0.,   9.]]])\n","outX: tensor([[[-7.9597]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-7.9597]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(67.2353, grad_fn=<MseLossBackward0>)\n","50\n","[0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","inX: tensor([[0., 2., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inSquee tensor([[[0., 2., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n","outX: tensor([[[-0.0583]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-0.0583]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(0.8495, grad_fn=<MseLossBackward0>)\n","51\n","[ 0.  1.  3.  1.  0.  3.  0.  0.  1.  1. 17.]\n","inX: tensor([[ 0.,  1.,  3.,  1.,  0.,  3.,  0.,  0.,  1.,  1., 17.]])\n","inSquee tensor([[[ 0.,  1.,  3.,  1.,  0.,  3.,  0.,  0.,  1.,  1., 17.]]])\n","outX: tensor([[[-0.0283]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-0.0283]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(2.1955, grad_fn=<MseLossBackward0>)\n","52\n","[  0.   3.  15.   0.   1.  11.   0.   0.   0.   1. 104.]\n","inX: tensor([[  0.,   3.,  15.,   0.,   1.,  11.,   0.,   0.,   0.,   1., 104.]])\n","inSquee tensor([[[  0.,   3.,  15.,   0.,   1.,  11.,   0.,   0.,   0.,   1., 104.]]])\n","outX: tensor([[[-0.4885]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-0.4885]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(0.2386, grad_fn=<MseLossBackward0>)\n","53\n","[ 7. 23.  4.  0.  5.  1.  0.  0.  0.  1. 28.]\n","inX: tensor([[ 7., 23.,  4.,  0.,  5.,  1.,  0.,  0.,  0.,  1., 28.]])\n","inSquee tensor([[[ 7., 23.,  4.,  0.,  5.,  1.,  0.,  0.,  0.,  1., 28.]]])\n","outX: tensor([[[1.7569]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[1.7569]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(16.0555, grad_fn=<MseLossBackward0>)\n","54\n","[ 1.  5.  7.  0.  1.  0.  0.  0.  2.  1. 17.]\n","inX: tensor([[ 1.,  5.,  7.,  0.,  1.,  0.,  0.,  0.,  2.,  1., 17.]])\n","inSquee tensor([[[ 1.,  5.,  7.,  0.,  1.,  0.,  0.,  0.,  2.,  1., 17.]]])\n","outX: tensor([[[-0.6810]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-0.6810]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(0.0789, grad_fn=<MseLossBackward0>)\n","55\n","[ 7. 24. 21.  4.  0.  1.  0.  0.  1.  2. 33.]\n","inX: tensor([[ 7., 24., 21.,  4.,  0.,  1.,  0.,  0.,  1.,  2., 33.]])\n","inSquee tensor([[[ 7., 24., 21.,  4.,  0.,  1.,  0.,  0.,  1.,  2., 33.]]])\n","outX: tensor([[[3.2373]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[3.2373]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(10.4800, grad_fn=<MseLossBackward0>)\n","56\n","[ 4.  0. 15.  2.  1.  1.  0.  0.  1.  0.  9.]\n","inX: tensor([[ 4.,  0., 15.,  2.,  1.,  1.,  0.,  0.,  1.,  0.,  9.]])\n","inSquee tensor([[[ 4.,  0., 15.,  2.,  1.,  1.,  0.,  0.,  1.,  0.,  9.]]])\n","outX: tensor([[[0.6556]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[0.6556]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(4.6466, grad_fn=<MseLossBackward0>)\n","57\n","[0. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0.]\n","inX: tensor([[0., 0., 2., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inSquee tensor([[[0., 0., 2., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n","outX: tensor([[[-0.1445]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-0.1445]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(0.0021, grad_fn=<MseLossBackward0>)\n","58\n","[ 3.  7.  6.  0.  1.  1.  0.  0.  2.  1. 14.]\n","inX: tensor([[ 3.,  7.,  6.,  0.,  1.,  1.,  0.,  0.,  2.,  1., 14.]])\n","inSquee tensor([[[ 3.,  7.,  6.,  0.,  1.,  1.,  0.,  0.,  2.,  1., 14.]]])\n","outX: tensor([[[-0.7571]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-0.7571]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(57.3484, grad_fn=<MseLossBackward0>)\n","59\n","[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","inX: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inSquee tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n","outX: tensor([[[-0.3094]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-0.3094]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(6.0979, grad_fn=<MseLossBackward0>)\n","60\n","[ 0.  1.  4.  0.  0.  0.  0.  0.  0.  0. 35.]\n","inX: tensor([[ 0.,  1.,  4.,  0.,  0.,  0.,  0.,  0.,  0.,  0., 35.]])\n","inSquee tensor([[[ 0.,  1.,  4.,  0.,  0.,  0.,  0.,  0.,  0.,  0., 35.]]])\n","outX: tensor([[[-3.3455]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-3.3455]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(17.7701, grad_fn=<MseLossBackward0>)\n","61\n","[ 0.  0.  1.  0.  0.  1.  0.  0.  0.  0. 17.]\n","inX: tensor([[ 0.,  0.,  1.,  0.,  0.,  1.,  0.,  0.,  0.,  0., 17.]])\n","inSquee tensor([[[ 0.,  0.,  1.,  0.,  0.,  1.,  0.,  0.,  0.,  0., 17.]]])\n","outX: tensor([[[-1.1786]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-1.1786]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(0.0952, grad_fn=<MseLossBackward0>)\n","62\n","[ 0.  0.  0.  0.  0.  2.  0.  0.  0.  2. 24.]\n","inX: tensor([[ 0.,  0.,  0.,  0.,  0.,  2.,  0.,  0.,  0.,  2., 24.]])\n","inSquee tensor([[[ 0.,  0.,  0.,  0.,  0.,  2.,  0.,  0.,  0.,  2., 24.]]])\n","outX: tensor([[[-1.8226]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-1.8226]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(0.0248, grad_fn=<MseLossBackward0>)\n","63\n","[ 6.  2.  8.  0.  0.  5.  0.  0.  0.  1. 42.]\n","inX: tensor([[ 6.,  2.,  8.,  0.,  0.,  5.,  0.,  0.,  0.,  1., 42.]])\n","inSquee tensor([[[ 6.,  2.,  8.,  0.,  0.,  5.,  0.,  0.,  0.,  1., 42.]]])\n","outX: tensor([[[-1.4504]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-1.4504]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(0.0143, grad_fn=<MseLossBackward0>)\n","64\n","[17. 29. 15.  0.  0.  1.  0.  0.  4.  0. 24.]\n","inX: tensor([[17., 29., 15.,  0.,  0.,  1.,  0.,  0.,  4.,  0., 24.]])\n","inSquee tensor([[[17., 29., 15.,  0.,  0.,  1.,  0.,  0.,  4.,  0., 24.]]])\n","outX: tensor([[[0.9214]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[0.9214]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(0.8491, grad_fn=<MseLossBackward0>)\n","65\n","[ 3.  8. 15.  3.  1.  6.  0.  0.  4.  3. 67.]\n","inX: tensor([[ 3.,  8., 15.,  3.,  1.,  6.,  0.,  0.,  4.,  3., 67.]])\n","inSquee tensor([[[ 3.,  8., 15.,  3.,  1.,  6.,  0.,  0.,  4.,  3., 67.]]])\n","outX: tensor([[[-3.2478]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-3.2478]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(17.4541, grad_fn=<MseLossBackward0>)\n","66\n","[25.  5.  2.  1.  0.  0.  0.  0.  0.  0. 12.]\n","inX: tensor([[25.,  5.,  2.,  1.,  0.,  0.,  0.,  0.,  0.,  0., 12.]])\n","inSquee tensor([[[25.,  5.,  2.,  1.,  0.,  0.,  0.,  0.,  0.,  0., 12.]]])\n","outX: tensor([[[0.4739]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[0.4739]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(9.9470, grad_fn=<MseLossBackward0>)\n","67\n","[6. 5. 2. 0. 0. 0. 0. 0. 0. 0. 6.]\n","inX: tensor([[6., 5., 2., 0., 0., 0., 0., 0., 0., 0., 6.]])\n","inSquee tensor([[[6., 5., 2., 0., 0., 0., 0., 0., 0., 0., 6.]]])\n","outX: tensor([[[0.0612]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[0.0612]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(1.9915, grad_fn=<MseLossBackward0>)\n","68\n","[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 4.]\n","inX: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 4.]])\n","inSquee tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 4.]]])\n","outX: tensor([[[-0.4132]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-0.4132]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(0.0881, grad_fn=<MseLossBackward0>)\n","69\n","[17. 15. 10.  0.  1.  0.  0.  0.  1.  0. 16.]\n","inX: tensor([[17., 15., 10.,  0.,  1.,  0.,  0.,  0.,  1.,  0., 16.]])\n","inSquee tensor([[[17., 15., 10.,  0.,  1.,  0.,  0.,  0.,  1.,  0., 16.]]])\n","outX: tensor([[[0.7479]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[0.7479]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(3.8336, grad_fn=<MseLossBackward0>)\n","70\n","[10. 18. 10.  0.  1.  0.  0.  0.  1.  0. 13.]\n","inX: tensor([[10., 18., 10.,  0.,  1.,  0.,  0.,  0.,  1.,  0., 13.]])\n","inSquee tensor([[[10., 18., 10.,  0.,  1.,  0.,  0.,  0.,  1.,  0., 13.]]])\n","outX: tensor([[[0.8241]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[0.8241]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(2.4153, grad_fn=<MseLossBackward0>)\n","71\n","[2. 7. 5. 1. 0. 0. 0. 0. 1. 1. 3.]\n","inX: tensor([[2., 7., 5., 1., 0., 0., 0., 0., 1., 1., 3.]])\n","inSquee tensor([[[2., 7., 5., 1., 0., 0., 0., 0., 1., 1., 3.]]])\n","outX: tensor([[[0.1782]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[0.1782]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(0.4465, grad_fn=<MseLossBackward0>)\n","72\n","[  2.   6.  24.   0.   0.  17.   0.   0.   0.   4. 134.]\n","inX: tensor([[  2.,   6.,  24.,   0.,   0.,  17.,   0.,   0.,   0.,   4., 134.]])\n","inSquee tensor([[[  2.,   6.,  24.,   0.,   0.,  17.,   0.,   0.,   0.,   4., 134.]]])\n","outX: tensor([[[-0.1716]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-0.1716]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(0.0910, grad_fn=<MseLossBackward0>)\n","73\n","[ 0.  5. 10.  0.  0.  9.  0.  0.  0.  1. 37.]\n","inX: tensor([[ 0.,  5., 10.,  0.,  0.,  9.,  0.,  0.,  0.,  1., 37.]])\n","inSquee tensor([[[ 0.,  5., 10.,  0.,  0.,  9.,  0.,  0.,  0.,  1., 37.]]])\n","outX: tensor([[[1.4600]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[1.4600]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(8.0088, grad_fn=<MseLossBackward0>)\n","74\n","[ 5.  6.  7.  0.  0.  4.  0.  0.  0.  0. 12.]\n","inX: tensor([[ 5.,  6.,  7.,  0.,  0.,  4.,  0.,  0.,  0.,  0., 12.]])\n","inSquee tensor([[[ 5.,  6.,  7.,  0.,  0.,  4.,  0.,  0.,  0.,  0., 12.]]])\n","outX: tensor([[[0.6973]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[0.6973]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(4.3152, grad_fn=<MseLossBackward0>)\n","75\n","[9. 3. 1. 0. 0. 1. 0. 0. 0. 0. 0.]\n","inX: tensor([[9., 3., 1., 0., 0., 1., 0., 0., 0., 0., 0.]])\n","inSquee tensor([[[9., 3., 1., 0., 0., 1., 0., 0., 0., 0., 0.]]])\n","outX: tensor([[[-0.0449]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-0.0449]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(0.9314, grad_fn=<MseLossBackward0>)\n","76\n","[ 0.  1.  8.  0.  0.  1.  0.  0.  1.  1. 18.]\n","inX: tensor([[ 0.,  1.,  8.,  0.,  0.,  1.,  0.,  0.,  1.,  1., 18.]])\n","inSquee tensor([[[ 0.,  1.,  8.,  0.,  0.,  1.,  0.,  0.,  1.,  1., 18.]]])\n","outX: tensor([[[-0.8813]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-0.8813]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(1.6607, grad_fn=<MseLossBackward0>)\n","77\n","[ 4.  3.  2.  0.  0.  3.  0.  0.  0.  3. 13.]\n","inX: tensor([[ 4.,  3.,  2.,  0.,  0.,  3.,  0.,  0.,  0.,  3., 13.]])\n","inSquee tensor([[[ 4.,  3.,  2.,  0.,  0.,  3.,  0.,  0.,  0.,  3., 13.]]])\n","outX: tensor([[[-0.7356]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-0.7356]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(0.0212, grad_fn=<MseLossBackward0>)\n","78\n","[ 6. 12. 19.  1.  0.  1.  0.  0.  1.  0.  1.]\n","inX: tensor([[ 6., 12., 19.,  1.,  0.,  1.,  0.,  0.,  1.,  0.,  1.]])\n","inSquee tensor([[[ 6., 12., 19.,  1.,  0.,  1.,  0.,  0.,  1.,  0.,  1.]]])\n","outX: tensor([[[1.6254]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[1.6254]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(13.5086, grad_fn=<MseLossBackward0>)\n","79\n","[ 2. 40.  7.  0.  1.  2.  0.  0.  2.  1. 45.]\n","inX: tensor([[ 2., 40.,  7.,  0.,  1.,  2.,  0.,  0.,  2.,  1., 45.]])\n","inSquee tensor([[[ 2., 40.,  7.,  0.,  1.,  2.,  0.,  0.,  2.,  1., 45.]]])\n","outX: tensor([[[-1.0578]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-1.0578]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(0.2230, grad_fn=<MseLossBackward0>)\n","80\n","[10. 16. 14.  0.  0.  1.  0.  0.  1.  2. 11.]\n","inX: tensor([[10., 16., 14.,  0.,  0.,  1.,  0.,  0.,  1.,  2., 11.]])\n","inSquee tensor([[[10., 16., 14.,  0.,  0.,  1.,  0.,  0.,  1.,  2., 11.]]])\n","outX: tensor([[[-0.1769]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-0.1769]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(0.0014, grad_fn=<MseLossBackward0>)\n","81\n","[ 1.  3.  3.  0.  0.  0.  0.  0.  1.  1. 12.]\n","inX: tensor([[ 1.,  3.,  3.,  0.,  0.,  0.,  0.,  0.,  1.,  1., 12.]])\n","inSquee tensor([[[ 1.,  3.,  3.,  0.,  0.,  0.,  0.,  0.,  1.,  1., 12.]]])\n","outX: tensor([[[-1.2096]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-1.2096]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(0.5923, grad_fn=<MseLossBackward0>)\n","82\n","[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 4.]\n","inX: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 4.]])\n","inSquee tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 4.]]])\n","outX: tensor([[[-0.5605]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-0.5605]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(0.3142, grad_fn=<MseLossBackward0>)\n","83\n","[ 9.  3. 45.  1.  0.  0.  0.  0.  1.  0.  3.]\n","inX: tensor([[ 9.,  3., 45.,  1.,  0.,  0.,  0.,  0.,  1.,  0.,  3.]])\n","inSquee tensor([[[ 9.,  3., 45.,  1.,  0.,  0.,  0.,  0.,  1.,  0.,  3.]]])\n","outX: tensor([[[1.7262]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[1.7262]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(2.9797, grad_fn=<MseLossBackward0>)\n","84\n","[2. 2. 3. 0. 0. 2. 0. 0. 2. 0. 3.]\n","inX: tensor([[2., 2., 3., 0., 0., 2., 0., 0., 2., 0., 3.]])\n","inSquee tensor([[[2., 2., 3., 0., 0., 2., 0., 0., 2., 0., 3.]]])\n","outX: tensor([[[-0.4205]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-0.4205]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(0.5783, grad_fn=<MseLossBackward0>)\n","85\n","[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 5.]\n","inX: tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 5.]])\n","inSquee tensor([[[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 5.]]])\n","outX: tensor([[[-0.5779]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-0.5779]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(5.2801, grad_fn=<MseLossBackward0>)\n","86\n","[ 0.  1.  0.  0.  0.  1.  0.  0.  0.  1. 15.]\n","inX: tensor([[ 0.,  1.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  1., 15.]])\n","inSquee tensor([[[ 0.,  1.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  1., 15.]]])\n","outX: tensor([[[-1.1235]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-1.1235]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(6.2674, grad_fn=<MseLossBackward0>)\n","87\n","[13. 33. 24.  1.  3. 16.  0.  0.  2.  1. 27.]\n","inX: tensor([[13., 33., 24.,  1.,  3., 16.,  0.,  0.,  2.,  1., 27.]])\n","inSquee tensor([[[13., 33., 24.,  1.,  3., 16.,  0.,  0.,  2.,  1., 27.]]])\n","outX: tensor([[[2.7821]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[2.7821]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(17.3231, grad_fn=<MseLossBackward0>)\n","88\n","[0. 3. 3. 0. 0. 0. 0. 0. 0. 0. 3.]\n","inX: tensor([[0., 3., 3., 0., 0., 0., 0., 0., 0., 0., 3.]])\n","inSquee tensor([[[0., 3., 3., 0., 0., 0., 0., 0., 0., 0., 3.]]])\n","outX: tensor([[[-0.4260]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-0.4260]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(3.5118, grad_fn=<MseLossBackward0>)\n","89\n","[ 2. 33.  1.  0.  0.  1.  0.  0.  0.  1. 20.]\n","inX: tensor([[ 2., 33.,  1.,  0.,  0.,  1.,  0.,  0.,  0.,  1., 20.]])\n","inSquee tensor([[[ 2., 33.,  1.,  0.,  0.,  1.,  0.,  0.,  0.,  1., 20.]]])\n","outX: tensor([[[-1.2735]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-1.2735]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(0.6030, grad_fn=<MseLossBackward0>)\n","90\n","[0. 2. 5. 0. 0. 1. 0. 0. 1. 0. 7.]\n","inX: tensor([[0., 2., 5., 0., 0., 1., 0., 0., 1., 0., 7.]])\n","inSquee tensor([[[0., 2., 5., 0., 0., 1., 0., 0., 1., 0., 7.]]])\n","outX: tensor([[[-0.8050]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-0.8050]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(19.3162, grad_fn=<MseLossBackward0>)\n","91\n","[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","inX: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inSquee tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n","outX: tensor([[[-0.3126]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-0.3126]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(0.2329, grad_fn=<MseLossBackward0>)\n","92\n","[ 4. 21. 14.  0.  0.  0.  0.  0.  1.  1.  3.]\n","inX: tensor([[ 4., 21., 14.,  0.,  0.,  0.,  0.,  0.,  1.,  1.,  3.]])\n","inSquee tensor([[[ 4., 21., 14.,  0.,  0.,  0.,  0.,  0.,  1.,  1.,  3.]]])\n","outX: tensor([[[-0.7277]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-0.7277]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(0.0002, grad_fn=<MseLossBackward0>)\n","93\n","[ 6.  4. 19.  0.  0. 10.  0.  0.  0.  3. 83.]\n","inX: tensor([[ 6.,  4., 19.,  0.,  0., 10.,  0.,  0.,  0.,  3., 83.]])\n","inSquee tensor([[[ 6.,  4., 19.,  0.,  0., 10.,  0.,  0.,  0.,  3., 83.]]])\n","outX: tensor([[[-5.9631]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-5.9631]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(47.3767, grad_fn=<MseLossBackward0>)\n","94\n","[ 4.  3.  1.  0.  0.  1.  0.  0.  0.  0. 15.]\n","inX: tensor([[ 4.,  3.,  1.,  0.,  0.,  1.,  0.,  0.,  0.,  0., 15.]])\n","inSquee tensor([[[ 4.,  3.,  1.,  0.,  0.,  1.,  0.,  0.,  0.,  0., 15.]]])\n","outX: tensor([[[0.2826]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[0.2826]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(5.9176, grad_fn=<MseLossBackward0>)\n","95\n","[ 5. 20. 19.  3.  0.  3.  0.  0.  3.  0. 29.]\n","inX: tensor([[ 5., 20., 19.,  3.,  0.,  3.,  0.,  0.,  3.,  0., 29.]])\n","inSquee tensor([[[ 5., 20., 19.,  3.,  0.,  3.,  0.,  0.,  3.,  0., 29.]]])\n","outX: tensor([[[1.6158]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[1.6158]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(32.2151, grad_fn=<MseLossBackward0>)\n","96\n","[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 7.]\n","inX: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 7.]])\n","inSquee tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 7.]]])\n","outX: tensor([[[-0.3930]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-0.3930]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(0.6774, grad_fn=<MseLossBackward0>)\n","97\n","[ 6.  3. 26.  0.  0.  0.  0.  0.  4.  0.  6.]\n","inX: tensor([[ 6.,  3., 26.,  0.,  0.,  0.,  0.,  0.,  4.,  0.,  6.]])\n","inSquee tensor([[[ 6.,  3., 26.,  0.,  0.,  0.,  0.,  0.,  4.,  0.,  6.]]])\n","outX: tensor([[[-1.1845]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-1.1845]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(0.5105, grad_fn=<MseLossBackward0>)\n","98\n","[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","inX: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inSquee tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n","outX: tensor([[[-0.3124]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-0.3124]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(2.1245, grad_fn=<MseLossBackward0>)\n","99\n","[ 8.  8. 21.  0.  0. 10.  0.  0.  2.  3. 83.]\n","inX: tensor([[ 8.,  8., 21.,  0.,  0., 10.,  0.,  0.,  2.,  3., 83.]])\n","inSquee tensor([[[ 8.,  8., 21.,  0.,  0., 10.,  0.,  0.,  2.,  3., 83.]]])\n","outX: tensor([[[-0.0033]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-0.0033]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(0.2534, grad_fn=<MseLossBackward0>)\n","100\n","[14. 30. 18.  0.  1.  2.  0.  0.  1.  1. 22.]\n","inX: tensor([[14., 30., 18.,  0.,  1.,  2.,  0.,  0.,  1.,  1., 22.]])\n","inSquee tensor([[[14., 30., 18.,  0.,  1.,  2.,  0.,  0.,  1.,  1., 22.]]])\n","outX: tensor([[[-0.7918]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-0.7918]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(9.6211, grad_fn=<MseLossBackward0>)\n","101\n","[5. 0. 5. 0. 0. 1. 0. 0. 0. 0. 1.]\n","inX: tensor([[5., 0., 5., 0., 0., 1., 0., 0., 0., 0., 1.]])\n","inSquee tensor([[[5., 0., 5., 0., 0., 1., 0., 0., 0., 0., 1.]]])\n","outX: tensor([[[-0.0803]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-0.0803]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(2.3114, grad_fn=<MseLossBackward0>)\n","102\n","[0. 2. 1. 0. 0. 0. 0. 0. 0. 0. 6.]\n","inX: tensor([[0., 2., 1., 0., 0., 0., 0., 0., 0., 0., 6.]])\n","inSquee tensor([[[0., 2., 1., 0., 0., 0., 0., 0., 0., 0., 6.]]])\n","outX: tensor([[[-0.1754]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-0.1754]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(0.2858, grad_fn=<MseLossBackward0>)\n","103\n","[1. 0. 5. 0. 0. 0. 0. 0. 0. 0. 2.]\n","inX: tensor([[1., 0., 5., 0., 0., 0., 0., 0., 0., 0., 2.]])\n","inSquee tensor([[[1., 0., 5., 0., 0., 0., 0., 0., 0., 0., 2.]]])\n","outX: tensor([[[-0.1434]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-0.1434]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(0.0469, grad_fn=<MseLossBackward0>)\n","104\n","[0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1.]\n","inX: tensor([[0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1.]])\n","inSquee tensor([[[0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1.]]])\n","outX: tensor([[[-0.2534]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-0.2534]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(1.1955, grad_fn=<MseLossBackward0>)\n","105\n","[ 22. 140.  26.   2.   0.  27.   0.   0.   1.   0.  39.]\n","inX: tensor([[ 22., 140.,  26.,   2.,   0.,  27.,   0.,   0.,   1.,   0.,  39.]])\n","inSquee tensor([[[ 22., 140.,  26.,   2.,   0.,  27.,   0.,   0.,   1.,   0.,  39.]]])\n","outX: tensor([[[7.7741]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[7.7741]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(92.2393, grad_fn=<MseLossBackward0>)\n","106\n","[0. 3. 2. 0. 0. 0. 0. 0. 1. 0. 0.]\n","inX: tensor([[0., 3., 2., 0., 0., 0., 0., 0., 1., 0., 0.]])\n","inSquee tensor([[[0., 3., 2., 0., 0., 0., 0., 0., 1., 0., 0.]]])\n","outX: tensor([[[-1.3558]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-1.3558]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(10.9282, grad_fn=<MseLossBackward0>)\n","107\n","[ 2. 20.  7.  0.  1.  2.  0.  0.  0.  1. 29.]\n","inX: tensor([[ 2., 20.,  7.,  0.,  1.,  2.,  0.,  0.,  0.,  1., 29.]])\n","inSquee tensor([[[ 2., 20.,  7.,  0.,  1.,  2.,  0.,  0.,  0.,  1., 29.]]])\n","outX: tensor([[[-7.4417]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-7.4417]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(129.7702, grad_fn=<MseLossBackward0>)\n","108\n","[0. 0. 2. 0. 0. 0. 0. 0. 0. 0. 1.]\n","inX: tensor([[0., 0., 2., 0., 0., 0., 0., 0., 0., 0., 1.]])\n","inSquee tensor([[[0., 0., 2., 0., 0., 0., 0., 0., 0., 0., 1.]]])\n","outX: tensor([[[-0.3036]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-0.3036]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(2.2995, grad_fn=<MseLossBackward0>)\n","109\n","[5. 8. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","inX: tensor([[5., 8., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inSquee tensor([[[5., 8., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n","outX: tensor([[[-2.3126]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-2.3126]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(7.2501, grad_fn=<MseLossBackward0>)\n","110\n","[  2. 119. 142.   2.   0.   1.   0.   0.   0.   1.  16.]\n","inX: tensor([[  2., 119., 142.,   2.,   0.,   1.,   0.,   0.,   0.,   1.,  16.]])\n","inSquee tensor([[[  2., 119., 142.,   2.,   0.,   1.,   0.,   0.,   0.,   1.,  16.]]])\n","outX: tensor([[[-24.2217]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-24.2217]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(566.9980, grad_fn=<MseLossBackward0>)\n","111\n","[ 6. 18. 21.  0.  0.  1.  0.  0.  0.  0. 17.]\n","inX: tensor([[ 6., 18., 21.,  0.,  0.,  1.,  0.,  0.,  0.,  0., 17.]])\n","inSquee tensor([[[ 6., 18., 21.,  0.,  0.,  1.,  0.,  0.,  0.,  0., 17.]]])\n","outX: tensor([[[21.6238]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[21.6238]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(457.2654, grad_fn=<MseLossBackward0>)\n","112\n","[ 6. 27. 18.  1.  0.  2.  0.  0.  2.  3. 23.]\n","inX: tensor([[ 6., 27., 18.,  1.,  0.,  2.,  0.,  0.,  2.,  3., 23.]])\n","inSquee tensor([[[ 6., 27., 18.,  1.,  0.,  2.,  0.,  0.,  2.,  3., 23.]]])\n","outX: tensor([[[17.2125]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[17.2125]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(367.9668, grad_fn=<MseLossBackward0>)\n","113\n","[ 5.  3. 25.  8.  0.  2.  0.  0.  1.  1. 49.]\n","inX: tensor([[ 5.,  3., 25.,  8.,  0.,  2.,  0.,  0.,  1.,  1., 49.]])\n","inSquee tensor([[[ 5.,  3., 25.,  8.,  0.,  2.,  0.,  0.,  1.,  1., 49.]]])\n","outX: tensor([[[10.0569]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[10.0569]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(198.4403, grad_fn=<MseLossBackward0>)\n","114\n","[2. 1. 2. 0. 0. 0. 0. 0. 0. 0. 1.]\n","inX: tensor([[2., 1., 2., 0., 0., 0., 0., 0., 0., 0., 1.]])\n","inSquee tensor([[[2., 1., 2., 0., 0., 0., 0., 0., 0., 0., 1.]]])\n","outX: tensor([[[0.3001]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[0.3001]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(0.0009, grad_fn=<MseLossBackward0>)\n","115\n","[0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0.]\n","inX: tensor([[0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0.]])\n","inSquee tensor([[[0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0.]]])\n","outX: tensor([[[-0.1643]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-0.1643]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(0.1987, grad_fn=<MseLossBackward0>)\n","116\n","[7. 1. 2. 0. 0. 0. 0. 0. 0. 1. 0.]\n","inX: tensor([[7., 1., 2., 0., 0., 0., 0., 0., 0., 1., 0.]])\n","inSquee tensor([[[7., 1., 2., 0., 0., 0., 0., 0., 0., 1., 0.]]])\n","outX: tensor([[[-0.2260]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-0.2260]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(0.0511, grad_fn=<MseLossBackward0>)\n","117\n","[ 0.  0.  3.  1.  0.  1.  0.  0.  0.  0. 13.]\n","inX: tensor([[ 0.,  0.,  3.,  1.,  0.,  1.,  0.,  0.,  0.,  0., 13.]])\n","inSquee tensor([[[ 0.,  0.,  3.,  1.,  0.,  1.,  0.,  0.,  0.,  0., 13.]]])\n","outX: tensor([[[-1.5213]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-1.5213]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(0.3349, grad_fn=<MseLossBackward0>)\n","118\n","[ 8. 31. 27.  2.  1.  3.  0.  0.  3.  0. 35.]\n","inX: tensor([[ 8., 31., 27.,  2.,  1.,  3.,  0.,  0.,  3.,  0., 35.]])\n","inSquee tensor([[[ 8., 31., 27.,  2.,  1.,  3.,  0.,  0.,  3.,  0., 35.]]])\n","outX: tensor([[[8.3426]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[8.3426]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(93.7521, grad_fn=<MseLossBackward0>)\n","119\n","[4. 4. 2. 0. 0. 0. 0. 0. 0. 0. 4.]\n","inX: tensor([[4., 4., 2., 0., 0., 0., 0., 0., 0., 0., 4.]])\n","inSquee tensor([[[4., 4., 2., 0., 0., 0., 0., 0., 0., 0., 4.]]])\n","outX: tensor([[[-0.7578]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-0.7578]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(1.2148, grad_fn=<MseLossBackward0>)\n","120\n","[15. 23. 24.  0.  1.  4.  0.  0.  0.  2. 31.]\n","inX: tensor([[15., 23., 24.,  0.,  1.,  4.,  0.,  0.,  0.,  2., 31.]])\n","inSquee tensor([[[15., 23., 24.,  0.,  1.,  4.,  0.,  0.,  0.,  2., 31.]]])\n","outX: tensor([[[0.8632]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[0.8632]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(8.0268, grad_fn=<MseLossBackward0>)\n","121\n","[ 5. 14. 16.  5.  2.  0.  0.  0.  0.  0.  0.]\n","inX: tensor([[ 5., 14., 16.,  5.,  2.,  0.,  0.,  0.,  0.,  0.,  0.]])\n","inSquee tensor([[[ 5., 14., 16.,  5.,  2.,  0.,  0.,  0.,  0.,  0.,  0.]]])\n","outX: tensor([[[7.0046]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[7.0046]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(54.8277, grad_fn=<MseLossBackward0>)\n","122\n","[ 2. 18.  3.  0.  0.  0.  0.  0.  0.  0.  6.]\n","inX: tensor([[ 2., 18.,  3.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  6.]])\n","inSquee tensor([[[ 2., 18.,  3.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  6.]]])\n","outX: tensor([[[0.0207]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[0.0207]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(1.0830, grad_fn=<MseLossBackward0>)\n","123\n","[0. 1. 1. 0. 1. 0. 0. 0. 1. 0. 5.]\n","inX: tensor([[0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 5.]])\n","inSquee tensor([[[0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 5.]]])\n","outX: tensor([[[-1.8204]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-1.8204]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(5.3378, grad_fn=<MseLossBackward0>)\n","124\n","[1. 7. 1. 1. 0. 0. 0. 0. 0. 1. 6.]\n","inX: tensor([[1., 7., 1., 1., 0., 0., 0., 0., 0., 1., 6.]])\n","inSquee tensor([[[1., 7., 1., 1., 0., 0., 0., 0., 0., 1., 6.]]])\n","outX: tensor([[[-1.4376]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-1.4376]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(1.4219, grad_fn=<MseLossBackward0>)\n","125\n","[ 8.  8. 23.  0.  0.  4.  0.  0.  2.  1. 40.]\n","inX: tensor([[ 8.,  8., 23.,  0.,  0.,  4.,  0.,  0.,  2.,  1., 40.]])\n","inSquee tensor([[[ 8.,  8., 23.,  0.,  0.,  4.,  0.,  0.,  2.,  1., 40.]]])\n","outX: tensor([[[-5.1544]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-5.1544]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(18.7873, grad_fn=<MseLossBackward0>)\n","126\n","[ 1.  3. 12.  0.  0.  8.  0.  0.  1.  0. 33.]\n","inX: tensor([[ 1.,  3., 12.,  0.,  0.,  8.,  0.,  0.,  1.,  0., 33.]])\n","inSquee tensor([[[ 1.,  3., 12.,  0.,  0.,  8.,  0.,  0.,  1.,  0., 33.]]])\n","outX: tensor([[[-3.8551]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-3.8551]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(9.5179, grad_fn=<MseLossBackward0>)\n","127\n","[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","inX: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inSquee tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n","outX: tensor([[[-0.3194]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-0.3194]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(0.3129, grad_fn=<MseLossBackward0>)\n","128\n","[0. 1. 5. 0. 0. 0. 0. 0. 1. 0. 0.]\n","inX: tensor([[0., 1., 5., 0., 0., 0., 0., 0., 1., 0., 0.]])\n","inSquee tensor([[[0., 1., 5., 0., 0., 0., 0., 0., 1., 0., 0.]]])\n","outX: tensor([[[1.4324]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[1.4324]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(0.4794, grad_fn=<MseLossBackward0>)\n","129\n","[1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n","inX: tensor([[1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inSquee tensor([[[1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n","outX: tensor([[[-0.0768]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-0.0768]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(1.4961, grad_fn=<MseLossBackward0>)\n","130\n","[ 4.  4. 11.  5.  0.  2.  0.  0.  1.  2. 25.]\n","inX: tensor([[ 4.,  4., 11.,  5.,  0.,  2.,  0.,  0.,  1.,  2., 25.]])\n","inSquee tensor([[[ 4.,  4., 11.,  5.,  0.,  2.,  0.,  0.,  1.,  2., 25.]]])\n","outX: tensor([[[-2.1936]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-2.1936]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(0.0002, grad_fn=<MseLossBackward0>)\n","131\n","[3. 5. 2. 1. 0. 0. 0. 0. 0. 1. 0.]\n","inX: tensor([[3., 5., 2., 1., 0., 0., 0., 0., 0., 1., 0.]])\n","inSquee tensor([[[3., 5., 2., 1., 0., 0., 0., 0., 0., 1., 0.]]])\n","outX: tensor([[[0.4201]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[0.4201]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(0.0960, grad_fn=<MseLossBackward0>)\n","132\n","[0. 1. 5. 0. 0. 0. 0. 0. 3. 0. 0.]\n","inX: tensor([[0., 1., 5., 0., 0., 0., 0., 0., 3., 0., 0.]])\n","inSquee tensor([[[0., 1., 5., 0., 0., 0., 0., 0., 3., 0., 0.]]])\n","outX: tensor([[[0.9193]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[0.9193]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(0.1151, grad_fn=<MseLossBackward0>)\n","133\n","[ 6.  4. 39.  0.  0.  0.  0.  0.  7.  0.  8.]\n","inX: tensor([[ 6.,  4., 39.,  0.,  0.,  0.,  0.,  0.,  7.,  0.,  8.]])\n","inSquee tensor([[[ 6.,  4., 39.,  0.,  0.,  0.,  0.,  0.,  7.,  0.,  8.]]])\n","outX: tensor([[[10.3192]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[10.3192]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(106.4851, grad_fn=<MseLossBackward0>)\n","134\n","[0. 2. 1. 0. 0. 0. 0. 0. 0. 0. 5.]\n","inX: tensor([[0., 2., 1., 0., 0., 0., 0., 0., 0., 0., 5.]])\n","inSquee tensor([[[0., 2., 1., 0., 0., 0., 0., 0., 0., 0., 5.]]])\n","outX: tensor([[[-1.2162]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-1.2162]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(7.9308, grad_fn=<MseLossBackward0>)\n","135\n","[1. 5. 3. 0. 2. 0. 0. 0. 0. 1. 1.]\n","inX: tensor([[1., 5., 3., 0., 2., 0., 0., 0., 0., 1., 1.]])\n","inSquee tensor([[[1., 5., 3., 0., 2., 0., 0., 0., 0., 1., 1.]]])\n","outX: tensor([[[0.1204]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[0.1204]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(3.8430, grad_fn=<MseLossBackward0>)\n","136\n","[ 5.  4.  6.  1.  0.  1.  0.  0.  0.  1. 19.]\n","inX: tensor([[ 5.,  4.,  6.,  1.,  0.,  1.,  0.,  0.,  0.,  1., 19.]])\n","inSquee tensor([[[ 5.,  4.,  6.,  1.,  0.,  1.,  0.,  0.,  0.,  1., 19.]]])\n","outX: tensor([[[-3.9224]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-3.9224]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(5.8198, grad_fn=<MseLossBackward0>)\n","137\n","[  1.   3.  20.   0.   1.  15.   0.   0.   0.   4. 134.]\n","inX: tensor([[  1.,   3.,  20.,   0.,   1.,  15.,   0.,   0.,   0.,   4., 134.]])\n","inSquee tensor([[[  1.,   3.,  20.,   0.,   1.,  15.,   0.,   0.,   0.,   4., 134.]]])\n","outX: tensor([[[-27.2833]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-27.2833]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(625.6651, grad_fn=<MseLossBackward0>)\n","138\n","[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","inX: tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inSquee tensor([[[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n","outX: tensor([[[-0.2260]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-0.2260]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(0.0154, grad_fn=<MseLossBackward0>)\n","139\n","[ 1.  0.  4.  0.  0.  2.  0.  0.  0.  1. 51.]\n","inX: tensor([[ 1.,  0.,  4.,  0.,  0.,  2.,  0.,  0.,  0.,  1., 51.]])\n","inSquee tensor([[[ 1.,  0.,  4.,  0.,  0.,  2.,  0.,  0.,  0.,  1., 51.]]])\n","outX: tensor([[[22.5069]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[22.5069]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(666.5103, grad_fn=<MseLossBackward0>)\n","140\n","[0. 6. 5. 2. 1. 0. 0. 0. 2. 0. 6.]\n","inX: tensor([[0., 6., 5., 2., 1., 0., 0., 0., 2., 0., 6.]])\n","inSquee tensor([[[0., 6., 5., 2., 1., 0., 0., 0., 2., 0., 6.]]])\n","outX: tensor([[[2.6794]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[2.6794]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(7.1791, grad_fn=<MseLossBackward0>)\n","141\n","[1. 3. 4. 0. 0. 1. 0. 0. 0. 0. 6.]\n","inX: tensor([[1., 3., 4., 0., 0., 1., 0., 0., 0., 0., 6.]])\n","inSquee tensor([[[1., 3., 4., 0., 0., 1., 0., 0., 0., 0., 6.]]])\n","outX: tensor([[[2.4010]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[2.4010]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(35.5336, grad_fn=<MseLossBackward0>)\n","142\n","[ 0.  4.  2.  0.  0.  5.  0.  0.  1.  1. 20.]\n","inX: tensor([[ 0.,  4.,  2.,  0.,  0.,  5.,  0.,  0.,  1.,  1., 20.]])\n","inSquee tensor([[[ 0.,  4.,  2.,  0.,  0.,  5.,  0.,  0.,  1.,  1., 20.]]])\n","outX: tensor([[[4.1623]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[4.1623]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(20.9978, grad_fn=<MseLossBackward0>)\n","143\n","[0. 0. 2. 0. 0. 0. 0. 0. 0. 0. 9.]\n","inX: tensor([[0., 0., 2., 0., 0., 0., 0., 0., 0., 0., 9.]])\n","inSquee tensor([[[0., 0., 2., 0., 0., 0., 0., 0., 0., 0., 9.]]])\n","outX: tensor([[[1.5146]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[1.5146]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(3.8203, grad_fn=<MseLossBackward0>)\n","144\n","[10.  6. 14.  0.  0.  1.  0.  0.  1.  0.  7.]\n","inX: tensor([[10.,  6., 14.,  0.,  0.,  1.,  0.,  0.,  1.,  0.,  7.]])\n","inSquee tensor([[[10.,  6., 14.,  0.,  0.,  1.,  0.,  0.,  1.,  0.,  7.]]])\n","outX: tensor([[[4.6197]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[4.6197]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(29.3731, grad_fn=<MseLossBackward0>)\n","145\n","[1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n","inX: tensor([[1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inSquee tensor([[[1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n","outX: tensor([[[-0.1299]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[-0.1299]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(0.1226, grad_fn=<MseLossBackward0>)\n","146\n","[1. 1. 3. 0. 0. 0. 0. 0. 0. 0. 0.]\n","inX: tensor([[1., 1., 3., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inSquee tensor([[[1., 1., 3., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n","outX: tensor([[[0.6621]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[0.6621]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(1.0243, grad_fn=<MseLossBackward0>)\n","147\n","[ 4.  4. 20.  0.  0.  3.  0.  0.  0.  2. 23.]\n","inX: tensor([[ 4.,  4., 20.,  0.,  0.,  3.,  0.,  0.,  0.,  2., 23.]])\n","inSquee tensor([[[ 4.,  4., 20.,  0.,  0.,  3.,  0.,  0.,  0.,  2., 23.]]])\n","outX: tensor([[[9.3139]]], grad_fn=<AddBackward0>)\n","the value of logit tensor([[[9.3139]]], grad_fn=<AddBackward0>)\n","the value of loss tensor(116.5087, grad_fn=<MseLossBackward0>)\n"]}]},{"cell_type":"code","source":["def predict(pred_x, model):\n","    pred_x_tensor = torch.tensor(pred_x).float() #convert numpy to torch tensor\n","    logit = model(pred_x_tensor)\n","\n","    return logit"],"metadata":{"id":"5ALdsRGAZiV5","executionInfo":{"status":"ok","timestamp":1652096218831,"user_tz":-540,"elapsed":70,"user":{"displayName":"Ha Jun","userId":"06515366270624202236"}}},"execution_count":125,"outputs":[]},{"cell_type":"code","source":["prediction_df = pd.DataFrame(columns = ['기업', '등락율'])\n","top100 = list(top100)\n","for com_index, count_df in enumerate(count_df_list):\n","  if len(count_df) < 1:\n","    continue\n","  input_x = count_df[count_df.columns[1:]].to_numpy()\n","  logit = predict(input_x, best_model)\n","  prediction_df = prediction_df.append(pd.Series([top100[com_index], logit.item()], index = prediction_df.columns), ignore_index = True)\n","  "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9H768keMVGdD","executionInfo":{"status":"ok","timestamp":1652096219928,"user_tz":-540,"elapsed":1163,"user":{"displayName":"Ha Jun","userId":"06515366270624202236"}},"outputId":"0b14f652-bdb8-43f4-a492-f79a1aa5e415"},"execution_count":126,"outputs":[{"output_type":"stream","name":"stdout","text":["inX: tensor([[0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1.]])\n","inSquee tensor([[[0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1.]]])\n","outX: tensor([[[-0.0089]]], grad_fn=<AddBackward0>)\n","inX: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inSquee tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n","outX: tensor([[[-0.3278]]], grad_fn=<AddBackward0>)\n","inX: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inSquee tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n","outX: tensor([[[-0.3278]]], grad_fn=<AddBackward0>)\n","inX: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inSquee tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n","outX: tensor([[[-0.3278]]], grad_fn=<AddBackward0>)\n","inX: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inSquee tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n","outX: tensor([[[-0.3278]]], grad_fn=<AddBackward0>)\n","inX: tensor([[0., 0., 2., 0., 0., 1., 0., 0., 0., 0., 6.]])\n","inSquee tensor([[[0., 0., 2., 0., 0., 1., 0., 0., 0., 0., 6.]]])\n","outX: tensor([[[0.8586]]], grad_fn=<AddBackward0>)\n","inX: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inSquee tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n","outX: tensor([[[-0.3278]]], grad_fn=<AddBackward0>)\n","inX: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inSquee tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n","outX: tensor([[[-0.3278]]], grad_fn=<AddBackward0>)\n","inX: tensor([[0., 0., 3., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inSquee tensor([[[0., 0., 3., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n","outX: tensor([[[0.6195]]], grad_fn=<AddBackward0>)\n","inX: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inSquee tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n","outX: tensor([[[-0.3278]]], grad_fn=<AddBackward0>)\n","inX: tensor([[0., 0., 3., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inSquee tensor([[[0., 0., 3., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n","outX: tensor([[[0.6195]]], grad_fn=<AddBackward0>)\n","inX: tensor([[2., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inSquee tensor([[[2., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n","outX: tensor([[[-0.3581]]], grad_fn=<AddBackward0>)\n","inX: tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inSquee tensor([[[0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n","outX: tensor([[[-0.0121]]], grad_fn=<AddBackward0>)\n","inX: tensor([[0., 7., 2., 0., 0., 1., 0., 0., 1., 0., 2.]])\n","inSquee tensor([[[0., 7., 2., 0., 0., 1., 0., 0., 1., 0., 2.]]])\n","outX: tensor([[[0.7959]]], grad_fn=<AddBackward0>)\n","inX: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 2.]])\n","inSquee tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 2.]]])\n","outX: tensor([[[-0.2077]]], grad_fn=<AddBackward0>)\n","inX: tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 3.]])\n","inSquee tensor([[[0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 3.]]])\n","outX: tensor([[[0.1681]]], grad_fn=<AddBackward0>)\n","inX: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inSquee tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n","outX: tensor([[[-0.3278]]], grad_fn=<AddBackward0>)\n","inX: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inSquee tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n","outX: tensor([[[-0.3278]]], grad_fn=<AddBackward0>)\n","inX: tensor([[0., 0., 2., 0., 0., 1., 0., 0., 0., 0., 6.]])\n","inSquee tensor([[[0., 0., 2., 0., 0., 1., 0., 0., 0., 0., 6.]]])\n","outX: tensor([[[0.8586]]], grad_fn=<AddBackward0>)\n","inX: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inSquee tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n","outX: tensor([[[-0.3278]]], grad_fn=<AddBackward0>)\n","inX: tensor([[0., 7., 2., 0., 0., 1., 0., 0., 1., 0., 2.]])\n","inSquee tensor([[[0., 7., 2., 0., 0., 1., 0., 0., 1., 0., 2.]]])\n","outX: tensor([[[0.7959]]], grad_fn=<AddBackward0>)\n","inX: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inSquee tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n","outX: tensor([[[-0.3278]]], grad_fn=<AddBackward0>)\n","inX: tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 3.]])\n","inSquee tensor([[[0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 3.]]])\n","outX: tensor([[[0.1681]]], grad_fn=<AddBackward0>)\n","inX: tensor([[0., 0., 3., 0., 1., 0., 0., 0., 1., 0., 0.]])\n","inSquee tensor([[[0., 0., 3., 0., 1., 0., 0., 0., 1., 0., 0.]]])\n","outX: tensor([[[0.2204]]], grad_fn=<AddBackward0>)\n","inX: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]])\n","inSquee tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]]])\n","outX: tensor([[[-0.6000]]], grad_fn=<AddBackward0>)\n","inX: tensor([[0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 6.]])\n","inSquee tensor([[[0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 6.]]])\n","outX: tensor([[[0.4125]]], grad_fn=<AddBackward0>)\n","inX: tensor([[0., 0., 2., 0., 0., 1., 0., 0., 0., 0., 6.]])\n","inSquee tensor([[[0., 0., 2., 0., 0., 1., 0., 0., 0., 0., 6.]]])\n","outX: tensor([[[0.8586]]], grad_fn=<AddBackward0>)\n","inX: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inSquee tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n","outX: tensor([[[-0.3278]]], grad_fn=<AddBackward0>)\n","inX: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inSquee tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n","outX: tensor([[[-0.3278]]], grad_fn=<AddBackward0>)\n","inX: tensor([[0., 3., 1., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inSquee tensor([[[0., 3., 1., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n","outX: tensor([[[0.1807]]], grad_fn=<AddBackward0>)\n","inX: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inSquee tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n","outX: tensor([[[-0.3278]]], grad_fn=<AddBackward0>)\n","inX: tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inSquee tensor([[[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n","outX: tensor([[[-0.2636]]], grad_fn=<AddBackward0>)\n","inX: tensor([[0., 7., 4., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inSquee tensor([[[0., 7., 4., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n","outX: tensor([[[1.3849]]], grad_fn=<AddBackward0>)\n","inX: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inSquee tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n","outX: tensor([[[-0.3278]]], grad_fn=<AddBackward0>)\n","inX: tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 3.]])\n","inSquee tensor([[[0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 3.]]])\n","outX: tensor([[[0.1681]]], grad_fn=<AddBackward0>)\n","inX: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inSquee tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n","outX: tensor([[[-0.3278]]], grad_fn=<AddBackward0>)\n","inX: tensor([[1., 0., 3., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inSquee tensor([[[1., 0., 3., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n","outX: tensor([[[0.4464]]], grad_fn=<AddBackward0>)\n","inX: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inSquee tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n","outX: tensor([[[-0.3278]]], grad_fn=<AddBackward0>)\n","inX: tensor([[0., 0., 3., 0., 1., 0., 0., 0., 1., 0., 0.]])\n","inSquee tensor([[[0., 0., 3., 0., 1., 0., 0., 0., 1., 0., 0.]]])\n","outX: tensor([[[0.2204]]], grad_fn=<AddBackward0>)\n","inX: tensor([[0., 0., 2., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inSquee tensor([[[0., 0., 2., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n","outX: tensor([[[0.3037]]], grad_fn=<AddBackward0>)\n","inX: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inSquee tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n","outX: tensor([[[-0.3278]]], grad_fn=<AddBackward0>)\n","inX: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inSquee tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n","outX: tensor([[[-0.3278]]], grad_fn=<AddBackward0>)\n","inX: tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inSquee tensor([[[0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n","outX: tensor([[[-0.0121]]], grad_fn=<AddBackward0>)\n","inX: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inSquee tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n","outX: tensor([[[-0.3278]]], grad_fn=<AddBackward0>)\n","inX: tensor([[0., 0., 2., 0., 0., 1., 0., 0., 0., 0., 6.]])\n","inSquee tensor([[[0., 0., 2., 0., 0., 1., 0., 0., 0., 0., 6.]]])\n","outX: tensor([[[0.8586]]], grad_fn=<AddBackward0>)\n","inX: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inSquee tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n","outX: tensor([[[-0.3278]]], grad_fn=<AddBackward0>)\n","inX: tensor([[0., 2., 0., 0., 0., 0., 0., 0., 0., 0., 3.]])\n","inSquee tensor([[[0., 2., 0., 0., 0., 0., 0., 0., 0., 0., 3.]]])\n","outX: tensor([[[-0.0192]]], grad_fn=<AddBackward0>)\n","inX: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inSquee tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n","outX: tensor([[[-0.3278]]], grad_fn=<AddBackward0>)\n","inX: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inSquee tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n","outX: tensor([[[-0.3278]]], grad_fn=<AddBackward0>)\n","inX: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inSquee tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n","outX: tensor([[[-0.3278]]], grad_fn=<AddBackward0>)\n","inX: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inSquee tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n","outX: tensor([[[-0.3278]]], grad_fn=<AddBackward0>)\n","inX: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inSquee tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n","outX: tensor([[[-0.3278]]], grad_fn=<AddBackward0>)\n","inX: tensor([[0., 0., 2., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inSquee tensor([[[0., 0., 2., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n","outX: tensor([[[0.3037]]], grad_fn=<AddBackward0>)\n","inX: tensor([[ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  1., 15.]])\n","inSquee tensor([[[ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  1., 15.]]])\n","outX: tensor([[[0.6135]]], grad_fn=<AddBackward0>)\n","inX: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inSquee tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n","outX: tensor([[[-0.3278]]], grad_fn=<AddBackward0>)\n","inX: tensor([[ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  1., 15.]])\n","inSquee tensor([[[ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  1., 15.]]])\n","outX: tensor([[[0.6135]]], grad_fn=<AddBackward0>)\n","inX: tensor([[0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 5.]])\n","inSquee tensor([[[0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 5.]]])\n","outX: tensor([[[0.3525]]], grad_fn=<AddBackward0>)\n","inX: tensor([[0., 0., 3., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inSquee tensor([[[0., 0., 3., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n","outX: tensor([[[0.6195]]], grad_fn=<AddBackward0>)\n","inX: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inSquee tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n","outX: tensor([[[-0.3278]]], grad_fn=<AddBackward0>)\n","inX: tensor([[0., 0., 2., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inSquee tensor([[[0., 0., 2., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n","outX: tensor([[[0.3037]]], grad_fn=<AddBackward0>)\n","inX: tensor([[0., 4., 0., 1., 0., 0., 0., 0., 0., 0., 0.]])\n","inSquee tensor([[[0., 4., 0., 1., 0., 0., 0., 0., 0., 0., 0.]]])\n","outX: tensor([[[0.0799]]], grad_fn=<AddBackward0>)\n","inX: tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inSquee tensor([[[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n","outX: tensor([[[-0.2636]]], grad_fn=<AddBackward0>)\n","inX: tensor([[0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 4.]])\n","inSquee tensor([[[0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 4.]]])\n","outX: tensor([[[-0.1502]]], grad_fn=<AddBackward0>)\n","inX: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inSquee tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n","outX: tensor([[[-0.3278]]], grad_fn=<AddBackward0>)\n","inX: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 6.]])\n","inSquee tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 6.]]])\n","outX: tensor([[[0.0325]]], grad_fn=<AddBackward0>)\n","inX: tensor([[0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 5.]])\n","inSquee tensor([[[0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 5.]]])\n","outX: tensor([[[0.3525]]], grad_fn=<AddBackward0>)\n","inX: tensor([[0., 5., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inSquee tensor([[[0., 5., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n","outX: tensor([[[-0.0066]]], grad_fn=<AddBackward0>)\n","inX: tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inSquee tensor([[[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n","outX: tensor([[[-0.2636]]], grad_fn=<AddBackward0>)\n","inX: tensor([[0., 0., 2., 0., 0., 1., 0., 0., 0., 0., 6.]])\n","inSquee tensor([[[0., 0., 2., 0., 0., 1., 0., 0., 0., 0., 6.]]])\n","outX: tensor([[[0.8586]]], grad_fn=<AddBackward0>)\n","inX: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inSquee tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n","outX: tensor([[[-0.3278]]], grad_fn=<AddBackward0>)\n","inX: tensor([[0., 3., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inSquee tensor([[[0., 3., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n","outX: tensor([[[-0.1351]]], grad_fn=<AddBackward0>)\n","inX: tensor([[0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 5.]])\n","inSquee tensor([[[0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 5.]]])\n","outX: tensor([[[0.3525]]], grad_fn=<AddBackward0>)\n","inX: tensor([[2., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inSquee tensor([[[2., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n","outX: tensor([[[-0.2938]]], grad_fn=<AddBackward0>)\n","inX: tensor([[1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inSquee tensor([[[1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n","outX: tensor([[[-0.1851]]], grad_fn=<AddBackward0>)\n","inX: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 2.]])\n","inSquee tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 2.]]])\n","outX: tensor([[[-0.2077]]], grad_fn=<AddBackward0>)\n","inX: tensor([[0., 6., 3., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inSquee tensor([[[0., 6., 3., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n","outX: tensor([[[1.0049]]], grad_fn=<AddBackward0>)\n","inX: tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inSquee tensor([[[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n","outX: tensor([[[-0.5008]]], grad_fn=<AddBackward0>)\n","inX: tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inSquee tensor([[[0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n","outX: tensor([[[-0.0121]]], grad_fn=<AddBackward0>)\n","inX: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inSquee tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n","outX: tensor([[[-0.3278]]], grad_fn=<AddBackward0>)\n","inX: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inSquee tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n","outX: tensor([[[-0.3278]]], grad_fn=<AddBackward0>)\n","inX: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 2.]])\n","inSquee tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 2.]]])\n","outX: tensor([[[-0.2077]]], grad_fn=<AddBackward0>)\n","inX: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inSquee tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n","outX: tensor([[[-0.3278]]], grad_fn=<AddBackward0>)\n","inX: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inSquee tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n","outX: tensor([[[-0.3278]]], grad_fn=<AddBackward0>)\n","inX: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inSquee tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n","outX: tensor([[[-0.3278]]], grad_fn=<AddBackward0>)\n","inX: tensor([[0., 5., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inSquee tensor([[[0., 5., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n","outX: tensor([[[-0.0066]]], grad_fn=<AddBackward0>)\n","inX: tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inSquee tensor([[[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n","outX: tensor([[[-0.5008]]], grad_fn=<AddBackward0>)\n","inX: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inSquee tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n","outX: tensor([[[-0.3278]]], grad_fn=<AddBackward0>)\n","inX: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inSquee tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n","outX: tensor([[[-0.3278]]], grad_fn=<AddBackward0>)\n","inX: tensor([[0., 0., 2., 0., 0., 1., 0., 0., 0., 0., 6.]])\n","inSquee tensor([[[0., 0., 2., 0., 0., 1., 0., 0., 0., 0., 6.]]])\n","outX: tensor([[[0.8586]]], grad_fn=<AddBackward0>)\n","inX: tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inSquee tensor([[[0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n","outX: tensor([[[-0.0121]]], grad_fn=<AddBackward0>)\n","inX: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inSquee tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n","outX: tensor([[[-0.3278]]], grad_fn=<AddBackward0>)\n","inX: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inSquee tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n","outX: tensor([[[-0.3278]]], grad_fn=<AddBackward0>)\n","inX: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inSquee tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n","outX: tensor([[[-0.3278]]], grad_fn=<AddBackward0>)\n","inX: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inSquee tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n","outX: tensor([[[-0.3278]]], grad_fn=<AddBackward0>)\n","inX: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 2.]])\n","inSquee tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 2.]]])\n","outX: tensor([[[-0.2077]]], grad_fn=<AddBackward0>)\n"]}]},{"cell_type":"code","source":["prediction_df.to_csv(os.path.join(regression_data_dir, 'prediction.csv'), encoding = 'cp949')"],"metadata":{"id":"ptblepHdb-f3","executionInfo":{"status":"ok","timestamp":1652096219931,"user_tz":-540,"elapsed":39,"user":{"displayName":"Ha Jun","userId":"06515366270624202236"}}},"execution_count":127,"outputs":[]},{"cell_type":"code","source":["prediction_df[prediction_df['등락율'] > 0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"6kNAMNqlbqaS","executionInfo":{"status":"ok","timestamp":1652096219933,"user_tz":-540,"elapsed":37,"user":{"displayName":"Ha Jun","userId":"06515366270624202236"}},"outputId":"ac1a757f-3503-4c0c-9581-579b7334d1c6"},"execution_count":128,"outputs":[{"output_type":"execute_result","data":{"text/plain":["           기업       등락율\n","5       삼성전자우  0.858649\n","8         현대차  0.619458\n","10         기아  0.619458\n","13      카카오뱅크  0.795925\n","15       신한지주  0.168111\n","18    SK이노베이션  0.858649\n","20      카카오페이  0.795925\n","22     하나금융지주  0.168111\n","23       한국전력  0.220446\n","25       크래프톤  0.412523\n","26     LG생활건강  0.858649\n","29      SK텔레콤  0.180663\n","32  SK바이오사이언스  1.384903\n","34     우리금융지주  0.168111\n","36       고려아연  0.446445\n","38      현대중공업  0.220446\n","39       대한항공  0.303699\n","44     포스코케미칼  0.858649\n","52      롯데케미칼  0.303699\n","53     SK바이오팜  0.613494\n","55      한온시스템  0.613494\n","56     한국조선해양  0.352466\n","57     LG유플러스  0.619458\n","59        SKC  0.303699\n","60   에스디바이오센서  0.079875\n","64     맥쿼리인프라  0.032523\n","65       현대제철  0.352466\n","68     미래에셋증권  0.858649\n","71      삼성중공업  0.352466\n","75     한국금융지주  1.004903\n","88        팬오션  0.858649"],"text/html":["\n","  <div id=\"df-f9bd9f25-d6e8-43ac-af2e-17d690e8b249\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>기업</th>\n","      <th>등락율</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>5</th>\n","      <td>삼성전자우</td>\n","      <td>0.858649</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>현대차</td>\n","      <td>0.619458</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>기아</td>\n","      <td>0.619458</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>카카오뱅크</td>\n","      <td>0.795925</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>신한지주</td>\n","      <td>0.168111</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>SK이노베이션</td>\n","      <td>0.858649</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>카카오페이</td>\n","      <td>0.795925</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>하나금융지주</td>\n","      <td>0.168111</td>\n","    </tr>\n","    <tr>\n","      <th>23</th>\n","      <td>한국전력</td>\n","      <td>0.220446</td>\n","    </tr>\n","    <tr>\n","      <th>25</th>\n","      <td>크래프톤</td>\n","      <td>0.412523</td>\n","    </tr>\n","    <tr>\n","      <th>26</th>\n","      <td>LG생활건강</td>\n","      <td>0.858649</td>\n","    </tr>\n","    <tr>\n","      <th>29</th>\n","      <td>SK텔레콤</td>\n","      <td>0.180663</td>\n","    </tr>\n","    <tr>\n","      <th>32</th>\n","      <td>SK바이오사이언스</td>\n","      <td>1.384903</td>\n","    </tr>\n","    <tr>\n","      <th>34</th>\n","      <td>우리금융지주</td>\n","      <td>0.168111</td>\n","    </tr>\n","    <tr>\n","      <th>36</th>\n","      <td>고려아연</td>\n","      <td>0.446445</td>\n","    </tr>\n","    <tr>\n","      <th>38</th>\n","      <td>현대중공업</td>\n","      <td>0.220446</td>\n","    </tr>\n","    <tr>\n","      <th>39</th>\n","      <td>대한항공</td>\n","      <td>0.303699</td>\n","    </tr>\n","    <tr>\n","      <th>44</th>\n","      <td>포스코케미칼</td>\n","      <td>0.858649</td>\n","    </tr>\n","    <tr>\n","      <th>52</th>\n","      <td>롯데케미칼</td>\n","      <td>0.303699</td>\n","    </tr>\n","    <tr>\n","      <th>53</th>\n","      <td>SK바이오팜</td>\n","      <td>0.613494</td>\n","    </tr>\n","    <tr>\n","      <th>55</th>\n","      <td>한온시스템</td>\n","      <td>0.613494</td>\n","    </tr>\n","    <tr>\n","      <th>56</th>\n","      <td>한국조선해양</td>\n","      <td>0.352466</td>\n","    </tr>\n","    <tr>\n","      <th>57</th>\n","      <td>LG유플러스</td>\n","      <td>0.619458</td>\n","    </tr>\n","    <tr>\n","      <th>59</th>\n","      <td>SKC</td>\n","      <td>0.303699</td>\n","    </tr>\n","    <tr>\n","      <th>60</th>\n","      <td>에스디바이오센서</td>\n","      <td>0.079875</td>\n","    </tr>\n","    <tr>\n","      <th>64</th>\n","      <td>맥쿼리인프라</td>\n","      <td>0.032523</td>\n","    </tr>\n","    <tr>\n","      <th>65</th>\n","      <td>현대제철</td>\n","      <td>0.352466</td>\n","    </tr>\n","    <tr>\n","      <th>68</th>\n","      <td>미래에셋증권</td>\n","      <td>0.858649</td>\n","    </tr>\n","    <tr>\n","      <th>71</th>\n","      <td>삼성중공업</td>\n","      <td>0.352466</td>\n","    </tr>\n","    <tr>\n","      <th>75</th>\n","      <td>한국금융지주</td>\n","      <td>1.004903</td>\n","    </tr>\n","    <tr>\n","      <th>88</th>\n","      <td>팬오션</td>\n","      <td>0.858649</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f9bd9f25-d6e8-43ac-af2e-17d690e8b249')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-f9bd9f25-d6e8-43ac-af2e-17d690e8b249 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-f9bd9f25-d6e8-43ac-af2e-17d690e8b249');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":128}]},{"cell_type":"code","source":["prediction_df[prediction_df['등락율'] < 0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":423},"id":"Wq3sCeoukQya","executionInfo":{"status":"ok","timestamp":1652096219935,"user_tz":-540,"elapsed":33,"user":{"displayName":"Ha Jun","userId":"06515366270624202236"}},"outputId":"6eb13da9-7e63-41fb-fbb7-59fbd8409587"},"execution_count":129,"outputs":[{"output_type":"execute_result","data":{"text/plain":["          기업       등락율\n","0       삼성전자 -0.008913\n","1   LG에너지솔루션 -0.327819\n","2     SK하이닉스 -0.327819\n","3      NAVER -0.327819\n","4   삼성바이오로직스 -0.327819\n","..       ...       ...\n","90       오리온 -0.327819\n","91      롯데지주 -0.327819\n","92      한미약품 -0.327819\n","93    현대오토에버 -0.327819\n","94      한전기술 -0.207705\n","\n","[64 rows x 2 columns]"],"text/html":["\n","  <div id=\"df-719f2058-1700-4dd3-a0b3-58d56ce251f1\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>기업</th>\n","      <th>등락율</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>삼성전자</td>\n","      <td>-0.008913</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>LG에너지솔루션</td>\n","      <td>-0.327819</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>SK하이닉스</td>\n","      <td>-0.327819</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>NAVER</td>\n","      <td>-0.327819</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>삼성바이오로직스</td>\n","      <td>-0.327819</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>90</th>\n","      <td>오리온</td>\n","      <td>-0.327819</td>\n","    </tr>\n","    <tr>\n","      <th>91</th>\n","      <td>롯데지주</td>\n","      <td>-0.327819</td>\n","    </tr>\n","    <tr>\n","      <th>92</th>\n","      <td>한미약품</td>\n","      <td>-0.327819</td>\n","    </tr>\n","    <tr>\n","      <th>93</th>\n","      <td>현대오토에버</td>\n","      <td>-0.327819</td>\n","    </tr>\n","    <tr>\n","      <th>94</th>\n","      <td>한전기술</td>\n","      <td>-0.207705</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>64 rows × 2 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-719f2058-1700-4dd3-a0b3-58d56ce251f1')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-719f2058-1700-4dd3-a0b3-58d56ce251f1 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-719f2058-1700-4dd3-a0b3-58d56ce251f1');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":129}]},{"cell_type":"markdown","metadata":{"id":"CiXG3Xw86uiJ"},"source":["## 기사에서 긍정 키워드가 사용된 기업 리스트"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":415,"status":"ok","timestamp":1652012584226,"user":{"displayName":"Ha Jun","userId":"06515366270624202236"},"user_tz":-540},"id":"K6i98C7dm8SL","outputId":"3a50966a-891d-4e29-f6cc-301c0cb90a26"},"outputs":[{"data":{"text/plain":["71"]},"execution_count":27,"metadata":{},"output_type":"execute_result"}],"source":["len(news_keywords)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zspYw_We7Fz3"},"outputs":[],"source":["def getScore(news_list, top100):\n","  count_list = []\n","  \n","  positive_keyword = [ '출시',  '선보일', '상승한', '상승했다'  '신모델을' , '기술이다' , '활용했지만', '신제품에서는' , '제공하는' , '탑재하고' , '탑재했다' , '늘어나고' , '출시한' , '출시했다' , '강력한' , '출시된다' , '혁신을' , '제공하며' , '완성할' , '풍부해진' , '순매수했다' , '최적화된' , '고급스러운' , '출고가는' , '기록했다' , '돋보이는' , '늘었다' , '확대' , '압도적인' , '성능에' , '증가했다' , '마감했다' , '최고의' , '결과물이다' , '달성했다' , '올랐다' , '특징이다' , '팔아치웠다' , '예상된다' , '신제품들을' , '강세' , '이뤘다' , '실적이' , '분석했다' , '집계됐다' , '수준이다' , '완벽한' , '어려운' , '수요가' , '상승에' , '높이는' , '상황이다' , '성장을' , '전망이다' , '밝혔습니다' , '차지했다' , '설명이다' , '계약을' , '대표적인' , '꼽힌다' , '기반으로' , '완화' , '출시했다고' , '가능성' , '제공한다' , '지속적으로' , '두드러졌다' , '가능하다' , '콘텐츠를' , '활용한' ,'높다', '줄었다' , '전략' , '영향으로' , '기록하며' ]\n","  negative_keyword = ['정상화', '운항확대', '회복', '리오프닝', '증편', '재개', '향상', '열린다', '확대', '일상회복', '방역완화', '호실적', '강세', '상승', '도입', '기대', '주목', '이익', '공급증가', '수요증가', '유가하락', '유가급락', '기대감', '면제', '희망', '긍정', '리오프닝', '수혜',  '완화', '급증', '허가', '혜택', '성장', '폭등', '살아가다' , '늘린다', '유력', '상향', '가능성', '상승세', '추가', '진행', '허용', '긍정적', '회복방안', '지급', '관심', '늘다', '맺어', '박차', '양호', '수요', '증가', '제공', '오름세', '프로모션', '환영', '확정', '오름', '개선', '급등', '흑자', '면제', '이벤트']\n","  #positive_keyword = ['개선', '종전', '평화', '증가', '확대', '흑자', '호재']\n","  #negative_keyword = ['인플레이션', '스테그플레이션','역성장', '축소', '악재', '하락']\n","  for news in news_list:\n","    count = Counter()\n","    p_count = Counter()\n","    for one_news in news:\n","      news_body = getNewsBody(one_news['link'])\n","      if news_body is None:\n","        continue\n","      count.update(morpheme_separation(' '.join(getKorean(news_body))))\n","    count_list.append(count)\n","    \n","  positive_keywords_cnt = []\n","  negative_keywords_cnt = []\n","  i = 0\n","  for com in top100:\n","    keywords_cnt = 0\n","    for keyword in negative_keyword:\n","      if count_list[i].get(keyword) is not None:\n","        keywords_cnt += count_list[i].get(keyword)\n","    \n","    positive_keywords_cnt.append([keywords_cnt, com])\n","    i += 1\n","  positive_keywords_cnt.sort(reverse = True)\n","  np_positive_keywords_cnt = np.array(positive_keywords_cnt)\n","\n","  i = 0\n","  for com in top100:\n","    keywords_cnt = 0\n","    for keyword in positive_keyword:\n","      if count_list[i].get(keyword) is not None:\n","        keywords_cnt += count_list[i].get(keyword)\n","    \n","    negative_keywords_cnt.append([keywords_cnt, com])\n","    i += 1\n","  negative_keywords_cnt.sort(reverse = True)\n","  np_negative_keywords_cnt = np.array(negative_keywords_cnt)\n","  return np_positive_keywords_cnt, np_negative_keywords_cnt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BWlg2Yz35t6u"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3551,"status":"ok","timestamp":1652055625459,"user":{"displayName":"Ha Jun","userId":"06515366270624202236"},"user_tz":-540},"id":"XInWQ18pwtbO","outputId":"87436148-9cba-462e-c70c-8c408cdbae22"},"outputs":[{"name":"stdout","output_type":"stream","text":["[2022-05-09 00:20:24.046134] Url Request Success\n","202205082030 제주항공\n","2 건\n","[2022-05-09 00:20:24.675943] Url Request Success\n","202205082030 아시아나항공\n","2 건\n","[2022-05-09 00:20:25.357222] Url Request Success\n","202205082030 대한항공\n","2 건\n","[2022-05-09 00:20:25.996272] Url Request Success\n","202205082030 진에어\n","2 건\n"]}],"source":["top100 = ['제주항공','아시아나항공', '대한항공', '진에어']\n","#top100 = getTop100()\n","news = getDayNews('202205082030', top100, 2, False, True) #(날짜, 검색 기업 리스트, 기업 당 검색 건수, 저장여부, 최신 여부)\n","#rank = getScore(news, top100) #arg1 : 뉴스 리스트, arg2 : 뉴스 리스트에 포함된 기업들"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4sZvRmDq6NS3"},"outputs":[],"source":["rank, p_rank = getScore(news, top100)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FCG1JRnQDeVL"},"outputs":[],"source":["p_rank"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"teKY-qmCueyX"},"outputs":[],"source":["rank[0:30]"]},{"cell_type":"markdown","metadata":{"id":"P-2qtydW8ebS"},"source":["## 키워드 사용빈도와 등락률 plot"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NGpJLEAe8kgc"},"outputs":[],"source":["com_list = rank[:3, 1]\n","keyword_freq = rank[:3, 0].astype(np.int64)\n","\n","data_frame = pd.DataFrame()\n","date = 20220507\n","up_down_r = []\n","for com in com_list:\n","  data_path = os.path.join(com_data_dir, com + '.csv')\n","  if os.path.isfile(data_path) is False:\n","    continue\n","  \n","  data_frame_com = pd.read_csv(data_path, encoding = 'UTF-8')\n","  data_frame = data_frame.append(data_frame_com[data_frame_com['날짜'] == date])\n","  up_down_r.append(data_frame_com[data_frame_com['날짜'] == date]['등락률'].astype(np.float64))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AVfs33l1-lfI"},"outputs":[],"source":["data_frame['등락률'] = data_frame['등락률'].astype(np.float64)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":49},"executionInfo":{"elapsed":18,"status":"ok","timestamp":1652011651292,"user":{"displayName":"Ha Jun","userId":"06515366270624202236"},"user_tz":-540},"id":"tDEORvsj8lff","outputId":"83e27d5f-be5d-4e09-a57b-0fbcd01d6da2"},"outputs":[{"data":{"text/html":["\n","  <div id=\"df-1a55ef2d-cbbd-4af0-848f-ab1528158adb\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>종목코드</th>\n","      <th>종목명</th>\n","      <th>종가</th>\n","      <th>대비</th>\n","      <th>등락률</th>\n","      <th>시가</th>\n","      <th>고가</th>\n","      <th>저가</th>\n","      <th>거래량</th>\n","      <th>거래대금</th>\n","      <th>시가총액</th>\n","      <th>상장주식수</th>\n","      <th>날짜</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1a55ef2d-cbbd-4af0-848f-ab1528158adb')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-1a55ef2d-cbbd-4af0-848f-ab1528158adb button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-1a55ef2d-cbbd-4af0-848f-ab1528158adb');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["Empty DataFrame\n","Columns: [Unnamed: 0, 종목코드, 종목명, 종가, 대비, 등락률, 시가, 고가, 저가, 거래량, 거래대금, 시가총액, 상장주식수, 날짜]\n","Index: []"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["data_frame[data_frame['등락률'] > 0]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":49},"executionInfo":{"elapsed":699,"status":"ok","timestamp":1652011656241,"user":{"displayName":"Ha Jun","userId":"06515366270624202236"},"user_tz":-540},"id":"pp12zIDk9KcV","outputId":"513f377b-e646-49dc-e946-553636a5b187"},"outputs":[{"data":{"text/html":["\n","  <div id=\"df-06e6f5a2-926a-4737-b69f-79e69c709ed6\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>종목코드</th>\n","      <th>종목명</th>\n","      <th>종가</th>\n","      <th>대비</th>\n","      <th>등락률</th>\n","      <th>시가</th>\n","      <th>고가</th>\n","      <th>저가</th>\n","      <th>거래량</th>\n","      <th>거래대금</th>\n","      <th>시가총액</th>\n","      <th>상장주식수</th>\n","      <th>날짜</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-06e6f5a2-926a-4737-b69f-79e69c709ed6')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-06e6f5a2-926a-4737-b69f-79e69c709ed6 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-06e6f5a2-926a-4737-b69f-79e69c709ed6');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["Empty DataFrame\n","Columns: [Unnamed: 0, 종목코드, 종목명, 종가, 대비, 등락률, 시가, 고가, 저가, 거래량, 거래대금, 시가총액, 상장주식수, 날짜]\n","Index: []"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["data_frame[data_frame['등락률'] < 0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A2ZgAEUHEsbB"},"outputs":[],"source":["down = data_frame[data_frame['등락률'] < 0]['종목명']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Bn2eBcitEwtX"},"outputs":[],"source":["down_news = []\n","for com in down:\n","  down_news.append(news[top100.index(com)])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":313,"status":"ok","timestamp":1649251255605,"user":{"displayName":"Ha Jun","userId":"06515366270624202236"},"user_tz":-540},"id":"yZm-4a9uJT86","outputId":"4e5715c0-530e-4054-83af-ca32ff024534"},"outputs":[{"data":{"text/plain":["[62    0.63\n"," Name: 등락률, dtype: float64, 62    2.54\n"," Name: 등락률, dtype: float64, 62    0.48\n"," Name: 등락률, dtype: float64]"]},"execution_count":49,"metadata":{},"output_type":"execute_result"}],"source":["up_down_r"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":745,"status":"ok","timestamp":1649251932269,"user":{"displayName":"Ha Jun","userId":"06515366270624202236"},"user_tz":-540},"id":"V-GMVmYBDK8H","outputId":"be4b6a74-04b1-4a60-d167-edf011abd604"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 45824 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 54620 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 54637 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 44277 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 51228 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 51452 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 51652 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 50640 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 50612 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 45824 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 54620 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 54637 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 44277 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 51228 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 51452 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 51652 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 50640 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 50612 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n"]},{"data":{"image/png":"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\n","text/plain":["<Figure size 1080x720 with 2 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["fig = plt.figure(figsize=(15, 10))\n","sub = fig.add_subplot(111)\n","plt.rc(\"font\", family='NanumGothic')\n","sub.bar(com_list, keyword_freq,color='slateblue',label='빈도수')\n","\n","sub2 = sub.twinx()\n","sub2.plot(com_list, up_down_r,marker='o',color='deepskyblue',label='등락률')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"cix73uII7hMG"},"source":["##임시 저장소\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3nvYMrOmIJfH"},"outputs":[],"source":["#file_list = os.listdir(news_link_dir)\n","#각 기업별 뉴스 기사 본문 추출 후 keywords 수 count\n","count_list = []\n","N = 30\n","for com in top100:\n","  count = Counter()\n","  with open(os.path.join(news_link_dir, com + '_naver_news.json'), 'r', encoding = 'utf-8') as f:\n","    json_data = json.load(f)\n","    for idx in range(0, 10):\n","      try:\n","        body = get_naver_news(json_data[idx]['link'])\n","      except:\n","        continue\n","      if body is None:\n","        continue\n","     \n","      text = body.get_text()\n","\n","      news_body_start = []\n","      text_split = text.split(' ')\n","      rm_set = {''}\n","      text_split = [i for i in text_split if i not in rm_set]\n","\n","      for word_index in range(0, len(text_split) - N):\n","        word_len = 0\n","        korean_word_len = 0\n","        for i in range(0, N):\n","          korean_word_len += koreanLen(text_split[word_index + i])\n","          word_len += len(text_split[word_index + i])\n","        news_body_start.append(korean_word_len / word_len)\n","\n","      avg = []\n","      avg_index = []\n","      \n","      i = 0\n","      while i < len(text_split) - N:\n","        avg.append(sum(news_body_start[i:i + N]) / N)\n","        avg_index.append(i)\n","        i += N\n","      if len(avg) == 0:\n","        continue\n","\n","      max_index = avg.index(max(avg))\n","      avg_avg = sum(avg) / len(avg)\n","      i = 0\n","      start = max_index\n","      end = max_index\n","      while max_index > i:\n","        if avg[max_index - i] > avg_avg:\n","          start = max_index - i\n","        else:\n","          break\n","        i += 1\n","      i = 0\n","      while (i + max_index) < len(avg):\n","        if avg[max_index + i] > avg_avg:\n","          end = max_index + i\n","        else:\n","          break\n","        i += 1\n","      count.update(getKorean(' '.join(text_split[avg_index[start] : avg_index[end] + N])))\n","    count_list.append(count)\n","      "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ajjhc8mXAvxD"},"outputs":[],"source":["word_count = dict()\n","for tag, counts in count.most_common(5000):\n","    if(len(str(tag))>1):\n","        word_count[tag] = counts"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9n7UabblK-vq"},"outputs":[],"source":["N = 30\n","count = Counter()\n","with open(os.path.join(news_link_dir, 'SK하이닉스_naver_news.json', ), 'r', encoding = 'utf-8') as f:\n","  #data = f.read().encode('utf-8').decode('unicode_escape')\n","  json_data = json.load(f)\n","  for i in range(0, 10):\n","    news_body = getNewsBody(json_data[i]['link'])\n","    if news_body is None:\n","      continue\n","    count.update(getKorean(news_body))\n","    #print(i, ' ', json_data[i]['link'], ' ', getKorean(news_body))\n","    "]},{"cell_type":"markdown","metadata":{"id":"Ce37e0z90m7B"},"source":["##형태소 분석"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ci363o9D0pSA"},"outputs":[],"source":["!apt-get update \n","!apt-get install g++ openjdk-8-jdk \n","!pip install konlpy JPype1-py3 \n","!bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)\n","\n","from konlpy.tag import Mecab\n","mecab = Mecab()\n","sentence=\"여러 차례의 규제 완화 조치로 하늘길이 더 넓어질 것이라는 전망에 항공주와 레저주 일부 종목이 오름세다. 항공주는 지방 공항의 국제선 운항 재개가 유력해지면서 중소 저비용항공사(LCC) 중심으로 주가가 상승했다. 코로나19 사태 이전에 LCC는 지방 공항으로 사업 외연을 확장하면서 국제선 여객 중 48%나 담당할 정도로 빠르게 성장했다. 6일 오후 2시 기준 제주항공은 전 거래일 대비 2.75% 상승한 2만4300원에 거래 중이며, 이 밖에도 티웨이항공과 에어부산이 각각 1.86%, 0.63% 상승했다. 레저주도 소폭 오름세다. 모두투어가 전 거래일 대비 3.6%, 롯데관광개발은 1.93%, 하나투어도 1.11% 상승했다. 국제선 항공편이 증가하고 해외여행에 대한 욕구가 살아나면서 레저업계에 여행 패키지 상품이나 여행 관련 문의가 증가하고 있다. 실제 롯데관광개발이 3일 현대홈쇼핑에서 판매한 고가의 여행 패키지 상품이 70분 만에 1600건의 주문으로 약 260억원의 매출을 기록했다. 항공·레저주에 대해 증권업계에서는 아직 지켜볼 필요가 있다고 분석한다. 김현용 현대차증권 애널리스트는 “패키지여행 수요 회복은 빠르면 3분기에 감지될 것”이라면서 “전반적인 여행 수요가 급증할 것은 자명한 사실이지만 아직 코로나 이전의 25%에 불과하다”고 말했다. 김인식 IBK투자증권 애널리스트도 리오프닝주와 관련한 분석 보고서에서 “러시아-우크라이나 지정학적 리스크와 국제 유가 불안정성으로 아직은 경계해야 한다”고 말했다.\"\n","temp_X = mecab.pos(sentence)\n","\n","temp_X\n","\n","\n","from konlpy.tag import Mecab\n","mecab = Mecab()\n","sentence=\"여러 차례의 규제 완화 조치로 하늘길이 더 넓어질 것이라는 전망에 항공주와 레저주 일부 종목이 오름세다. 항공주는 지방 공항의 국제선 운항 재개가 유력해지면서 중소 저비용항공사(LCC) 중심으로 주가가 상승했다. 코로나19 사태 이전에 LCC는 지방 공항으로 사업 외연을 확장하면서 국제선 여객 중 48%나 담당할 정도로 빠르게 성장했다. 6일 오후 2시 기준 제주항공은 전 거래일 대비 2.75% 상승한 2만4300원에 거래 중이며, 이 밖에도 티웨이항공과 에어부산이 각각 1.86%, 0.63% 상승했다. 레저주도 소폭 오름세다. 모두투어가 전 거래일 대비 3.6%, 롯데관광개발은 1.93%, 하나투어도 1.11% 상승했다. 국제선 항공편이 증가하고 해외여행에 대한 욕구가 살아나면서 레저업계에 여행 패키지 상품이나 여행 관련 문의가 증가하고 있다. 실제 롯데관광개발이 3일 현대홈쇼핑에서 판매한 고가의 여행 패키지 상품이 70분 만에 1600건의 주문으로 약 260억원의 매출을 기록했다. 항공·레저주에 대해 증권업계에서는 아직 지켜볼 필요가 있다고 분석한다. 김현용 현대차증권 애널리스트는 “패키지여행 수요 회복은 빠르면 3분기에 감지될 것”이라면서 “전반적인 여행 수요가 급증할 것은 자명한 사실이지만 아직 코로나 이전의 25%에 불과하다”고 말했다. 김인식 IBK투자증권 애널리스트도 리오프닝주와 관련한 분석 보고서에서 “러시아-우크라이나 지정학적 리스크와 국제 유가 불안정성으로 아직은 경계해야 한다”고 말했다.\"\n","temp_X = mecab.pos(sentence)\n","# http://openuiz.blogspot.com/2016/07/mecab-ko-dic.html\n","for i in temp_X:\n","    if i[1]=='NNG' or i[1]=='NNP' or i[1]=='VV' or i[1]=='VX' or i[1]=='VA':\n","\t        if i[1]=='VA':#형용사 일때 '다'를 붙여줌 ex) 넓 + 다\n","            print(i[0]+'다', end=' ')\n","        else:\n","            print(i[0], end=' ')\n","\n","\n","from konlpy.tag import Mecab\n","\n","def morpheme_separation(sentence):\n","    mecab = Mecab()\n","    split_sentence=[]\n","    temp_X = mecab.pos(sentence)\n","    # http://openuiz.blogspot.com/2016/07/mecab-ko-dic.html\n","    for i in temp_X:\n","        if i[1]=='NNG' or i[1]=='NNP' or i[1]=='VV' or i[1]=='VX' or i[1]=='VA':\n","                if i[1]=='VA':#형용사 일때 '다'를 붙여줌 ex) 넓 + 다\n","                    split_sentence.append(i[0]+'다')\n","                else:\n","                    split_sentence.append(i[0])\n","    return split_sentence\n","\n","sentence=\"여러 차례의 규제 완화 조치로 하늘길이 더 넓어질 것이라는 전망에 항공주와 레저주 일부 종목이 오름세다. 항공주는 지방 공항의 국제선 운항 재개가 유력해지면서 중소 저비용항공사(LCC) 중심으로 주가가 상승했다. 코로나19 사태 이전에 LCC는 지방 공항으로 사업 외연을 확장하면서 국제선 여객 중 48%나 담당할 정도로 빠르게 성장했다. 6일 오후 2시 기준 제주항공은 전 거래일 대비 2.75% 상승한 2만4300원에 거래 중이며, 이 밖에도 티웨이항공과 에어부산이 각각 1.86%, 0.63% 상승했다. 레저주도 소폭 오름세다. 모두투어가 전 거래일 대비 3.6%, 롯데관광개발은 1.93%, 하나투어도 1.11% 상승했다. 국제선 항공편이 증가하고 해외여행에 대한 욕구가 살아나면서 레저업계에 여행 패키지 상품이나 여행 관련 문의가 증가하고 있다. 실제 롯데관광개발이 3일 현대홈쇼핑에서 판매한 고가의 여행 패키지 상품이 70분 만에 1600건의 주문으로 약 260억원의 매출을 기록했다. 항공·레저주에 대해 증권업계에서는 아직 지켜볼 필요가 있다고 분석한다. 김현용 현대차증권 애널리스트는 “패키지여행 수요 회복은 빠르면 3분기에 감지될 것”이라면서 “전반적인 여행 수요가 급증할 것은 자명한 사실이지만 아직 코로나 이전의 25%에 불과하다”고 말했다. 김인식 IBK투자증권 애널리스트도 리오프닝주와 관련한 분석 보고서에서 “러시아-우크라이나 지정학적 리스크와 국제 유가 불안정성으로 아직은 경계해야 한다”고 말했다.\"\n","morpheme_separation(sentence)"]}],"metadata":{"colab":{"collapsed_sections":["cix73uII7hMG","Ce37e0z90m7B"],"name":"주가분석.ipynb","provenance":[],"authorship_tag":"ABX9TyOGgfIpCZaQPHA9ge8hVexZ"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}